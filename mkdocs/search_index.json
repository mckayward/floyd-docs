{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nWelcome to \nFloydHub\n! Here you'll find comprehensive information for training and deploying your deep learning and AI applications with our platform. We do our best to make this documentation clear and user friendly, but if you have unanswered questions, please visit the \ncommunity forum\n or \ncontact us\n.\n\n\nThe fastest way to get up and running is to use our \nquickstart guide\n, which walks through an entire FloydHub training job step-by-step. You'll create a new Project on the FloydHub web dashboard, connect it to a local directory on your computer, and then kick-off a job using the FloydHub CLI to train your deep learning model on FloydHub's GPU servers.\n\n\nDeep learning without the DevOps:\n\n\nFrictionless data science\n\n\nWhy worry about provisioning GPUs, installing drivers, or managing software dependency hell? With FloydHub, we take care of your entire deep learning DevOps workflow - so you can focus on the science.\n\n\nTraining a TensorFlow model using GPUs on the cloud is as simple as executing this command on your terminal: \n\nfloyd run --gpu --env tensorflow \npython train.py\n. Try it now with our \nquickstart guide\n.\n\n\nPowerful workflow tools\n\n\nWhether you're using our web dashboard or our command line interface, our tools make your work easier and your team more productive:\n\n\n\n\nInteractive Jupyter Notebook support\n\n\nEnd-to-end version control for data science\n\n\nFull reproducibility of jobs\n\n\nDeploy models as REST endpoints to integrate with your apps\n\n\n\n\nDeep learning community\n\n\nFloydHub hosts open source Projects and Datasets that you can discover, clone, and reproduce (or reconfigure with your own Dataset).\n\n\nTry it now with the \nNeural Style Transfer\n.\n\n\n\n\n\n\n\n\nWe're here to help!\n\n\nWe're always happy to help with any questions you might have! \nSearch\n our documentation or check answers to \nfrequently asked questions\n. The \nFloydHub community forum\n is another place to ask questions, request features, or share cool Projects. For more help, \nsend us an email\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#introduction", 
            "text": "Welcome to  FloydHub ! Here you'll find comprehensive information for training and deploying your deep learning and AI applications with our platform. We do our best to make this documentation clear and user friendly, but if you have unanswered questions, please visit the  community forum  or  contact us .  The fastest way to get up and running is to use our  quickstart guide , which walks through an entire FloydHub training job step-by-step. You'll create a new Project on the FloydHub web dashboard, connect it to a local directory on your computer, and then kick-off a job using the FloydHub CLI to train your deep learning model on FloydHub's GPU servers.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#deep-learning-without-the-devops", 
            "text": "", 
            "title": "Deep learning without the DevOps:"
        }, 
        {
            "location": "/#frictionless-data-science", 
            "text": "Why worry about provisioning GPUs, installing drivers, or managing software dependency hell? With FloydHub, we take care of your entire deep learning DevOps workflow - so you can focus on the science.  Training a TensorFlow model using GPUs on the cloud is as simple as executing this command on your terminal:  floyd run --gpu --env tensorflow  python train.py . Try it now with our  quickstart guide .", 
            "title": "Frictionless data science"
        }, 
        {
            "location": "/#powerful-workflow-tools", 
            "text": "Whether you're using our web dashboard or our command line interface, our tools make your work easier and your team more productive:   Interactive Jupyter Notebook support  End-to-end version control for data science  Full reproducibility of jobs  Deploy models as REST endpoints to integrate with your apps", 
            "title": "Powerful workflow tools"
        }, 
        {
            "location": "/#deep-learning-community", 
            "text": "FloydHub hosts open source Projects and Datasets that you can discover, clone, and reproduce (or reconfigure with your own Dataset).  Try it now with the  Neural Style Transfer .", 
            "title": "Deep learning community"
        }, 
        {
            "location": "/#were-here-to-help", 
            "text": "We're always happy to help with any questions you might have!  Search  our documentation or check answers to  frequently asked questions . The  FloydHub community forum  is another place to ask questions, request features, or share cool Projects. For more help,  send us an email .", 
            "title": "We're here to help!"
        }, 
        {
            "location": "/getstarted/core_concepts/", 
            "text": "Core Concepts\n\n\nThe essential points for understanding and effectively using FloydHub can be\ngrouped into the following categories:\n\n\n\n\nProjects\n\n\nJobs\n\n\nDatasets\n\n\nEnvironments\n\n\nOutput\n\n\n\n\nThis document serves as an introduction to each of these categories. When\nyou're ready to dive deeper, each category has its own section in our\ndocumentation that you can explore.\n\n\nProjects\n\n\n\n\nQuick Look\n\n\nA project is a group of \njobs\n aimed at accomplishing the same goal. A\nproject keeps track of each job, along with its \noutput\n and logs.\n\n\n\n\n\n\nKey Points of Understanding\n\n\n\n\nCreating a project\n\n\n\n\n\n\nThe project is the most central construct of the FloydHub platform.\nUnderstanding what a project is on FloydHub isn't too difficult because it\ndirectly correlates with what you'd think of as a deep learning project outside\nof FloydHub: You have a problem you need to solve with a deep learning model.\nYou get on your computer, create a new directory with a name like \nmnist-cnn\n\nand boom, you've started a new project. In that directory, you'll write some\ncode, run some experiments, and iterate until you have created a deep learning\nmodel that meets your needs.\n\n\nOn FloydHub, a project is a collection of all the work and iterations you\nperform when developing a deep learning model. In contrast to a typical deep\nlearning workflow, your experiments and iterations (we call them\n\njobs\n) will be versioned and kept organized for you to reference in the\nfuture.\n\n\nJobs\n\n\n\n\nQuick Look\n\n\nA job is an execution of your code on FloydHub's deep-learning servers.\n\n\n\n\n\n\nKey Points of Understanding\n\n\n\n\nRunning a job\n\n\n\n\n\n\nA job is what pulls together your \ncode\n and \ndataset(s)\n, sends them to a\ndeep-learning server configured with the right \nenvironment\n, and\nactually kicks off the necessary code to get the data science done.\n\n\nAfter a job is completed, you'll be able to go back and reference/review it on\nfloydhub.com. For each job, you'll be able to see:\n\n\n\n\nA snapshot of the code used for the job\n\n\nA record of which \nDataset(s)\n you used for the job\n\n\nA record of what \nEnvironment\n was used for the job\n\n\nThe \nOutput\n and logs of the job\n\n\n\n\nYou run jobs using Floyd CLI's \nfloyd run\n command. The command has various\nflags and parameters that let you customize how the job runs: What dataset do\nyou want to use? Where should the dataset be mounted on the server? What\nenvironment do you want to use? Should the server run your job using a CPU or a\nGPU? What command(s) should the server use to run your code? Any other commands\nyou'd like to run on the server to set it up before running your job? Etc.\n\n\nDatasets\n\n\n\n\nQuick Look\n\n\nDatasets are securely uploaded to FloydHub. They are versioned and can be\nattached to any job.\n\n\n\n\n\n\nKey Points of Understanding\n\n\n\n\nHow to upload a dataset\n\n\nHow to attach, or \"mount\", a dataset to a job so your code can use it\n\n\nMaking sure your code references your data in the correct location\n\n\n\n\n\n\nFloydHub's dataset workflow is one of two things that tend to feel a bit\nforeign to users who are used to local development (the other being\n\noutput\n). When working on your local machine, you might have your dataset in\nthe same directory as your code, or in a directory where you keep many\ndifferent datasets. When using FloydHub, datasets are always kept separate from\ncode.\n\n\nWhy keep datasets separate from code?\n\n\nAs a data scientist you tweak your code often during the process of creating a\ndeep-learning model. However, you don't change the underlying data nearly\nas often, if at all.\n\n\nEach time you run a job on FloydHub, a copy of your code is uploaded to\nFloydHub and run on one of FloydHub's powerful deep-learning servers. Because\nyour data isn't changing from job to job, it wouldn't make sense to keep your\ndata with your code and upload it with each job. Instead, we upload a dataset\nonce, and attach, or \"mount\", it to each job. This saves a significant amount\nof time on each job.\n\n\nBeyond that, keeping data separate from code allows you to collaborate more\neasily with others. A dataset can be used by any user who has access to it, so\nteams and communities can work on solving problems together using the same\nunderlying data.\n\n\nConnecting code and datasets\n\n\nYou'll still be writing your code locally when using FloydHub, but when you run\na job, your code will be uploaded to FloydHub and executed on a powerful\ndeep-learning server that has your datasets mounted to it.\n\n\nYou can specify the places where your datasets will be\n\nmounted\n on the server, but you'll have to make\nsure that your code references your datasets with the file paths of the data on\nthe \nserver\n, not where you might have them locally.\n\n\nEnvironments\n\n\n\n\nQuick Look\n\n\nAn environment is what defines the software packages that will be available\nto your code during a job\n\n\n\n\n\n\nKey Points of Understanding\n\n\n\n\nHow to select an environment for your job\n\n\nHow to customize your environment\n\n\n\n\n\n\nFloydHub has a bunch of different \ndeep learning environments to choose\nfrom\n. When you run a job on FloydHub, you'll be able\nto specify the environment you want use, straight from the command line. You'll\nalso be able to specify whether you want the job run using a \nGPU or a\nCPU\n.\n\n\nIf FloydHub's stock deep learning environments don't meet your needs, you can\ncreate a custom environment for your job. See \nthis\nguide\n for instructions on that.\n\n\nOutput\n\n\n\n\nQuick Look\n\n\nOutput is any data, logs, or files from a job you want to save for future\nreference and use.\n\n\n\n\n\n\nKey Points of Understanding\n\n\n\n\nHow to capture/save output from a job\n\n\nHow to use it again in a future job\n\n\n\n\n\n\nOutput is anything from a job that you want to save for future use. The most\ncommon form of output is model checkpoints (the weights and biases of your\nmodel) that you developed during a job. If you save these checkpoints (or\nanything else you'd like to preserve) during a job, you'll have them to\nreference, download, and reuse in the future.\n\n\nOutput is the way that you can link jobs together: You run a job to test an\nidea you have. If it works, you may want to start where you left off and run\nanother job. If going down that path leads to a dead end, you may want to go\nback to a previous output and start again from there. Knowing how to store\noutput is key to optimizing your deep learning workflow.\n\n\nSaving\n and \nreusing\n\noutput on FloydHub can feel foriegn to users who are used to working on their\nown machines. But once you learn how to do it, it becomes very simple and is\none of the most valuable parts of the FloydHub workflow.", 
            "title": "Core Concepts"
        }, 
        {
            "location": "/getstarted/core_concepts/#core-concepts", 
            "text": "The essential points for understanding and effectively using FloydHub can be\ngrouped into the following categories:   Projects  Jobs  Datasets  Environments  Output   This document serves as an introduction to each of these categories. When\nyou're ready to dive deeper, each category has its own section in our\ndocumentation that you can explore.", 
            "title": "Core Concepts"
        }, 
        {
            "location": "/getstarted/core_concepts/#projects", 
            "text": "Quick Look  A project is a group of  jobs  aimed at accomplishing the same goal. A\nproject keeps track of each job, along with its  output  and logs.    Key Points of Understanding   Creating a project    The project is the most central construct of the FloydHub platform.\nUnderstanding what a project is on FloydHub isn't too difficult because it\ndirectly correlates with what you'd think of as a deep learning project outside\nof FloydHub: You have a problem you need to solve with a deep learning model.\nYou get on your computer, create a new directory with a name like  mnist-cnn \nand boom, you've started a new project. In that directory, you'll write some\ncode, run some experiments, and iterate until you have created a deep learning\nmodel that meets your needs.  On FloydHub, a project is a collection of all the work and iterations you\nperform when developing a deep learning model. In contrast to a typical deep\nlearning workflow, your experiments and iterations (we call them jobs ) will be versioned and kept organized for you to reference in the\nfuture.", 
            "title": "Projects"
        }, 
        {
            "location": "/getstarted/core_concepts/#jobs", 
            "text": "Quick Look  A job is an execution of your code on FloydHub's deep-learning servers.    Key Points of Understanding   Running a job    A job is what pulls together your  code  and  dataset(s) , sends them to a\ndeep-learning server configured with the right  environment , and\nactually kicks off the necessary code to get the data science done.  After a job is completed, you'll be able to go back and reference/review it on\nfloydhub.com. For each job, you'll be able to see:   A snapshot of the code used for the job  A record of which  Dataset(s)  you used for the job  A record of what  Environment  was used for the job  The  Output  and logs of the job   You run jobs using Floyd CLI's  floyd run  command. The command has various\nflags and parameters that let you customize how the job runs: What dataset do\nyou want to use? Where should the dataset be mounted on the server? What\nenvironment do you want to use? Should the server run your job using a CPU or a\nGPU? What command(s) should the server use to run your code? Any other commands\nyou'd like to run on the server to set it up before running your job? Etc.", 
            "title": "Jobs"
        }, 
        {
            "location": "/getstarted/core_concepts/#datasets", 
            "text": "Quick Look  Datasets are securely uploaded to FloydHub. They are versioned and can be\nattached to any job.    Key Points of Understanding   How to upload a dataset  How to attach, or \"mount\", a dataset to a job so your code can use it  Making sure your code references your data in the correct location    FloydHub's dataset workflow is one of two things that tend to feel a bit\nforeign to users who are used to local development (the other being output ). When working on your local machine, you might have your dataset in\nthe same directory as your code, or in a directory where you keep many\ndifferent datasets. When using FloydHub, datasets are always kept separate from\ncode.", 
            "title": "Datasets"
        }, 
        {
            "location": "/getstarted/core_concepts/#why-keep-datasets-separate-from-code", 
            "text": "As a data scientist you tweak your code often during the process of creating a\ndeep-learning model. However, you don't change the underlying data nearly\nas often, if at all.  Each time you run a job on FloydHub, a copy of your code is uploaded to\nFloydHub and run on one of FloydHub's powerful deep-learning servers. Because\nyour data isn't changing from job to job, it wouldn't make sense to keep your\ndata with your code and upload it with each job. Instead, we upload a dataset\nonce, and attach, or \"mount\", it to each job. This saves a significant amount\nof time on each job.  Beyond that, keeping data separate from code allows you to collaborate more\neasily with others. A dataset can be used by any user who has access to it, so\nteams and communities can work on solving problems together using the same\nunderlying data.", 
            "title": "Why keep datasets separate from code?"
        }, 
        {
            "location": "/getstarted/core_concepts/#connecting-code-and-datasets", 
            "text": "You'll still be writing your code locally when using FloydHub, but when you run\na job, your code will be uploaded to FloydHub and executed on a powerful\ndeep-learning server that has your datasets mounted to it.  You can specify the places where your datasets will be mounted  on the server, but you'll have to make\nsure that your code references your datasets with the file paths of the data on\nthe  server , not where you might have them locally.", 
            "title": "Connecting code and datasets"
        }, 
        {
            "location": "/getstarted/core_concepts/#environments", 
            "text": "Quick Look  An environment is what defines the software packages that will be available\nto your code during a job    Key Points of Understanding   How to select an environment for your job  How to customize your environment    FloydHub has a bunch of different  deep learning environments to choose\nfrom . When you run a job on FloydHub, you'll be able\nto specify the environment you want use, straight from the command line. You'll\nalso be able to specify whether you want the job run using a  GPU or a\nCPU .  If FloydHub's stock deep learning environments don't meet your needs, you can\ncreate a custom environment for your job. See  this\nguide  for instructions on that.", 
            "title": "Environments"
        }, 
        {
            "location": "/getstarted/core_concepts/#output", 
            "text": "Quick Look  Output is any data, logs, or files from a job you want to save for future\nreference and use.    Key Points of Understanding   How to capture/save output from a job  How to use it again in a future job    Output is anything from a job that you want to save for future use. The most\ncommon form of output is model checkpoints (the weights and biases of your\nmodel) that you developed during a job. If you save these checkpoints (or\nanything else you'd like to preserve) during a job, you'll have them to\nreference, download, and reuse in the future.  Output is the way that you can link jobs together: You run a job to test an\nidea you have. If it works, you may want to start where you left off and run\nanother job. If going down that path leads to a dead end, you may want to go\nback to a previous output and start again from there. Knowing how to store\noutput is key to optimizing your deep learning workflow.  Saving  and  reusing \noutput on FloydHub can feel foriegn to users who are used to working on their\nown machines. But once you learn how to do it, it becomes very simple and is\none of the most valuable parts of the FloydHub workflow.", 
            "title": "Output"
        }, 
        {
            "location": "/getstarted/quick_start/", 
            "text": "Introduction\n\n\nWith this quickstart guide, we've tried to make it as easy as possible to get up and running with FloydHub.\n\n\nWe'll start with an overview of FloydHub and then jump into running your first job using TensorFlow and the MNIST dataset (better known as the \"Hello, world!\" of data science). We'll be training a convolutional neural network (CNN) model to recognize hand-written digits using FloydHub's GPU servers. For more details on the data and the model, please refer to the \nTensorflow documentation\n.\n\n\nPlatform overview\n\n\nTo help you get oriented with FloydHub, let's start by defining some basics:\n\n\nTools\n\n\n\n\nfloyd-cli\n: Convenient command line tool to interact with FloydHub from your terminal\n\n\nDashboard\n: Website to create, monitor, and explore Projects and Datasets\n\n\n\n\nFeatures\n\n\n\n\nJob\n: Command executed from the \nfloyd-cli\n with optional configurations (including mounting Datasets, importing libraries like TensorFlow or PyTorch, running in Jupyter Notebook mode, or processing on GPU instances)\n\n\nProject\n: Collection of Jobs and their corresponding code, Datasets, logs, and results\n\n\nDataset\n: Input data that can be connected to a Project via a Job\n\n\n\n\nSetting up your first Project on FloydHub\n\n\nQuick preparation checklist\n\n\n\n\nCreate a FloydHub account\n\n\nInstall \nfloyd-cli\n on your computer\n\n\nLog in to FloydHub through \nfloyd-cli\n\n\n\n\nGet the code\n\n\nClone the \nquick-start repository\n from GitHub onto your computer.\n\n\n$ git clone https://github.com/floydhub/quick-start.git\nCloning into \nquick-start\n...\n...\n$ \ncd\n quick-start\n\n\n\n\n$ ls\nLICENSE    mnist_cnn.py    mnist_cnn.ipynb    README.md\n\n\n\n\nThis repository contains two important files: \nmnist_cnn.py\n (a Python script to train a convolutional neural network model against the MNIST dataset) and \nmnist_cnn.ipynb\n (a Jupyter Notebook to interactively explore the MNIST dataset). \n\n\nIn this quickstart tutorial, we'll only be using the Python script. Our separate \nJupyter Notebook tutorial\n explains how to run this Project in Jupyter Notebook mode.\n\n\nInitialize your Project\n\n\nIf you're a new user, then you should already see a default Project named \nquick-start\n in your \nProjects dashboard\n.\n\n\n\n\nAlternatively, you can simply create a new Project named \nquick-start\n in the FloydHub Dashboard with these \ninstructions\n.\n\n\nWe'll need to link the Python model-training code with your \nquick-start\n Project on FloydHub. Use the \nfloyd init\n command in the current directory of your terminal to initialize your Project. \n\n\n$ floyd init quick-start\nProject \nquick-start\n initialized in the current directory\n\n\n\n\nThis tells FloydHub that all the Jobs you run from this local directory belong to your Project named \nquick-start\n.\n\n\nRunning your first Job\n\n\nRunning a job on FloydHub is simple - when you use the \nfloyd run\n command, the \nfloyd-cli\n tool syncs your code with FloydHub's servers and runs the command in the cloud. \n\n\nFloydHub will run any command you provide as a new Job - even a Job as simple as listing your current directory with the \nls\n command. However, FloydHub is specialized for running deep learning and data science processing commands.\n\n\nFor this tutorial, we'll run the \nmnist_cnn.py\n Python script on FloydHub's GPU servers to train our convolutional neural network model against the MNIST data.\n\n\n$ floyd run --gpu --env tensorflow \npython mnist_cnn.py\n\nSyncing code ...\nRUN ID                  NAME               \n----------------------  -------------------\nAKpnXqj9BEU6d8KhmygTyb  alice/quick-start/1\n\nTo view the logs enter:\n    floyd logs alice/quick-start/1\n\n\n\n\nCongratulations! Your first job is now running on FloydHub's GPU servers. Behind the scenes, FloydHub does the following:\n\n\n\n\nSyncs your local code to FloydHub's servers\n\n\nProvisions a GPU instance on the cloud (because you set the \n--gpu\n flag)\n\n\nSets up a deep learning environment with GPU drivers and Tensorflow installed (because you set the enviroment flag to \n--env tensorflow\n)\n\n\nExecutes the command \npython mnist_cnn.py\n inside this environment\n\n\nStores the output logs and generated output data\n\n\nTerminates the GPU instance once the command finishes execution\n\n\n\n\nMonitoring your Job\n\n\nYou can view the status of your Job from your terminal using the \nfloyd status\n command. You can specify a single Job name (e.g. \nfloyd status alice/quick-start/1\n) to get its status, or the \nfloyd-cli\n will show the status of all Jobs in the current Project.\n\n\n$ floyd status\nRUN ID                  CREATED        STATUS    DURATION\n(\ns\n)\n  NAME                 INSTANCE    DESCRIPTION\n----------------------  ---------      --------  -----------  -------------------  ---------   -----------\nAKpnXqj9BEU6d8KhmygTyb  just now       running            \n15\n  alice/quick-start:1  gpu         \n\n\n\n\nYou can also view the status of your job in your browser by visiting the \nJob URL\n printed by the \nfloyd run\n command. For example, \nhttps://www.floydhub.com/alice/quick-start/1\n\n\n\n\nViewing your Job's logs\n\n\nIt's easy to view the logs generated by the job from your terminal with the \nfloyd logs\n command. You'll need to specify the Job name in the command.\n\n\n$ floyd logs -t alice/quick-start/1\n...\n\n2017\n-07-12 \n16\n:00:07,446 INFO - Starting attempt \n1\n at \n2017\n-07-12 \n16\n:00:07.436349\n\n2017\n-07-12 \n16\n:00:09,088 INFO - Starting container...\n\n2017\n-07-12 \n16\n:00:09,297 INFO - \n...\n\n##############################################################################\n\n\n2017\n-07-12 \n16\n:00:09,297 INFO - Run Output:\n\n2017\n-07-12 \n16\n:01:46,154 INFO - Successfully downloaded train-images-idx3-ubyte.gz \n9912422\n bytes.\n\n2017\n-07-12 \n16\n:01:46,158 INFO - Iter \n1280\n, Minibatch \nLoss\n=\n \n39855\n.289062, Training \nAccuracy\n=\n \n0\n.17969\n\n2017\n-07-12 \n16\n:01:46,159 INFO - Iter \n2560\n, Minibatch \nLoss\n=\n \n14964\n.132812, Training \nAccuracy\n=\n \n0\n.42969\n...\n\n##############################################################################\n\n...\n\n\n\n\nThe output of your code is printed in the \nRun Output\n section of the logs, between the \n#########\n lines. Anything you log or print in your code will appear here, so this is a great way to monitor the progress of your model training command. In our \nquick-start\n project, we're logging the Training Accuracy of our model.\n\n\nUsing the \n-t\n (tail) flag will stream the logs as they are generated.\n\n\nYou can also view the logs in your browser using your \nJob URL\n. However, the logs in the Dashboard are not currently refreshed dynamically, so you'll need to refresh your browser periodically or press \nF5\n to get the latest logs from the Dashboard.\n\n\nViewing the Job output\n\n\nThe model that we trained in this quickstart tutorial does not save any new output models - instead it simply prints the model results to the logs. We'll explore how to save model outputs in the Jupyter Notebook tutorial.\n\n\nIterating on your Project\n\n\nCongratulations! You've run your first job on FloydHub \ud83c\udf89\n\n\nAt this point, you can edit your Python code locally to make improvements or adjustments, and then kick off a new Job with the \nfloyd run\n command. The \nfloyd-cli\n will upload the newest versions of your code and submit another Job to the FloydHub servers. Along the way, FloydHub will be managing and tracking of all the iterations of Jobs within your Project.\n\n\nYou can always view details on all of the Jobs in your current Project with the \nfloyd status\n command from your terminal, or by visiting the \nProject URL\n in your browser.\n\n\nExample: \nwww.floydhub.com/alice/quick-start\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/getstarted/quick_start/#introduction", 
            "text": "With this quickstart guide, we've tried to make it as easy as possible to get up and running with FloydHub.  We'll start with an overview of FloydHub and then jump into running your first job using TensorFlow and the MNIST dataset (better known as the \"Hello, world!\" of data science). We'll be training a convolutional neural network (CNN) model to recognize hand-written digits using FloydHub's GPU servers. For more details on the data and the model, please refer to the  Tensorflow documentation .", 
            "title": "Introduction"
        }, 
        {
            "location": "/getstarted/quick_start/#platform-overview", 
            "text": "To help you get oriented with FloydHub, let's start by defining some basics:", 
            "title": "Platform overview"
        }, 
        {
            "location": "/getstarted/quick_start/#tools", 
            "text": "floyd-cli : Convenient command line tool to interact with FloydHub from your terminal  Dashboard : Website to create, monitor, and explore Projects and Datasets", 
            "title": "Tools"
        }, 
        {
            "location": "/getstarted/quick_start/#features", 
            "text": "Job : Command executed from the  floyd-cli  with optional configurations (including mounting Datasets, importing libraries like TensorFlow or PyTorch, running in Jupyter Notebook mode, or processing on GPU instances)  Project : Collection of Jobs and their corresponding code, Datasets, logs, and results  Dataset : Input data that can be connected to a Project via a Job", 
            "title": "Features"
        }, 
        {
            "location": "/getstarted/quick_start/#setting-up-your-first-project-on-floydhub", 
            "text": "", 
            "title": "Setting up your first Project on FloydHub"
        }, 
        {
            "location": "/getstarted/quick_start/#quick-preparation-checklist", 
            "text": "Create a FloydHub account  Install  floyd-cli  on your computer  Log in to FloydHub through  floyd-cli", 
            "title": "Quick preparation checklist"
        }, 
        {
            "location": "/getstarted/quick_start/#get-the-code", 
            "text": "Clone the  quick-start repository  from GitHub onto your computer.  $ git clone https://github.com/floydhub/quick-start.git\nCloning into  quick-start ...\n...\n$  cd  quick-start  $ ls\nLICENSE    mnist_cnn.py    mnist_cnn.ipynb    README.md  This repository contains two important files:  mnist_cnn.py  (a Python script to train a convolutional neural network model against the MNIST dataset) and  mnist_cnn.ipynb  (a Jupyter Notebook to interactively explore the MNIST dataset).   In this quickstart tutorial, we'll only be using the Python script. Our separate  Jupyter Notebook tutorial  explains how to run this Project in Jupyter Notebook mode.", 
            "title": "Get the code"
        }, 
        {
            "location": "/getstarted/quick_start/#initialize-your-project", 
            "text": "If you're a new user, then you should already see a default Project named  quick-start  in your  Projects dashboard .   Alternatively, you can simply create a new Project named  quick-start  in the FloydHub Dashboard with these  instructions .  We'll need to link the Python model-training code with your  quick-start  Project on FloydHub. Use the  floyd init  command in the current directory of your terminal to initialize your Project.   $ floyd init quick-start\nProject  quick-start  initialized in the current directory  This tells FloydHub that all the Jobs you run from this local directory belong to your Project named  quick-start .", 
            "title": "Initialize your Project"
        }, 
        {
            "location": "/getstarted/quick_start/#running-your-first-job", 
            "text": "Running a job on FloydHub is simple - when you use the  floyd run  command, the  floyd-cli  tool syncs your code with FloydHub's servers and runs the command in the cloud.   FloydHub will run any command you provide as a new Job - even a Job as simple as listing your current directory with the  ls  command. However, FloydHub is specialized for running deep learning and data science processing commands.  For this tutorial, we'll run the  mnist_cnn.py  Python script on FloydHub's GPU servers to train our convolutional neural network model against the MNIST data.  $ floyd run --gpu --env tensorflow  python mnist_cnn.py \nSyncing code ...\nRUN ID                  NAME               \n----------------------  -------------------\nAKpnXqj9BEU6d8KhmygTyb  alice/quick-start/1\n\nTo view the logs enter:\n    floyd logs alice/quick-start/1  Congratulations! Your first job is now running on FloydHub's GPU servers. Behind the scenes, FloydHub does the following:   Syncs your local code to FloydHub's servers  Provisions a GPU instance on the cloud (because you set the  --gpu  flag)  Sets up a deep learning environment with GPU drivers and Tensorflow installed (because you set the enviroment flag to  --env tensorflow )  Executes the command  python mnist_cnn.py  inside this environment  Stores the output logs and generated output data  Terminates the GPU instance once the command finishes execution", 
            "title": "Running your first Job"
        }, 
        {
            "location": "/getstarted/quick_start/#monitoring-your-job", 
            "text": "You can view the status of your Job from your terminal using the  floyd status  command. You can specify a single Job name (e.g.  floyd status alice/quick-start/1 ) to get its status, or the  floyd-cli  will show the status of all Jobs in the current Project.  $ floyd status\nRUN ID                  CREATED        STATUS    DURATION ( s )   NAME                 INSTANCE    DESCRIPTION\n----------------------  ---------      --------  -----------  -------------------  ---------   -----------\nAKpnXqj9BEU6d8KhmygTyb  just now       running             15   alice/quick-start:1  gpu           You can also view the status of your job in your browser by visiting the  Job URL  printed by the  floyd run  command. For example,  https://www.floydhub.com/alice/quick-start/1", 
            "title": "Monitoring your Job"
        }, 
        {
            "location": "/getstarted/quick_start/#viewing-your-jobs-logs", 
            "text": "It's easy to view the logs generated by the job from your terminal with the  floyd logs  command. You'll need to specify the Job name in the command.  $ floyd logs -t alice/quick-start/1\n... 2017 -07-12  16 :00:07,446 INFO - Starting attempt  1  at  2017 -07-12  16 :00:07.436349 2017 -07-12  16 :00:09,088 INFO - Starting container... 2017 -07-12  16 :00:09,297 INFO - \n... ##############################################################################  2017 -07-12  16 :00:09,297 INFO - Run Output: 2017 -07-12  16 :01:46,154 INFO - Successfully downloaded train-images-idx3-ubyte.gz  9912422  bytes. 2017 -07-12  16 :01:46,158 INFO - Iter  1280 , Minibatch  Loss =   39855 .289062, Training  Accuracy =   0 .17969 2017 -07-12  16 :01:46,159 INFO - Iter  2560 , Minibatch  Loss =   14964 .132812, Training  Accuracy =   0 .42969\n... ############################################################################## \n...  The output of your code is printed in the  Run Output  section of the logs, between the  #########  lines. Anything you log or print in your code will appear here, so this is a great way to monitor the progress of your model training command. In our  quick-start  project, we're logging the Training Accuracy of our model.  Using the  -t  (tail) flag will stream the logs as they are generated.  You can also view the logs in your browser using your  Job URL . However, the logs in the Dashboard are not currently refreshed dynamically, so you'll need to refresh your browser periodically or press  F5  to get the latest logs from the Dashboard.", 
            "title": "Viewing your Job's logs"
        }, 
        {
            "location": "/getstarted/quick_start/#viewing-the-job-output", 
            "text": "The model that we trained in this quickstart tutorial does not save any new output models - instead it simply prints the model results to the logs. We'll explore how to save model outputs in the Jupyter Notebook tutorial.", 
            "title": "Viewing the Job output"
        }, 
        {
            "location": "/getstarted/quick_start/#iterating-on-your-project", 
            "text": "Congratulations! You've run your first job on FloydHub \ud83c\udf89  At this point, you can edit your Python code locally to make improvements or adjustments, and then kick off a new Job with the  floyd run  command. The  floyd-cli  will upload the newest versions of your code and submit another Job to the FloydHub servers. Along the way, FloydHub will be managing and tracking of all the iterations of Jobs within your Project.  You can always view details on all of the Jobs in your current Project with the  floyd status  command from your terminal, or by visiting the  Project URL  in your browser.  Example:  www.floydhub.com/alice/quick-start", 
            "title": "Iterating on your Project"
        }, 
        {
            "location": "/getstarted/quick_start/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/", 
            "text": "In this tutorial, we will run a Python \nJupyter Notebook\n on FloydHub. Notebooks allow you to create and share documents that contain live code, visualizations and explanatory texts. This is an \nexample Notebook\n. It is great for interactively writing and debugging your code and visualizing your results and data. \n\n\nSimilar to the \nQuick Start guide\n, we will train a CNN model for handwritten digit recognition using PyTorch and the MNIST database.\n\n\nIf you are new to FloydHub, please ensure you have followed the \nQuick Start guide\n first. It introduces some important concepts used in this tutorial.\n\n\nWhat we will accomplish in this guide\n\n\n\n\nLearn how to create a new project on FloydHub\n\n\nStart a Jupyter notebook on FloydHub's GPU server\n\n\nInteractively run and debug your code\n\n\nMount datasets to use in your code\n\n\n\n\nQuick preparation checklist\n\n\n\n\nYou must have a \nFloydHub account\n\n\nYou must have \nfloyd-cli\n \ninstalled on your computer\n\n\nYou must \nlog in to FloydHub through the CLI\n\n\n\n\nSetup\n\n\nCreate a new project\n\n\nFor this tutorial, we will create a new Project. This project will be a collection of the jobs you run and their data, logs and results.\n\n\nTo create a new Project, visit \nwww.floydhub.com/projects\n and click on the \"New Project\" button on the top right hand corner.\n\n\n\n\nWe will name this project \nmnist-pytorch\n. Feel free to provide an apt description.\n\n\nThe \nVisibility\n field indicates who can see your project. If you set it to \nPublic\n, anyone can see your project, your code and data. If you are working on an open source project, this is a great way to share and contribute to the FloydHub community. If your code or data is proprietary, please select \nPrivate\n. This will ensure that only you and your team will have access to this project.\n\n\nGet the code\n\n\nWe will clone the \nquick-start repository\n from Github to your local machine and run it on FloydHub. Run the \ngit clone\n command in a brand new directory on your computer:\n\n\n$ git clone https://github.com/floydhub/quick-start-pytorch.git\nCloning into \nquick-start-pytorch\n...\n$ \ncd\n quick-start-pytorch\n\n\n\n\n$ ls\n$ README.md mnist.ipynb\n\n\n\n\nIn this guide, we will use the \nmnist_cnn.ipynb\n Jupyter Notebook.\n\n\nInitialize new project\n\n\nNow that we have the code, we want to associate this directory with the new project you just created on FloydHub. Ensure that you are inside the \nquick-start-pytorch\n directory and execute:\n\n\n$ floyd init mnist-pytorch\nProject \nmnist-pytorch\n initialized in the current directory\n\n\n\n\nThis tells Floyd that all the jobs run from this directory belong to the same project.\n\n\nRunning Jupyter Notebook on FloydHub\n\n\nStarting a Jupyter Notebook on FloydHub is very simple. Use the \nfloyd run\n command with \n--mode jupyter\n flag.\n\n\nExecute the following command from the command line:\n\n\n$ floyd run --mode jupyter --gpu --env pytorch\nCreating project run. Total upload size: \n21\n.9KiB\nSyncing code ...\n\n[================================]\n \n23333\n/23333 - \n00\n:00:00\nRUN ID                  NAME\n----------------------  ---------------------\nMhDNgxBHi74EKaffBKSbTN  saip/mnist-pytorch/3\n\nSetting up your instance and waiting \nfor\n Jupyter notebook to become available ..............\n\nPath to jupyter notebook: https://www.floydhub.com/notebooks/pCoPyzZtYeo6mE9PpSWsmY\n\nTo view logs enter:\n    floyd logs saip/mnist-pytorch/3\n\nOpening the jupyter notebook in your browser now ...\n\n\n\n\nThis will take a little bit. As it executes, Floyd is doing the following behind the scenes:\n\n\n\n\nSync your local code to FloydHub's server\n\n\nProvision a GPU instance on the cloud (if you want CPU, drop the \n--gpu\n flag)\n\n\nSet up an deep learning environment with PyTorch installed (because \n--env pytorch\n)\n\n\nStart a Jupyter server on the cloud, and open the url in your browser\n\n\n\n\nYou can also open the link to the your Jupyter dashboard using the displayed URL. For example:\n\n\n\n\nOpen the \nmnist.ipynb\n Notebook and start training your model interactively!\n\n\nNext steps\n\n\nCheck the status of your job\n\n\nYou can view the status of your job from your terminal using the \nfloyd status\n command \n\n\n$ floyd status saip/mnist-pytorch/3\nRUN ID                  CREATED         STATUS      DURATION\n(\ns\n)\n  NAME                   INSTANCE    DESCRIPTION\n----------------------  --------------  --------  -------------  ---------------------  ----------  -------------\nMhDNgxBHi74EKaffBKSbTN  \n16\n minutes ago  running               \n0\n  saip/mnist-pytorch/3   gpu\n\n\n\n\nYou can also view the status by going to the project page in the web dashboard.\n\n\nStopping your Notebook\n\n\n\n\nWarning\n\n\nJupyter Notebooks are designed for interactive development. Your job starts running on FloydHub's server when you execute the \nfloyd run --mode jupyter\n command and it continues to be active till you explicitly stop your job.\n\n\nHence, even if you are not actively executing code inside your Notebook, the Jupyter server is still active on FloydHub and you are billed for the time.\n\n\n\n\nTo stop your notebook you can use the \nfloyd stop\n command.\n\n\n$ floyd stop saip/mnist-pytorch/3\nExperiment shutdown request submitted. Check status to confirm shutdown\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Jupyter Notebook Quick Start"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#what-we-will-accomplish-in-this-guide", 
            "text": "Learn how to create a new project on FloydHub  Start a Jupyter notebook on FloydHub's GPU server  Interactively run and debug your code  Mount datasets to use in your code", 
            "title": "What we will accomplish in this guide"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#quick-preparation-checklist", 
            "text": "You must have a  FloydHub account  You must have  floyd-cli   installed on your computer  You must  log in to FloydHub through the CLI", 
            "title": "Quick preparation checklist"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#setup", 
            "text": "", 
            "title": "Setup"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#create-a-new-project", 
            "text": "For this tutorial, we will create a new Project. This project will be a collection of the jobs you run and their data, logs and results.  To create a new Project, visit  www.floydhub.com/projects  and click on the \"New Project\" button on the top right hand corner.   We will name this project  mnist-pytorch . Feel free to provide an apt description.  The  Visibility  field indicates who can see your project. If you set it to  Public , anyone can see your project, your code and data. If you are working on an open source project, this is a great way to share and contribute to the FloydHub community. If your code or data is proprietary, please select  Private . This will ensure that only you and your team will have access to this project.", 
            "title": "Create a new project"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#get-the-code", 
            "text": "We will clone the  quick-start repository  from Github to your local machine and run it on FloydHub. Run the  git clone  command in a brand new directory on your computer:  $ git clone https://github.com/floydhub/quick-start-pytorch.git\nCloning into  quick-start-pytorch ...\n$  cd  quick-start-pytorch  $ ls\n$ README.md mnist.ipynb  In this guide, we will use the  mnist_cnn.ipynb  Jupyter Notebook.", 
            "title": "Get the code"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#initialize-new-project", 
            "text": "Now that we have the code, we want to associate this directory with the new project you just created on FloydHub. Ensure that you are inside the  quick-start-pytorch  directory and execute:  $ floyd init mnist-pytorch\nProject  mnist-pytorch  initialized in the current directory  This tells Floyd that all the jobs run from this directory belong to the same project.", 
            "title": "Initialize new project"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#running-jupyter-notebook-on-floydhub", 
            "text": "Starting a Jupyter Notebook on FloydHub is very simple. Use the  floyd run  command with  --mode jupyter  flag.  Execute the following command from the command line:  $ floyd run --mode jupyter --gpu --env pytorch\nCreating project run. Total upload size:  21 .9KiB\nSyncing code ... [================================]   23333 /23333 -  00 :00:00\nRUN ID                  NAME\n----------------------  ---------------------\nMhDNgxBHi74EKaffBKSbTN  saip/mnist-pytorch/3\n\nSetting up your instance and waiting  for  Jupyter notebook to become available ..............\n\nPath to jupyter notebook: https://www.floydhub.com/notebooks/pCoPyzZtYeo6mE9PpSWsmY\n\nTo view logs enter:\n    floyd logs saip/mnist-pytorch/3\n\nOpening the jupyter notebook in your browser now ...  This will take a little bit. As it executes, Floyd is doing the following behind the scenes:   Sync your local code to FloydHub's server  Provision a GPU instance on the cloud (if you want CPU, drop the  --gpu  flag)  Set up an deep learning environment with PyTorch installed (because  --env pytorch )  Start a Jupyter server on the cloud, and open the url in your browser   You can also open the link to the your Jupyter dashboard using the displayed URL. For example:   Open the  mnist.ipynb  Notebook and start training your model interactively!", 
            "title": "Running Jupyter Notebook on FloydHub"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#next-steps", 
            "text": "", 
            "title": "Next steps"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#check-the-status-of-your-job", 
            "text": "You can view the status of your job from your terminal using the  floyd status  command   $ floyd status saip/mnist-pytorch/3\nRUN ID                  CREATED         STATUS      DURATION ( s )   NAME                   INSTANCE    DESCRIPTION\n----------------------  --------------  --------  -------------  ---------------------  ----------  -------------\nMhDNgxBHi74EKaffBKSbTN   16  minutes ago  running                0   saip/mnist-pytorch/3   gpu  You can also view the status by going to the project page in the web dashboard.", 
            "title": "Check the status of your job"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#stopping-your-notebook", 
            "text": "Warning  Jupyter Notebooks are designed for interactive development. Your job starts running on FloydHub's server when you execute the  floyd run --mode jupyter  command and it continues to be active till you explicitly stop your job.  Hence, even if you are not actively executing code inside your Notebook, the Jupyter server is still active on FloydHub and you are billed for the time.   To stop your notebook you can use the  floyd stop  command.", 
            "title": "Stopping your Notebook"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#floyd-stop-saipmnist-pytorch3-experiment-shutdown-request-submitted-check-status-to-confirm-shutdown", 
            "text": "", 
            "title": "$ floyd stop saip/mnist-pytorch/3"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/basics/create_new/", 
            "text": "A \nProject\n is a collection of the jobs you run along with their logs and\nresults. If you have used GitHub, projects in FloydHub are a lot like code\nrepositories.\n\n\nTo create a new Project, visit \nwww.floydhub.com/projects\n and click on the \"New Project\" button on the top right hand corner.\n\n\n\n\nGive the project a name and an apt description.\n\n\nThe \nVisibility\n field indicates who can see your project. If you set it to \nPublic\n, anyone can see your project, your code and data. If you are working on an open source project, this is a great way to share and contribute to the FloydHub community. If your code or data is proprietary, please select \nPrivate\n. This will ensure that only you and your team will have access to this project.\n\n\nOnce you have created a Project, you can start running jobs using the \nfloyd run\n command. For example, to start a Jupyter Notebook job:\n\n\n$ floyd init quick-start\nProject \nquick-start\n initialized in the current directory\n\n$ floyd run --gpu --env tensorflow --mode jupyter\nSyncing code ...\n\n\n\n\nAdding a Project README\n\n\nFloydHub will display a README file for your project, if you include a README file in your local code directory for a Project when you run a job. A good README file will help people on FloydHub understand your project, why it's useful, and how they can run the project.\n\n\nTo add a README to your Project, simply add a Markdown-styled text file to your project called \nREADME.md\n and FloydHub will automatically display that README file when you run your next job.", 
            "title": "Create a New Project"
        }, 
        {
            "location": "/guides/basics/create_new/#adding-a-project-readme", 
            "text": "FloydHub will display a README file for your project, if you include a README file in your local code directory for a Project when you run a job. A good README file will help people on FloydHub understand your project, why it's useful, and how they can run the project.  To add a README to your Project, simply add a Markdown-styled text file to your project called  README.md  and FloydHub will automatically display that README file when you run your next job.", 
            "title": "Adding a Project README"
        }, 
        {
            "location": "/guides/basics/delete/", 
            "text": "You can delete a project by clicking \nDelete project\n button on the Settings tab of the project on the web dashboard.\n\n\nExample: \nhttps://www.floydhub.com/alice/projects/quick-start/settings\n\n\n\n\n\n\n\n\nImportant\n\n\nDeleting a project will delete all its jobs and their corresponding code,\noutput data and logs. This \ncannot\n be restored. Please be absolutely\nsure you want to delete a project before proceeding.\n\n\n\n\nWe recommend deleting individual jobs rather than projects.", 
            "title": "Delete a Project"
        }, 
        {
            "location": "/guides/delete_job/", 
            "text": "You can delete an individual job by clicking on \nDelete job\n button on the Settings tab of the job's page on the web dashboard.\n\n\nExample: \nhttps://www.floydhub.com/alice/projects/quick-start/1/settings\n\n\n\n\nDeleting a job using the CLI\n\n\nYou can also delete a job from the CLI using the \nfloyd delete\n command.\n\n\n$ floyd delete alice/projects/quick-start/1\n\nDelete Run: alice/quick-start/1? \n[\ny/N\n]\n: y\nJob BD4JMXSgCi2r2afbq3n3Vo: Deleted\n\n\n\n\nDeleting output of a Job\n\n\nIt is not possible to delete just the output of a job. You will have to delete the job itself.", 
            "title": "Delete a Job"
        }, 
        {
            "location": "/guides/delete_job/#deleting-a-job-using-the-cli", 
            "text": "You can also delete a job from the CLI using the  floyd delete  command.  $ floyd delete alice/projects/quick-start/1\n\nDelete Run: alice/quick-start/1?  [ y/N ] : y\nJob BD4JMXSgCi2r2afbq3n3Vo: Deleted", 
            "title": "Deleting a job using the CLI"
        }, 
        {
            "location": "/guides/delete_job/#deleting-output-of-a-job", 
            "text": "It is not possible to delete just the output of a job. You will have to delete the job itself.", 
            "title": "Deleting output of a Job"
        }, 
        {
            "location": "/guides/jobs/tensorboard/", 
            "text": "Tensorboard\n \nis a visualization tool for Tensorflow projects. Tensorboard can help \nvisualize the Tensorflow computation graph and plot quantitative metrics about your run. This \nguide will help you understand how to enable Tensorboard in your jobs.\n\n\nKey concepts of Tensorboard\n\n\nIf you would like to know more about the concepts of Tensorboard please check out\nthe \nTensorboard README\n\nfile. This page also goes into the details of Tensorboard and explains the various \ndashboards that are present in the Tensorboard UI.\n\n\nEnabling Tensorboard in your job\n\n\nTo enable Tensorboard in your job, you need to specify a \n--tensorboard\n flag \nwhen you run the job. Tensorboard can be enabled for both CLI jobs and when running \nJupyter notebooks.\n\n\nExample\n\n\nThis code snipped will train an MNIST model and also store the training summary \nto a log directory.\n\n\ngit clone https://github.com/floydhub/tensorflow-examples\n\ncd\n tensorflow-examples/tensorboard\n\n\n# Initialize the current directory to an existing or new project\n\nfloyd init mnist-tensorboard\nfloyd run --tensorboard \npython mnist_tensorboard.py --log_dir /output/mnist --max_steps 5000\n\n\n\n\n\n\n\nNotice that the the \nlog_dir\n parameter is set to a path in the \n/output\n directory.\nOn Floydhub, \n/output\n is a special directory that Tensorboard watches. Be sure to send \nall data meant for Tensorboard to any directory under \n/output\n path.\n\n\n\n\nNow you can view the job on your Project dashboard.\n\n\n\n\nClick on the job that was just started. You will notice that the job page now has a link \nto Tensorboard. Click on it to open the Tensorboard dashboard in a new tab.\n\n\n\n\nTensorboard Dashboard\n\n\n\n\nYou can see that the \"SCALARS\" tab of Tensorboard is logging the accuracy of the \ntraining and test data along with some other values. You may need to click on the title \nbars (like \naccuracy_1\n) for the graph to open.\n\n\nThe reason why these values are appearing on the dashboard is because the \n\nmnist_tensorboard.py\n code has the following lines:\n\n\ntf\n.\nsummary\n.\nscalar\n(\ncross_entropy\n,\n \ncross_entropy\n)\n\n\n...\n\n\ntf\n.\nsummary\n.\nscalar\n(\naccuracy\n,\n \naccuracy\n)\n\n\n\n\n\nYou can read more about how to use Tensorboard to log additional information in \nthe \nTensorboard README\n.\n\n\nExplore the other tabs in the Tensorboad dashboard like \"IMAGES\" and \"GRAPHS\".\n\n\nTensorboard Tabs\n\n\n\n\nThe IMAGES dashboard shows the transformations happening to the mnist images\nin real time while the training is happening.\n\n\n\n\nThe GRAPHS dashboard shows a representation of Tensorflow's computation graph.\nYou can click into each part of the model to get more details.\n\n\n\n\nTensorboard feature is only available for Tensorflow environments. \nSee \nthis\n page for full list of Tensorflow environments you \ncan use.\n\n\n\n\nStopping Tensorboard\n\n\nTensorboard runs in the same machine where your code is running. So you do not have \nto stop it explicitly. It will be up until your job finishes and then stop automatically. \nTensorboard will become inaccessible when the job finishes in any of the \nSuccess\n, \nFailed\n, \n\nTimeout\n or \nShutdown\n states.\n\n\nTensorboard in Jupyter mode\n\n\nTensorboard can be run in Jupyter mode as well. You will notice that the links for both \nthe Jupyter notebook and the Tensorboard appear in the Job page.\n\n\n\n\nOffline Training\n\n\nUntil now, we saw how to use Tensorboard directly on Floydhub \nwhile\n your job is actively running. \nAlternatively you can also view the metrics offline after your \ntraining is done.\n\n\nFor that, you need to first download the output of your project to your local \nmachine.\n\n\nmkdir tensorboard_output \n \ncd\n tensorboard_output\nfloyd data clone floydhub/mnist-tensorboard/6/output\n\n\n\n\nThen you need to install tensorflow in your local machine. The instructions depend \non your OS. See the Tensorflow install instructions \nhere\n.\n\n\nAfter that you can just run the \ntensorboard\n command and point it to the output \ndirectory downloaded from Floydhub.\n\n\ntensorboard --logdir\n=\ntensorboard_output\n\n\n\n\nThen you can view the Tensorboard dashboard on your machine running at \n\nhttp://127.0.0.1:6006/", 
            "title": "Enable Tensorboard"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#key-concepts-of-tensorboard", 
            "text": "If you would like to know more about the concepts of Tensorboard please check out\nthe  Tensorboard README \nfile. This page also goes into the details of Tensorboard and explains the various \ndashboards that are present in the Tensorboard UI.", 
            "title": "Key concepts of Tensorboard"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#enabling-tensorboard-in-your-job", 
            "text": "To enable Tensorboard in your job, you need to specify a  --tensorboard  flag \nwhen you run the job. Tensorboard can be enabled for both CLI jobs and when running \nJupyter notebooks.", 
            "title": "Enabling Tensorboard in your job"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#example", 
            "text": "This code snipped will train an MNIST model and also store the training summary \nto a log directory.  git clone https://github.com/floydhub/tensorflow-examples cd  tensorflow-examples/tensorboard # Initialize the current directory to an existing or new project \nfloyd init mnist-tensorboard\nfloyd run --tensorboard  python mnist_tensorboard.py --log_dir /output/mnist --max_steps 5000    Notice that the the  log_dir  parameter is set to a path in the  /output  directory.\nOn Floydhub,  /output  is a special directory that Tensorboard watches. Be sure to send \nall data meant for Tensorboard to any directory under  /output  path.   Now you can view the job on your Project dashboard.   Click on the job that was just started. You will notice that the job page now has a link \nto Tensorboard. Click on it to open the Tensorboard dashboard in a new tab.", 
            "title": "Example"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#tensorboard-dashboard", 
            "text": "You can see that the \"SCALARS\" tab of Tensorboard is logging the accuracy of the \ntraining and test data along with some other values. You may need to click on the title \nbars (like  accuracy_1 ) for the graph to open.  The reason why these values are appearing on the dashboard is because the  mnist_tensorboard.py  code has the following lines:  tf . summary . scalar ( cross_entropy ,   cross_entropy )  ...  tf . summary . scalar ( accuracy ,   accuracy )   You can read more about how to use Tensorboard to log additional information in \nthe  Tensorboard README .  Explore the other tabs in the Tensorboad dashboard like \"IMAGES\" and \"GRAPHS\".", 
            "title": "Tensorboard Dashboard"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#tensorboard-tabs", 
            "text": "The IMAGES dashboard shows the transformations happening to the mnist images\nin real time while the training is happening.   The GRAPHS dashboard shows a representation of Tensorflow's computation graph.\nYou can click into each part of the model to get more details.   Tensorboard feature is only available for Tensorflow environments. \nSee  this  page for full list of Tensorflow environments you \ncan use.", 
            "title": "Tensorboard Tabs"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#stopping-tensorboard", 
            "text": "Tensorboard runs in the same machine where your code is running. So you do not have \nto stop it explicitly. It will be up until your job finishes and then stop automatically. \nTensorboard will become inaccessible when the job finishes in any of the  Success ,  Failed ,  Timeout  or  Shutdown  states.", 
            "title": "Stopping Tensorboard"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#tensorboard-in-jupyter-mode", 
            "text": "Tensorboard can be run in Jupyter mode as well. You will notice that the links for both \nthe Jupyter notebook and the Tensorboard appear in the Job page.", 
            "title": "Tensorboard in Jupyter mode"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#offline-training", 
            "text": "Until now, we saw how to use Tensorboard directly on Floydhub  while  your job is actively running. \nAlternatively you can also view the metrics offline after your \ntraining is done.  For that, you need to first download the output of your project to your local \nmachine.  mkdir tensorboard_output    cd  tensorboard_output\nfloyd data clone floydhub/mnist-tensorboard/6/output  Then you need to install tensorflow in your local machine. The instructions depend \non your OS. See the Tensorflow install instructions  here .  After that you can just run the  tensorboard  command and point it to the output \ndirectory downloaded from Floydhub.  tensorboard --logdir = tensorboard_output  Then you can view the Tensorboard dashboard on your machine running at  http://127.0.0.1:6006/", 
            "title": "Offline Training"
        }, 
        {
            "location": "/guides/floyd_ignore/", 
            "text": "Floydignore is a special floyd cli construct that allows you to specify\nwhich files need to be uploaded to the server. This is very similar to\nhow gitignore works.\n\n\nMinimizing the number of files to upload saves upload time and disk space used\nby your experiments.\n\n\nInitialization\n\n\nEverytime a new dataset or project is \ninitialized\n using \nfloyd-cli\n a new\n\n.floydignore\n file is created in the current path.\n\n\n$ \ncd\n /code/project\n$ floyd init style-transfer\nProject \nstyle-transfer\n initialized in current directory\n$ cat .floydignore\n\n\n# Directories and files to ignore when uploading code to floyd\n\n\n.git\n.eggs\neggs\nlib\nlib64\nparts\nsdist\nvar\n\n\n\n\nNote: If a \n.floydignore\n file already exists in the initialization path, it will not be overridden.\n\n\nOptions\n\n\nThere are different ways to specify files, directories, or\n\nglob patterns\n that you want\nto be ignored. Below is a list with examples:\n\n\n# Ignore all .dat files in the whole project:\n*.dat\n\n# Ignore all .dat files in some_folder:\nsome_folder/*.dat\n\n# Ignore all .dat files in some_folder and its subfolders:\nsome_folder/**/*.dat\n\n# Ignore all files (and folders) named .DS_Store\n.DS_Store\n\n# Ignore a specific file named .DS_Store located in some_folder\nsome_folder/.DS_Store\n\n# Ignore all files named .DS_Store in some_folder and its subfolders\nsome_folder/**/.DS_Store\n\n# Ignore all files in some_folder\n/some_folder/\n\n# Ignore all files in some_folder (works the same as the rule above)\nsome_folder/\n\n# Ignore all files in some_folder (works the same as the rule above)\nsome_folder\n\n\n\n\nYou can also whitelist files, directories, and glob patterns by preceding them\nwith a \n!\n.  Items matching the pattern following the \n!\n that were excluded by\na previous pattern will become included again. It is not possible to re-include\na file if a parent directory of that file is excluded. Put a backslash (\n\\\n) in\nfront of the first \n!\n for patterns that begin with a literal \n!\n, for example,\n\n\\!important!.txt\n.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Ignore Files"
        }, 
        {
            "location": "/guides/floyd_ignore/#initialization", 
            "text": "Everytime a new dataset or project is  initialized  using  floyd-cli  a new .floydignore  file is created in the current path.  $  cd  /code/project\n$ floyd init style-transfer\nProject  style-transfer  initialized in current directory\n$ cat .floydignore # Directories and files to ignore when uploading code to floyd \n\n.git\n.eggs\neggs\nlib\nlib64\nparts\nsdist\nvar  Note: If a  .floydignore  file already exists in the initialization path, it will not be overridden.", 
            "title": "Initialization"
        }, 
        {
            "location": "/guides/floyd_ignore/#options", 
            "text": "There are different ways to specify files, directories, or glob patterns  that you want\nto be ignored. Below is a list with examples:  # Ignore all .dat files in the whole project:\n*.dat\n\n# Ignore all .dat files in some_folder:\nsome_folder/*.dat\n\n# Ignore all .dat files in some_folder and its subfolders:\nsome_folder/**/*.dat\n\n# Ignore all files (and folders) named .DS_Store\n.DS_Store\n\n# Ignore a specific file named .DS_Store located in some_folder\nsome_folder/.DS_Store\n\n# Ignore all files named .DS_Store in some_folder and its subfolders\nsome_folder/**/.DS_Store\n\n# Ignore all files in some_folder\n/some_folder/\n\n# Ignore all files in some_folder (works the same as the rule above)\nsome_folder/\n\n# Ignore all files in some_folder (works the same as the rule above)\nsome_folder  You can also whitelist files, directories, and glob patterns by preceding them\nwith a  ! .  Items matching the pattern following the  !  that were excluded by\na previous pattern will become included again. It is not possible to re-include\na file if a parent directory of that file is excluded. Put a backslash ( \\ ) in\nfront of the first  !  for patterns that begin with a literal  ! , for example, \\!important!.txt .", 
            "title": "Options"
        }, 
        {
            "location": "/guides/floyd_ignore/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/ssh/", 
            "text": "Quick Look\n\n\n$ floyd run --mode jupyter\n\n\n\n\n\n\n\n\nYou can start a bash session into your job's environment by running your job in\nJupyter Notebook mode:\n\n\n$ floyd run --mode jupyter\nCreating project run. Total upload size: \n183\n.0B\nSyncing code ...\n\n[================================]\n \n916\n/916 - \n00\n:00:00\n\nJOB NAME\n--------------------\nmckay/projects/ssh/1\n\nSetting up your instance and waiting \nfor\n Jupyter notebook to become available\n\nPath to jupyter notebook: https://floydhub.com/notebooks/YXau92xMFUshUbMdKqKVVX\n\n\n\n\nWhen you visit your running Jupyter Notebook in the browser, click the \nNew\n\nbutton in the top right corner of the screen and select the \nTerminal\n option\nas shown below:\n\n\n\n\nThat button will launch a terminal session in your running instance with root\naccess:\n\n\n\nCurrently, FloydHub does not offer true SSH access into instances, but the\nmethod described above is sufficient for what most users request SSH for.", 
            "title": "SSH into a Job"
        }, 
        {
            "location": "/faqs/job/", 
            "text": "Why does \nfloyd status\n return an empty list even though I have several\n\n\nruns in my account?\n\n\nFloyd CLI uses the project directory to store the run information (similar to git). So you need\nto be in the directory where you initialized the project and you should be able to see all your\nruns. You can also use the \nweb dashboard\n to view all your\nprojects in one place.\n\n\nWhat do I do when I get \"What do you do when you get \u201cYou are over the allowed limits for this operation. Consider upgrading your account\u201d?\n\n\nFloydhub currently allows only 1 active job per user for trial plan and 3 active jobs for individual plan. If you require more concurrency, contact\nus from the \npricing\n page.\n\n\nI get \"Too many open files\" error when I run my project.\n\n\nFloyd CLI throws this error when you have too many files in your current directory that needs to be uploaded.\nThe actual limit depends on your OS / machine specs.\n\n\nYou can either:\n\n\n\n\nRemove unnecessary files from the directory (like build directory, docs etc.)\n\n\nAdd them to \n.floydignore\n file. Floyd CLI will just ignore these directories.\nSee the \nfloydignore\n documentation to understand how this can be configured.\n\n\nTar them into a single file and untar them at runtime.\n\n\n\n\nAlternatively, instead of uploading files from your local machine, you can also\n\ndownload files\n from a remote URL\ndirectly into Floyd servers.\n\n\nWhy do I get an \"Experiments limit reached\" error when I run a job?\n\n\nFloydHub currently allows only 1 active job in the free Trial plan and 3 active jobs in the Individual plan. \nIf you see an \nError\n:\n \nExperiments\n \nlimit\n \nreached\n message when you run a job, it means you have maxed out your \nconcurrency limits. Please stop you running job(s) or wait for them to finish, and try again.\n\n\nWe have to enforce this concurrency constraint because we have a finite number of GPU machines and have to ensure that no single user is starving the group. In the near future, we will support queueing of jobs so that you can queue multiple jobs to be run as slots become available.\n\n\nI ran my project in Jupyter mode but the url does not seem to work.\n\n\nJupyter notebook server takes a couple of minutes to start. Until then you will get a \"Bad Gateway\"\nor similar error when you access the URL. You can check the status of the Jupyter notebook\nby running the \nlogs\n command.\n\n\nAm I using the GPU instance by default?\n\n\nJobs are run on CPU instances by default. You can specify \n--gpu\n to run them on GPU instances.\n\n\nMy job is taking a while to \"sync changes\". How do I make it go faster?\n\n\nFloyd CLI uploads \nall\n the files in your current directory before starting your experiment.\nThere are a few ways to make this go faster:\n\n\n\n\nRemove unnecessary files from the directory (like build directory, docs etc.)\n\n\nAdd sub-directories to \n.floydignore\n file. Floyd CLI will ignore and not upload these sub-directories.\nSee the \ninit\n command and \nignore files guide\n to understand how this can be configured.\n\n\nIf you have large data files consider uploading them separately as a \ndata source\n.\nYou can then \nrefer\n to them in your project.\n\n\n\n\nMy job finished but how I do I see my output?\n\n\nYou can use the floyd \noutput\n command to view the output of your\nproject. If you want to use this output in your next run view \nthis guide\n.\n\n\nDo I have to pay for the entire time my Jupyter Notebook is running?\n\n\nUnfortunately, yes. As much as we would like to, we are unable to charge you only for the \ncomputation time\n.\n\n\nThis is an engineering challenge. When you start a Jupyter Notebook instance and start executing commands,\nyour state is maintained in memory (both CPU and GPU memory). So the instance has to be alive for the\nentire duration of your notebook, not just when you are executing commands.\n\n\nFor example, when you execute \nimport\n \ntensorflow\n\ncommand, Tensorflow will allocate the entire GPU memory to the current session and waits for the next command. This makes the instance unusable by anyone else, so we have to charge you for the duration your Notebook is alive.\n\n\nCan I view my Jupyter Notebook after my job has stopped?\n\n\nYes. When you use a Jupyter Notebook on FloydHub, your Notebook is saved periodically in the \n/output\n dir. So, your work is not lost after your job has ended, shutdown or timed out.\n\n\nYou can view your saved Notebook using the \nfloyd output\n command. Example:\n\n\n$ floyd output saip/projects/mnist-pytorch/3\n\n\n\n\nOr in the \nOutput\n tab of your job on the web dashboard, example: \nwww.floydhub.com/saip/projects/mnist-pytorch/3/output\n\n\nCan I restart a stopped or timed out job?\n\n\nUnfortunately, not directly. We will be implementing a single command to do this soon!\n\n\nIn the meanwhile, you can follow these steps to do this manually:\n\n\n\n\nJupyter Notebook\n: Your Notebook is \nsaved periodically\n. To restart your Notebook after your job has stopped, please download the output of your stopped job to your machine and start another job. Example:\n\n\n\n\n# Download the saved Notebook from previous job\n\n\n# NOTE: This will overwrite the contents of your current dir\n\n$ floyd data clone saip/projects/mnist-pytorch/3/output\n\n\n# Start a new job\n\n$ floyd run --mode jupyter\n\n\n\n\n\n\nScript\n: If you are running a script/command, you will have to start a new job using the \nfloyd run \ncommand\n command.\n\n\n\n\nWhy is my job in the \"Queued\" state for several minutes?\n\n\nThis means that a machine is being prepared to run your job. \n\n\nMost times, we have several CPU and GPU machines that are ready and your job can start execution in a few seconds. During high traffic periods, we may not have a machine ready for you and have to spin up a new instance for your job on-demand (details below). This might take up to 10 minutes in some cases. We are actively working on reducing this wait time.\n\n\nDetails\n: When you execute a \nfloyd run\n command, Floyd does several things in the background:\n\n\n\n\nProvision a CPU or GPU instance on the cloud\n\n\nSet up a deep learning environment with GPU drivers and the correct environment (as specified by \n--env\n) installed using Docker\n\n\nMount any data you specify using the \n--data\n flag\n\n\nSpin up a Jupyter server, if \n--mode jupyter\n flag\n\n\n\n\nEach of these steps can take up to a couple of minutes. Usually Steps 1 and 2 are already done, but during peak usage hours, we might have to do this on-demand.\n\n\nWhy do I see \"Setting up your instance...\" for several minutes when running a Jupyter Notebook?\n\n\nThe \nSetting up your instance...\n message is displayed when a machine is being prepared to run your Jupyter Notebook.\n\n\n\n\nWhen you execute a \nfloyd run --mode jupyter\n command, the CLI waits for a CPU or GPU machine to be ready before it opens up an interactive Jupyter Notebook for your work on. Usually, this takes a few seconds, but during high traffic periods it can take up to 10 minutes in some cases.\n\n\nFor more details on why it takes time, please see \nWhy is my job in the \"Queued\" state for several minutes?\n\n\nWhy are my logs not displayed in real-time?\n\n\nYou can stream your logs from the CLI using the \nfloyd logs -t \nJOB_NAME\n command. However, sometimes you may notice that the logs are not displayed in real-time. This is because of output buffering. Please make sure that your logs are flushed out if you prefer to view real-time logs.\n\n\nFor example, in Python:\n\n\nimport\n \nsys\n\n\n...\n\n\nprint\n(\nHello world\n)\n\n\nsys\n.\nstdout\n.\nflush\n()\n\n\n\n\n\nWhy did my job timeout after 1 hour?\n\n\nYou are likely in the Free Trial Plan. Jobs run in the trial plan have a maximum runtime of 1 hour. It will automatically timeout after that. \n\n\n\n\nYou can upgrade to the \nPaid Plan\n to overcome these limits.\n\n\nWhy was my CPU job Killed without warning?\n\n\nOccasionally, you may notice that your CPU job died without warning. The output logs just display \nKilled\n. For example,\n\n\n################################################################################\n\n\n\n2017\n-07-24 \n03\n:33:42,530 INFO - Run Output:\n...\n\n2017\n-07-24 \n03\n:33:52,920 INFO - Using TensorFlow backend.\n\n2017\n-07-24 \n03\n:34:04,381 INFO - \n loading UNet of size 1152x256...\n\n2017\n-07-24 \n03\n:34:10,942 INFO - Epoch \n1\n/100\n\n2017\n-07-24 \n03\n:35:17,221 INFO - Killed\n\n2017\n-07-24 \n03\n:35:18,680 INFO - \n\n################################################################################\n\n\n\n\n\nThis happens when your machine runs out of memory (OOM). i.e. your job consumes more memory than is available on our CPU machines.\n\n\nAll jobs run on FloydHub are executed inside a Docker container. Our current CPU machines have \n7GB memory\n. When memory used by your job exceeds 7GB, Docker automatically kills the job to protect the system and avoid abuse.\n\n\nThe resolution is to optimize your code to consume less memory. For example, read less data into your in-memory datastructures or reduce your batch size. We will be introducing more powerful CPUs, which higher memory in the near future.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Troubleshooting & FAQs"
        }, 
        {
            "location": "/faqs/job/#why-does-floyd-status-return-an-empty-list-even-though-i-have-several", 
            "text": "runs in my account?  Floyd CLI uses the project directory to store the run information (similar to git). So you need\nto be in the directory where you initialized the project and you should be able to see all your\nruns. You can also use the  web dashboard  to view all your\nprojects in one place.", 
            "title": "Why does floyd status return an empty list even though I have several"
        }, 
        {
            "location": "/faqs/job/#what-do-i-do-when-i-get-what-do-you-do-when-you-get-you-are-over-the-allowed-limits-for-this-operation-consider-upgrading-your-account", 
            "text": "Floydhub currently allows only 1 active job per user for trial plan and 3 active jobs for individual plan. If you require more concurrency, contact\nus from the  pricing  page.", 
            "title": "What do I do when I get \"What do you do when you get \u201cYou are over the allowed limits for this operation. Consider upgrading your account\u201d?"
        }, 
        {
            "location": "/faqs/job/#i-get-too-many-open-files-error-when-i-run-my-project", 
            "text": "Floyd CLI throws this error when you have too many files in your current directory that needs to be uploaded.\nThe actual limit depends on your OS / machine specs.  You can either:   Remove unnecessary files from the directory (like build directory, docs etc.)  Add them to  .floydignore  file. Floyd CLI will just ignore these directories.\nSee the  floydignore  documentation to understand how this can be configured.  Tar them into a single file and untar them at runtime.   Alternatively, instead of uploading files from your local machine, you can also download files  from a remote URL\ndirectly into Floyd servers.", 
            "title": "I get \"Too many open files\" error when I run my project."
        }, 
        {
            "location": "/faqs/job/#why-do-i-get-an-experiments-limit-reached-error-when-i-run-a-job", 
            "text": "FloydHub currently allows only 1 active job in the free Trial plan and 3 active jobs in the Individual plan. \nIf you see an  Error :   Experiments   limit   reached  message when you run a job, it means you have maxed out your \nconcurrency limits. Please stop you running job(s) or wait for them to finish, and try again.  We have to enforce this concurrency constraint because we have a finite number of GPU machines and have to ensure that no single user is starving the group. In the near future, we will support queueing of jobs so that you can queue multiple jobs to be run as slots become available.", 
            "title": "Why do I get an \"Experiments limit reached\" error when I run a job?"
        }, 
        {
            "location": "/faqs/job/#i-ran-my-project-in-jupyter-mode-but-the-url-does-not-seem-to-work", 
            "text": "Jupyter notebook server takes a couple of minutes to start. Until then you will get a \"Bad Gateway\"\nor similar error when you access the URL. You can check the status of the Jupyter notebook\nby running the  logs  command.", 
            "title": "I ran my project in Jupyter mode but the url does not seem to work."
        }, 
        {
            "location": "/faqs/job/#am-i-using-the-gpu-instance-by-default", 
            "text": "Jobs are run on CPU instances by default. You can specify  --gpu  to run them on GPU instances.", 
            "title": "Am I using the GPU instance by default?"
        }, 
        {
            "location": "/faqs/job/#my-job-is-taking-a-while-to-sync-changes-how-do-i-make-it-go-faster", 
            "text": "Floyd CLI uploads  all  the files in your current directory before starting your experiment.\nThere are a few ways to make this go faster:   Remove unnecessary files from the directory (like build directory, docs etc.)  Add sub-directories to  .floydignore  file. Floyd CLI will ignore and not upload these sub-directories.\nSee the  init  command and  ignore files guide  to understand how this can be configured.  If you have large data files consider uploading them separately as a  data source .\nYou can then  refer  to them in your project.", 
            "title": "My job is taking a while to \"sync changes\". How do I make it go faster?"
        }, 
        {
            "location": "/faqs/job/#my-job-finished-but-how-i-do-i-see-my-output", 
            "text": "You can use the floyd  output  command to view the output of your\nproject. If you want to use this output in your next run view  this guide .", 
            "title": "My job finished but how I do I see my output?"
        }, 
        {
            "location": "/faqs/job/#do-i-have-to-pay-for-the-entire-time-my-jupyter-notebook-is-running", 
            "text": "Unfortunately, yes. As much as we would like to, we are unable to charge you only for the  computation time .  This is an engineering challenge. When you start a Jupyter Notebook instance and start executing commands,\nyour state is maintained in memory (both CPU and GPU memory). So the instance has to be alive for the\nentire duration of your notebook, not just when you are executing commands.  For example, when you execute  import   tensorflow \ncommand, Tensorflow will allocate the entire GPU memory to the current session and waits for the next command. This makes the instance unusable by anyone else, so we have to charge you for the duration your Notebook is alive.", 
            "title": "Do I have to pay for the entire time my Jupyter Notebook is running?"
        }, 
        {
            "location": "/faqs/job/#can-i-view-my-jupyter-notebook-after-my-job-has-stopped", 
            "text": "Yes. When you use a Jupyter Notebook on FloydHub, your Notebook is saved periodically in the  /output  dir. So, your work is not lost after your job has ended, shutdown or timed out.  You can view your saved Notebook using the  floyd output  command. Example:  $ floyd output saip/projects/mnist-pytorch/3  Or in the  Output  tab of your job on the web dashboard, example:  www.floydhub.com/saip/projects/mnist-pytorch/3/output", 
            "title": "Can I view my Jupyter Notebook after my job has stopped?"
        }, 
        {
            "location": "/faqs/job/#can-i-restart-a-stopped-or-timed-out-job", 
            "text": "Unfortunately, not directly. We will be implementing a single command to do this soon!  In the meanwhile, you can follow these steps to do this manually:   Jupyter Notebook : Your Notebook is  saved periodically . To restart your Notebook after your job has stopped, please download the output of your stopped job to your machine and start another job. Example:   # Download the saved Notebook from previous job  # NOTE: This will overwrite the contents of your current dir \n$ floyd data clone saip/projects/mnist-pytorch/3/output # Start a new job \n$ floyd run --mode jupyter   Script : If you are running a script/command, you will have to start a new job using the  floyd run  command  command.", 
            "title": "Can I restart a stopped or timed out job?"
        }, 
        {
            "location": "/faqs/job/#why-is-my-job-in-the-queued-state-for-several-minutes", 
            "text": "This means that a machine is being prepared to run your job.   Most times, we have several CPU and GPU machines that are ready and your job can start execution in a few seconds. During high traffic periods, we may not have a machine ready for you and have to spin up a new instance for your job on-demand (details below). This might take up to 10 minutes in some cases. We are actively working on reducing this wait time.  Details : When you execute a  floyd run  command, Floyd does several things in the background:   Provision a CPU or GPU instance on the cloud  Set up a deep learning environment with GPU drivers and the correct environment (as specified by  --env ) installed using Docker  Mount any data you specify using the  --data  flag  Spin up a Jupyter server, if  --mode jupyter  flag   Each of these steps can take up to a couple of minutes. Usually Steps 1 and 2 are already done, but during peak usage hours, we might have to do this on-demand.", 
            "title": "Why is my job in the \"Queued\" state for several minutes?"
        }, 
        {
            "location": "/faqs/job/#why-do-i-see-setting-up-your-instance-for-several-minutes-when-running-a-jupyter-notebook", 
            "text": "The  Setting up your instance...  message is displayed when a machine is being prepared to run your Jupyter Notebook.   When you execute a  floyd run --mode jupyter  command, the CLI waits for a CPU or GPU machine to be ready before it opens up an interactive Jupyter Notebook for your work on. Usually, this takes a few seconds, but during high traffic periods it can take up to 10 minutes in some cases.  For more details on why it takes time, please see  Why is my job in the \"Queued\" state for several minutes?", 
            "title": "Why do I see \"Setting up your instance...\" for several minutes when running a Jupyter Notebook?"
        }, 
        {
            "location": "/faqs/job/#why-are-my-logs-not-displayed-in-real-time", 
            "text": "You can stream your logs from the CLI using the  floyd logs -t  JOB_NAME  command. However, sometimes you may notice that the logs are not displayed in real-time. This is because of output buffering. Please make sure that your logs are flushed out if you prefer to view real-time logs.  For example, in Python:  import   sys  ...  print ( Hello world )  sys . stdout . flush ()", 
            "title": "Why are my logs not displayed in real-time?"
        }, 
        {
            "location": "/faqs/job/#why-did-my-job-timeout-after-1-hour", 
            "text": "You are likely in the Free Trial Plan. Jobs run in the trial plan have a maximum runtime of 1 hour. It will automatically timeout after that.    You can upgrade to the  Paid Plan  to overcome these limits.", 
            "title": "Why did my job timeout after 1 hour?"
        }, 
        {
            "location": "/faqs/job/#why-was-my-cpu-job-killed-without-warning", 
            "text": "Occasionally, you may notice that your CPU job died without warning. The output logs just display  Killed . For example,  ################################################################################  2017 -07-24  03 :33:42,530 INFO - Run Output:\n... 2017 -07-24  03 :33:52,920 INFO - Using TensorFlow backend. 2017 -07-24  03 :34:04,381 INFO -   loading UNet of size 1152x256... 2017 -07-24  03 :34:10,942 INFO - Epoch  1 /100 2017 -07-24  03 :35:17,221 INFO - Killed 2017 -07-24  03 :35:18,680 INFO -  ################################################################################   This happens when your machine runs out of memory (OOM). i.e. your job consumes more memory than is available on our CPU machines.  All jobs run on FloydHub are executed inside a Docker container. Our current CPU machines have  7GB memory . When memory used by your job exceeds 7GB, Docker automatically kills the job to protect the system and avoid abuse.  The resolution is to optimize your code to consume less memory. For example, read less data into your in-memory datastructures or reduce your batch size. We will be introducing more powerful CPUs, which higher memory in the near future.", 
            "title": "Why was my CPU job Killed without warning?"
        }, 
        {
            "location": "/faqs/job/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/", 
            "text": "Create a new Dataset\n\n\nA \nDataset\n is a collection of data. If you have used Github, datasets in\nFloydHub are also a lot like code repositories, except they are for storing and\nversioning data.\n\n\nTo create a new Dataset, visit\n\nwww.floydhub.com/datasets\n and click on the\n\"New Dataset\" button on the top right hand corner.\n\n\n\n\nGive the dataset a name and an apt description.\n\n\nThe \nVisibility\n field indicates who can see your dataset. If you set it to\n\nPublic\n, anyone can see your dataset and data versions. If you are working on\nan open source project, this is a great way to share and contribute to the\nFloydHub community. If your data is proprietary, please select \nPrivate\n. This\nwill ensure that only you and your team will have access to this dataset.\n\n\nUpload a Dataset\n\n\nOnce you have created a dataset, you can upload data from your terminal using\nthe \nfloyd data\n command. For example:\n\n\n$ floyd data init imagenet-2017\nDataset \nimagenet-2017\n initialized in current directory\n...\n$ floyd data upload\nCompressing data...", 
            "title": "Create and Upload a Dataset"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/#create-a-new-dataset", 
            "text": "A  Dataset  is a collection of data. If you have used Github, datasets in\nFloydHub are also a lot like code repositories, except they are for storing and\nversioning data.  To create a new Dataset, visit www.floydhub.com/datasets  and click on the\n\"New Dataset\" button on the top right hand corner.   Give the dataset a name and an apt description.  The  Visibility  field indicates who can see your dataset. If you set it to Public , anyone can see your dataset and data versions. If you are working on\nan open source project, this is a great way to share and contribute to the\nFloydHub community. If your data is proprietary, please select  Private . This\nwill ensure that only you and your team will have access to this dataset.", 
            "title": "Create a new Dataset"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/#upload-a-dataset", 
            "text": "Once you have created a dataset, you can upload data from your terminal using\nthe  floyd data  command. For example:  $ floyd data init imagenet-2017\nDataset  imagenet-2017  initialized in current directory\n...\n$ floyd data upload\nCompressing data...", 
            "title": "Upload a Dataset"
        }, 
        {
            "location": "/guides/delete_dataset/", 
            "text": "To delete a dataset, click the \nDelete dataset\n button on the \nSettings\n tab\nof the dataset on the web dashboard.\n\n\nExample: \nhttps://www.floydhub.com/alice/datasets/quick-start/settings\n\n\n\n\n\n\n\n\nImportant\n\n\nDeleting a dataset will delete all its individual versions of data. This\n\ncannot\n be restored. Please be absolutely sure you want to delete a\ndataset before proceeding.\n\n\n\n\nWe recommend deleting individual data versions rather than the entire dataset.\n\n\nDeleting an uploaded datasource\n\n\nYou can delete a particular version of a Dataset by clicking on \nDelete data\n\nbutton on the Settings tab of the data's page on the web dashboard.\n\n\nExample: \nhttps://www.floydhub.com/alice/datasets/quick-start/1/settings\n\n\n\n\nDeleting an uploaded datasource from CLI\n\n\nYou can also delete an uploaded datasource from the CLI using the \nfloyd data\ndelete\n command.\n\n\n$ floyd data delete alice/datasets/quick-start/1\n\nDelete Data: alice/quick-start/1? \n[\ny/N\n]\n: y\nData alice/datasets/quick-start/1: Deleted", 
            "title": "Delete a Dataset"
        }, 
        {
            "location": "/guides/delete_dataset/#deleting-an-uploaded-datasource", 
            "text": "You can delete a particular version of a Dataset by clicking on  Delete data \nbutton on the Settings tab of the data's page on the web dashboard.  Example:  https://www.floydhub.com/alice/datasets/quick-start/1/settings", 
            "title": "Deleting an uploaded datasource"
        }, 
        {
            "location": "/guides/delete_dataset/#deleting-an-uploaded-datasource-from-cli", 
            "text": "You can also delete an uploaded datasource from the CLI using the  floyd data\ndelete  command.  $ floyd data delete alice/datasets/quick-start/1\n\nDelete Data: alice/quick-start/1?  [ y/N ] : y\nData alice/datasets/quick-start/1: Deleted", 
            "title": "Deleting an uploaded datasource from CLI"
        }, 
        {
            "location": "/guides/data/mounting_data/", 
            "text": "In this guide, we will explain how to attach one or more datasets to a job.\nFirst, let's review some basics about FloydHub datasets.\n\n\nDatasets\n\n\nA Floyd dataset is a directory (folder) of data that can be used during a\njob. To create a new dataset, please follow\n\nthis guide\n. You can view the\ndatasets you have created in the datasets page in the dashboard. You can also\nview public datasets by searching for them on FloydHub.\n\n\nWhy keep data separate from code?\n\n\nA data scientist tweaks his/her code often during the process of creating a\ndeep-learning model. However, he/she doesn't change the underlying data nearly\nas often, if at all.\n\n\nEach time you run a job on FloydHub, a copy of your code is uploaded to\nFloydHub and run on one of FloyHub's powerful deep-learning servers. Because\nyour data isn't changing from job to job, it wouldn't make sense to keep your\ndata with your code and upload it with each job. Instead, we upload a\ndataset once, and attach, or \"mount\", it to each job. This saves a\nsignificant amount of time on each job.\n\n\nBeyond that, keeping data separate from code allows you to collaborate more\neasily with others. A dataset can be used by any user who has access to it, so\nteams and communities can work on solving problems together using the same\nunderlying data.\n\n\nMounting a Dataset\n\n\nWhat does it mean to \"mount\" a dataset to a job?\n\n\nIn the world of file systems, the term \"mount\" means to attach one file system\nor folder to another file system. For example, when you insert a flash drive\n(which is a mini file system) into your computer, its file system gets mounted\nto your computer's main file system so that you can retrieve, remove, or save\ndata to the flash drive. Once the flash drive is mounted to your computer's\nfile system, other programs can access it as if it were a native part of your\ncomputer's file system. This same mounting pattern is how datasets are handled\non FloydHub.\n\n\nWhen you use the \nfloyd run\n command to run a job, your code will be sent up to\nFloydHub and run on a powerful deep-learning server that can run your job. If\nyou want your code to be able to access a dataset during the job, you need to\nmount the dataset to the server where the job is running (just like you need to\nplug a flash drive into your computer in order to access its files). Mounting\ndatasets to a job is easy: just pass the \n--data\n flag to the \nfloyd run\n cli\ncommand as detailed below.\n\n\nThe \n--data\n flag\n\n\nTo properly use the \n--data\n flag with \nfloyd run\n, you need to specify two\nthings:\n\n\n\n\nThe name of the dataset you want to mount.\n\n\nA name for the folder where the data will be\n  accessible to your code during the job, we call this the \"mount point\". You\n  can give this folder (mount point) any name you want.\n\n\n\n\nThese two things are separated by a \n:\n with no spaces. Here's the syntax:\n\n\nfloyd run --data \ndata_set_name\n:\nmount_point\n \u2026(rest of run command)\n\n\n\n\nLet's go through a couple of examples to show how to mount one or more datasets to your job.\n\n\nExample 1\n\n\nThe command below will mount FloydHub's public\n\nUdacity GAN\n\n dataset at \n/my_data\n\n\nfloyd run --data floydhub/datasets/udacity-gan/1:/my_data \npython my_script.py\n\n\n\n\nA couple of things to note:\n\n\n\n\nThere is no space between the name of the dataset\n    (\nfloydhub/datasets/udacity-gan/1\n) and the mount point name (\n/my_data\n).\n\n\nA colon (\n:\n) is used to separate the name of the dataset and the\n    mount point.\n\n\nDatasets are always mounted at the root directory (\n/\n). This means that if\n    you specify \n/foo\n as the mountpoint, your data will be mounted at \n/foo\n.\n    If you specify \nfoo\n as the mount point, your data will still be mounted\n    at \n/foo\n. Preceding the mount point with a \n/\n is optional\u2012your data will\n    be mounted at the same location either way. You'll see us use both\n    variations in this guide.\n\n\nNested mount points are not supported. This means mount points like\n    \nmy_data/foo\n or \n/home/me/data\n will not work. If you need your data\n    available at a nested directory, check out the \nSymlinking mounted\n    data\n guide.\n\n\n\n\n\n\nImportant\n\n\nA common mistake is to for code to reference a mounted dataset without a\npreceding \n/\n. If you specify \nmy_data\n as the mount point for your\ndataset, your code needs to look in \n/my_data\n to find the dataset. Without\nthe preceding \n/\n, your code will look for the dataset in the wrong place\n(the directory the code is running in). In your code, always precede your\nreferences to the mount point with a \n/\n.\n\n\n\n\nExample 2\n\n\nThis example spins up a Jupyter Notebook and mounts the\n\nVGG 19-layers\n\ndataset under \n/vgg\n:\n\nfloyd run --data floydhub/datasets/vgg-ilsvrc-19-layers/1:vgg --mode jupyter\n\n\nThe Jupyter Notebook will have access to the VGGNet pre-trained models under\n\n/vgg\n.\n\n\n\n\nMounting the output of another job\n\n\nYou can link jobs by mounting the output of one job as the input of a new job.\nThis allows you to iterate on the ouput of a past job.\n\n\nYou can refer to the output of a job by its name with \n/output\n appended to it.\nFor example: \nfloydhub/projects/handwriting-recognition/12/output\n refers to\nthe output of the job \nfloydhub/projects/handwriting-recognition/12\n\n\nUse the \n--data\n flag in the \nfloyd run\n command, mount past output to a job,\njust as you would to mount a dataset. For example:\n\n\n$ floyd run \n\\\n\n  --data floydhub/projects/handwriting-recognition/12/output:filtered_training_data \n\\\n\n  \npython train.py\n\n\n\n\n\nThis will make the output of \nfloydhub/projects/handwriting-recognition/12\n\navailable at \n/filtered_training_data\n for the new job to use.\n\n\nNote: You need to have access to a job to be able to mount its output.\n\n\nMounting multiple datasources\n\n\nYou can attach up to five datasources (datasets and/or job outputs) to a job\nusing the \n--data\n flag in the \nfloyd run\n command. Ensure that each mount\npoint is unique.\n\n\n$ floyd run \n\\\n\n  --data floydhub/datasets/mnist/2:training \n\\\n\n  --data floydhub/datasets/digits/1:test \n\\\n\n  \npython script.py\n\n\n\n\n\nIn this case, the above datasets will be mounted at \n/training\n and \n/test\n,\nrespectively.\n\n\nViewing mounted datasets in the web dashboard\n\n\nYou can view the mounted datasets and their respective mount points for a\nspecific job by going to the Data tab:\n\n\n\n\nSymlinking your mounted data\n\n\nFloydhub's basic data-mounting functionality is sufficient for most users'\nneeds. However, if you find yourself with more complex requirements, symlinking\ncan almost certainly provide a solution.\n\n\nHere are some common FloydHub data-mounting needs that symlinking can solve:\n\n\n\n\nCode requires the data to be available at a location that is not valid with\n    the mounting syntax of \nfloyd run --data\n.\n\n\nMultiple mounted datasources need to be available under a single directory.\n\n\nDirectories in a single datasource need to be split into their own\n    locations.\n\n\n\n\nFor documentation on symlinking, please see this guide: \nSymlinking mounted\ndata\n\n\nUnderstanding dataset names\n\n\nThe full name of a datasource (\nusername\n/datasets/\ndataset_name\n/\nversion\n)\nconsists of three parts:\n\n\n\n\nUsername\n\n\nDataset Name\n\n\nVersion\n\n\n\n\nFor example: \nfloydhub/datasets/mnist/2\n\n\nDefault mount points\n\n\nWe highly recommend that you explicitly specify the mount points for your data\nusing the \n--data \ndata_name\n:\nmount_point\n convention.\n\n\nIf, however, you do not specify a mount point, the default values are:\n\n\n\n\n\n\nSingle data mount: If you only mount one datasource without specifying a\n  mount point, it is mounted at \n/input\n\n\n\n\n\n\nMultiple data mounts: If you mount multiple datasource without specifying\n  mount points, they will each be mounted under their respective GUIDs (e.g.\n  \n/xKduBzTr4LAsc6eVPZVPVd\n). GUIDs are 32-character random strings that\n  difficult to track down, so we highly discourage this pattern.\n\n\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Mount Data to a Job"
        }, 
        {
            "location": "/guides/data/mounting_data/#datasets", 
            "text": "A Floyd dataset is a directory (folder) of data that can be used during a\njob. To create a new dataset, please follow this guide . You can view the\ndatasets you have created in the datasets page in the dashboard. You can also\nview public datasets by searching for them on FloydHub.", 
            "title": "Datasets"
        }, 
        {
            "location": "/guides/data/mounting_data/#why-keep-data-separate-from-code", 
            "text": "A data scientist tweaks his/her code often during the process of creating a\ndeep-learning model. However, he/she doesn't change the underlying data nearly\nas often, if at all.  Each time you run a job on FloydHub, a copy of your code is uploaded to\nFloydHub and run on one of FloyHub's powerful deep-learning servers. Because\nyour data isn't changing from job to job, it wouldn't make sense to keep your\ndata with your code and upload it with each job. Instead, we upload a\ndataset once, and attach, or \"mount\", it to each job. This saves a\nsignificant amount of time on each job.  Beyond that, keeping data separate from code allows you to collaborate more\neasily with others. A dataset can be used by any user who has access to it, so\nteams and communities can work on solving problems together using the same\nunderlying data.", 
            "title": "Why keep data separate from code?"
        }, 
        {
            "location": "/guides/data/mounting_data/#mounting-a-dataset", 
            "text": "", 
            "title": "Mounting a Dataset"
        }, 
        {
            "location": "/guides/data/mounting_data/#what-does-it-mean-to-mount-a-dataset-to-a-job", 
            "text": "In the world of file systems, the term \"mount\" means to attach one file system\nor folder to another file system. For example, when you insert a flash drive\n(which is a mini file system) into your computer, its file system gets mounted\nto your computer's main file system so that you can retrieve, remove, or save\ndata to the flash drive. Once the flash drive is mounted to your computer's\nfile system, other programs can access it as if it were a native part of your\ncomputer's file system. This same mounting pattern is how datasets are handled\non FloydHub.  When you use the  floyd run  command to run a job, your code will be sent up to\nFloydHub and run on a powerful deep-learning server that can run your job. If\nyou want your code to be able to access a dataset during the job, you need to\nmount the dataset to the server where the job is running (just like you need to\nplug a flash drive into your computer in order to access its files). Mounting\ndatasets to a job is easy: just pass the  --data  flag to the  floyd run  cli\ncommand as detailed below.", 
            "title": "What does it mean to \"mount\" a dataset to a job?"
        }, 
        {
            "location": "/guides/data/mounting_data/#the-data-flag", 
            "text": "To properly use the  --data  flag with  floyd run , you need to specify two\nthings:   The name of the dataset you want to mount.  A name for the folder where the data will be\n  accessible to your code during the job, we call this the \"mount point\". You\n  can give this folder (mount point) any name you want.   These two things are separated by a  :  with no spaces. Here's the syntax:  floyd run --data  data_set_name : mount_point  \u2026(rest of run command)  Let's go through a couple of examples to show how to mount one or more datasets to your job.", 
            "title": "The --data flag"
        }, 
        {
            "location": "/guides/data/mounting_data/#example-1", 
            "text": "The command below will mount FloydHub's public Udacity GAN \n dataset at  /my_data  floyd run --data floydhub/datasets/udacity-gan/1:/my_data  python my_script.py   A couple of things to note:   There is no space between the name of the dataset\n    ( floydhub/datasets/udacity-gan/1 ) and the mount point name ( /my_data ).  A colon ( : ) is used to separate the name of the dataset and the\n    mount point.  Datasets are always mounted at the root directory ( / ). This means that if\n    you specify  /foo  as the mountpoint, your data will be mounted at  /foo .\n    If you specify  foo  as the mount point, your data will still be mounted\n    at  /foo . Preceding the mount point with a  /  is optional\u2012your data will\n    be mounted at the same location either way. You'll see us use both\n    variations in this guide.  Nested mount points are not supported. This means mount points like\n     my_data/foo  or  /home/me/data  will not work. If you need your data\n    available at a nested directory, check out the  Symlinking mounted\n    data  guide.    Important  A common mistake is to for code to reference a mounted dataset without a\npreceding  / . If you specify  my_data  as the mount point for your\ndataset, your code needs to look in  /my_data  to find the dataset. Without\nthe preceding  / , your code will look for the dataset in the wrong place\n(the directory the code is running in). In your code, always precede your\nreferences to the mount point with a  / .", 
            "title": "Example 1"
        }, 
        {
            "location": "/guides/data/mounting_data/#example-2", 
            "text": "This example spins up a Jupyter Notebook and mounts the VGG 19-layers \ndataset under  /vgg : floyd run --data floydhub/datasets/vgg-ilsvrc-19-layers/1:vgg --mode jupyter \nThe Jupyter Notebook will have access to the VGGNet pre-trained models under /vgg .", 
            "title": "Example 2"
        }, 
        {
            "location": "/guides/data/mounting_data/#mounting-the-output-of-another-job", 
            "text": "You can link jobs by mounting the output of one job as the input of a new job.\nThis allows you to iterate on the ouput of a past job.  You can refer to the output of a job by its name with  /output  appended to it.\nFor example:  floydhub/projects/handwriting-recognition/12/output  refers to\nthe output of the job  floydhub/projects/handwriting-recognition/12  Use the  --data  flag in the  floyd run  command, mount past output to a job,\njust as you would to mount a dataset. For example:  $ floyd run  \\ \n  --data floydhub/projects/handwriting-recognition/12/output:filtered_training_data  \\ \n   python train.py   This will make the output of  floydhub/projects/handwriting-recognition/12 \navailable at  /filtered_training_data  for the new job to use.  Note: You need to have access to a job to be able to mount its output.", 
            "title": "Mounting the output of another job"
        }, 
        {
            "location": "/guides/data/mounting_data/#mounting-multiple-datasources", 
            "text": "You can attach up to five datasources (datasets and/or job outputs) to a job\nusing the  --data  flag in the  floyd run  command. Ensure that each mount\npoint is unique.  $ floyd run  \\ \n  --data floydhub/datasets/mnist/2:training  \\ \n  --data floydhub/datasets/digits/1:test  \\ \n   python script.py   In this case, the above datasets will be mounted at  /training  and  /test ,\nrespectively.", 
            "title": "Mounting multiple datasources"
        }, 
        {
            "location": "/guides/data/mounting_data/#viewing-mounted-datasets-in-the-web-dashboard", 
            "text": "You can view the mounted datasets and their respective mount points for a\nspecific job by going to the Data tab:", 
            "title": "Viewing mounted datasets in the web dashboard"
        }, 
        {
            "location": "/guides/data/mounting_data/#symlinking-your-mounted-data", 
            "text": "Floydhub's basic data-mounting functionality is sufficient for most users'\nneeds. However, if you find yourself with more complex requirements, symlinking\ncan almost certainly provide a solution.  Here are some common FloydHub data-mounting needs that symlinking can solve:   Code requires the data to be available at a location that is not valid with\n    the mounting syntax of  floyd run --data .  Multiple mounted datasources need to be available under a single directory.  Directories in a single datasource need to be split into their own\n    locations.   For documentation on symlinking, please see this guide:  Symlinking mounted\ndata", 
            "title": "Symlinking your mounted data"
        }, 
        {
            "location": "/guides/data/mounting_data/#understanding-dataset-names", 
            "text": "The full name of a datasource ( username /datasets/ dataset_name / version )\nconsists of three parts:   Username  Dataset Name  Version   For example:  floydhub/datasets/mnist/2", 
            "title": "Understanding dataset names"
        }, 
        {
            "location": "/guides/data/mounting_data/#default-mount-points", 
            "text": "We highly recommend that you explicitly specify the mount points for your data\nusing the  --data  data_name : mount_point  convention.  If, however, you do not specify a mount point, the default values are:    Single data mount: If you only mount one datasource without specifying a\n  mount point, it is mounted at  /input    Multiple data mounts: If you mount multiple datasource without specifying\n  mount points, they will each be mounted under their respective GUIDs (e.g.\n   /xKduBzTr4LAsc6eVPZVPVd ). GUIDs are 32-character random strings that\n  difficult to track down, so we highly discourage this pattern.", 
            "title": "Default mount points"
        }, 
        {
            "location": "/guides/data/mounting_data/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/", 
            "text": "Floydhub's basic data-mounting functionality is sufficient for most users'\nneeds. However, if you find yourself with more complex requirements, symlinking\ncan almost certainly provide a solution.\n\n\nHere are some common FloydHub data-mounting needs that symlinking can solve:\n\n\n\n\nCode requires the data to be available at a location that is not valid with\n    the mounting syntax of \nfloyd run --data\n.\n\n\nMultiple mounted datasources need to be available under a single directory.\n\n\nDirectories in a single datasource need to be split into their own\n    locations.\n\n\n\n\nSimply copying the data from one location to another during your job would also\nsolve these problems, but copying data is slow and inefficient, especially for\nlarge datasets. Symlinks can be created very quickly and are a much more ideal\nsolution.\n\n\nCreating symlinks\n\n\nYou can create symlinks on FloydHub's deep-learning servers during your job\nusing the same \nln\n command available on *nix operating systems (like Linux\nand MacOS). To create a symlink during a job you'll need to send the \nln\n\ncommand to the server's operating system. If you already know how to create\nsymlinks on a *nix OS, you can skip forward to the \nWays to use the \nln\n\ncommand with FloydHub section\n. If\nyou don't know much about symlinks, read on for a quick primer.\n\n\nIntro to symlinks\n\n\nYou can think of a symlink as an alias for a file or directory that allows\nprograms to find the file or directory at more than one location on the file\nsystem. Calling a symlink an \"alias\" isn't a technically correct way to refer\nto it, but we'll sometimes use that term in this guide because it can be more\nfitting and easier to internalize and remember.\n\n\nSymlink (\nln\n) syntax\n\n\nCreating a symlink on a *nix OS uses the \nln\n command. Here is the syntax:\n\n$ ln -s \nTARGET\n \nLINK_PATH_WITH_OPTIONAL_NAME\n\n\n\n\nWhere \nTARGET\n is the path of the existing file or directory you want to\ncreate an alias/symlink for, and \nLINK_PATH_WITH_OPTIONAL_NAME\n is the path\nand (optional) name of the new symlink/alias of the \nTARGET\n.\n\n\nIf \nLINK_PATH_WITH_OPTIONAL_NAME\n doesn't include a name, the name of the\n\nTARGET\n will be used. Let's go through a couple of examples to clarify that.\nAssuming you want to create an alias/symlink for \n/my_data\n and have already\ncreated a new directory called \n/existing_dir\n:\n\n\n\n\nSpecifying a name in \nLINK_PATH_WITH_OPTIONAL_NAME\n:\n    \n$ ln -s /my_data /existing_dir/new_name_for_my_data\n`\n\n\n\n    The new name here is \nnew_name_for_my_data\n. Given the command above, your\n    data will be accessible at \n/my_data\n and\n    \n/existing_dir/new_name_for_my_data\n.\n\n\nWithout specifying a name in \nLINK_PATH_WITH_OPTIONAL_NAME\n:\n    \n$ ln -s /my_data /existing_dir\n`\n\n\n\n    Because no new name was given, the name of the \nTARGET\n (which is\n    \nmy_data\n) will be used. The data will accessible at \n/my_data\n and\n    \n/existing_dir/my_data\n.\n\n\n\n\nHere are some notes/gotchas about creating symlinks using \nln\n:\n\n\n\n\nAlways use absolute paths for both the \nTARGET\n and\n    \nLINK_PATH_WITH_OPTIONAL_NAME\n parameters. This will ensure you don't run\n    into some odd behaviors that can manifest when using relative paths.\n\n\nThe directory in which the \nLINK_PATH_WITH_OPTIONAL_NAME\n terminates must\n    already exist. For example, if you want to make the data located at\n    \n/my_data\n to be aliased/symlinked at \n/home/me/foo_data\n, you first need\n    to create the \n/home/me\n directory.The following commands would\n    successfully implement this goal:\n    \n# First we make sure the /home/me directory exists:\n$ mkdir -p /home/me\n\n# Now we are ready to create the symlink. We\nll supply the name foo_data\n# to the \nLINK_PATH_WITH_OPTIONAL_NAME\n parameter:\n$ ln -s /my_data /home/me/foo_data\n\n\n\n\n\nWays to use the \nln\n command with FloydHub\n\n\nTo send the \nln\n command to the server's OS to create a symlink during your\njob, you can follow one of at least a few approaches:\n\n\n1. Using the \n[COMMAND]\n portion of \nfloyd run\n\n\nWith this approach, we add the \nln\n calls to the \n[COMMAND]\n portion of the\ncall to \nfloyd run [OPTIONS] [COMMAND]\n (see the \nfloyd run\n syntax\n\nhere\n). This is the most straight-forward approach, but\nit can it can get a bit unwieldy if you have more complex needs.\n\n\nExample 1\n\n\nLet's say you have two datasources mounted under \n/train\n and \n/test\n,\nrespectively. Your Python script \ntrain_and_eval.py\n expects both the\ndatasources to be available under the same parent directory, say \n/data/train\n\nand \n/data/test\n. You can symlink the datasources to those locations.\n\n\nIn the example below, we create a directory called \n/data\n, and then create\nlinks inside of it to our datasets, which are at \n/train\n and \n/test\n (note the\n\n--data\n flags). This means that our Python script can reference \n/data/train\n\nand \n/data/test\n and it will find our datasets.\n\n\nThe \n[COMMAND]\n portion of the \nfloyd run [OPTIONS] [COMMAND]\n in this example\nchains a series of commands, which are executed in sequence:\n\n\n\n\nmkdir -p /data\n\n\nln -s /train /data\n\n\nln -s /test /data\n\n\npython train_and_eval.py\n\n\n\n\nHere's the command in full:\n\n\n$ floyd run \n\\\n\n  --data alice/datasets/imagenet-train/1:train \n\\\n\n  --data alice/datasets/imagenet-test/1:test \n\\\n\n  \nmkdir -p /data \n ln -s /train /data \n ln -s /test /data \n python train_and_eval.py\n\n\n\n\n\n2. Using a bash script\n\n\nAs you can see in the previous section, using the \n[COMMAND]\n portion of \nfloydrun [OPTIONS] [COMMAND]\n can get unwieldy when there are many commands. A\nbetter alternative is to create a bash script that creates our symlinks and\nalso kicks off our main Python script.\n\n\nIf you are not familiar with writing bash scripts, a quick search of the\nInternet can get you up to speed on the basics, but bash scripting is out of\nthe scope of this documentation.\n\n\nExample 1\n\n\nThis bash script should live in the root/top-level directory of your project.\nWe'll call ours \nrun.sh\n. Here's an example of what it might look like:\n\n\n#!/bin/bash\n\n\n\n# Create a /data directory\n\nmkdir /data\n\n\n# Symlink mounted data to their destinations\n\nln -s /train /data\nln -s /test /data\n\n\n# Execute Python script\n\npython train_and_eval.py\n\n\n\n\nLet's execute the bash script using \nfloyd run\n:\n\n\n$ floyd run \nbash run.sh\n\n\n\n\n\nBecause the last line of our bash script runs our Python script, we can kick\noff our entire job by running only the bash script.\n\n\nThis is a very effective pattern if your jobs require a more complex\nsetup\u2014create a bash script that sets up your environment, and then have the\nbash script call your python script. This keeps your setup separate from your\ncode, and keeps things clean.\n\n\nExample 2\n\n\nYour data is mounted under \n/vgg\n using \n--datafloydhub/datasets/vgg-ilsvrc-19-layers/1:vgg\n. However, your code expects the\ndata to be present at \n/home/data/vgg/2017\n. Let's create a symlink to make\nyour data available at \n/home/data/vgg/2017\n. Here's an example of a bash\nscript (we'll call it \nrun.sh\n) that takes care of the symlinking and\ncalls our training script.\n\n\n#!/bin/bash\n\n\n\n# Create directory\n\nmkdir -p /home/data/vgg\n\n\n# Symlink our data at /home/data/vgg/2017\n\nln -s /vgg /home/data/vgg/2017\n\n\n# Call our training script\n\npython train.py\n\n\n\n\nNow let's tie it all together with our \nfloyd run command\n:\n\n\n$ floyd run --data floydhub/datasets/vgg-ilsvrc-19-layers/1:vgg \nbash run.sh\n\n\n\n\n\n3. For Jupyter Notebooks\n\n\nSymlinking with Jupyter Notebooks follows the same principles that a regular\njob does. Here's an example:\n\n\nExample 1\n\n\nYour data is mounted under \n/vgg\n using \n--datafloydhub/datasets/vgg-ilsvrc-19-layers/1:vgg\n, but you want your data to be\npresent at \n/home/data/vgg/2017\n. The notebook below accomplishes that with a\nsymlink:\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Symlink Mounted Data"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#creating-symlinks", 
            "text": "You can create symlinks on FloydHub's deep-learning servers during your job\nusing the same  ln  command available on *nix operating systems (like Linux\nand MacOS). To create a symlink during a job you'll need to send the  ln \ncommand to the server's operating system. If you already know how to create\nsymlinks on a *nix OS, you can skip forward to the  Ways to use the  ln \ncommand with FloydHub section . If\nyou don't know much about symlinks, read on for a quick primer.", 
            "title": "Creating symlinks"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#intro-to-symlinks", 
            "text": "You can think of a symlink as an alias for a file or directory that allows\nprograms to find the file or directory at more than one location on the file\nsystem. Calling a symlink an \"alias\" isn't a technically correct way to refer\nto it, but we'll sometimes use that term in this guide because it can be more\nfitting and easier to internalize and remember.", 
            "title": "Intro to symlinks"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#symlink-ln-syntax", 
            "text": "Creating a symlink on a *nix OS uses the  ln  command. Here is the syntax: $ ln -s  TARGET   LINK_PATH_WITH_OPTIONAL_NAME   Where  TARGET  is the path of the existing file or directory you want to\ncreate an alias/symlink for, and  LINK_PATH_WITH_OPTIONAL_NAME  is the path\nand (optional) name of the new symlink/alias of the  TARGET .  If  LINK_PATH_WITH_OPTIONAL_NAME  doesn't include a name, the name of the TARGET  will be used. Let's go through a couple of examples to clarify that.\nAssuming you want to create an alias/symlink for  /my_data  and have already\ncreated a new directory called  /existing_dir :   Specifying a name in  LINK_PATH_WITH_OPTIONAL_NAME :\n     $ ln -s /my_data /existing_dir/new_name_for_my_data `  \n    The new name here is  new_name_for_my_data . Given the command above, your\n    data will be accessible at  /my_data  and\n     /existing_dir/new_name_for_my_data .  Without specifying a name in  LINK_PATH_WITH_OPTIONAL_NAME :\n     $ ln -s /my_data /existing_dir `  \n    Because no new name was given, the name of the  TARGET  (which is\n     my_data ) will be used. The data will accessible at  /my_data  and\n     /existing_dir/my_data .   Here are some notes/gotchas about creating symlinks using  ln :   Always use absolute paths for both the  TARGET  and\n     LINK_PATH_WITH_OPTIONAL_NAME  parameters. This will ensure you don't run\n    into some odd behaviors that can manifest when using relative paths.  The directory in which the  LINK_PATH_WITH_OPTIONAL_NAME  terminates must\n    already exist. For example, if you want to make the data located at\n     /my_data  to be aliased/symlinked at  /home/me/foo_data , you first need\n    to create the  /home/me  directory.The following commands would\n    successfully implement this goal:\n     # First we make sure the /home/me directory exists:\n$ mkdir -p /home/me\n\n# Now we are ready to create the symlink. We ll supply the name foo_data\n# to the  LINK_PATH_WITH_OPTIONAL_NAME  parameter:\n$ ln -s /my_data /home/me/foo_data", 
            "title": "Symlink (ln) syntax"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#ways-to-use-the-ln-command-with-floydhub", 
            "text": "To send the  ln  command to the server's OS to create a symlink during your\njob, you can follow one of at least a few approaches:", 
            "title": "Ways to use the ln command with FloydHub"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#1-using-the-command-portion-of-floyd-run", 
            "text": "With this approach, we add the  ln  calls to the  [COMMAND]  portion of the\ncall to  floyd run [OPTIONS] [COMMAND]  (see the  floyd run  syntax here ). This is the most straight-forward approach, but\nit can it can get a bit unwieldy if you have more complex needs.", 
            "title": "1. Using the [COMMAND] portion of floyd run"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#example-1", 
            "text": "Let's say you have two datasources mounted under  /train  and  /test ,\nrespectively. Your Python script  train_and_eval.py  expects both the\ndatasources to be available under the same parent directory, say  /data/train \nand  /data/test . You can symlink the datasources to those locations.  In the example below, we create a directory called  /data , and then create\nlinks inside of it to our datasets, which are at  /train  and  /test  (note the --data  flags). This means that our Python script can reference  /data/train \nand  /data/test  and it will find our datasets.  The  [COMMAND]  portion of the  floyd run [OPTIONS] [COMMAND]  in this example\nchains a series of commands, which are executed in sequence:   mkdir -p /data  ln -s /train /data  ln -s /test /data  python train_and_eval.py   Here's the command in full:  $ floyd run  \\ \n  --data alice/datasets/imagenet-train/1:train  \\ \n  --data alice/datasets/imagenet-test/1:test  \\ \n   mkdir -p /data   ln -s /train /data   ln -s /test /data   python train_and_eval.py", 
            "title": "Example 1"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#2-using-a-bash-script", 
            "text": "As you can see in the previous section, using the  [COMMAND]  portion of  floydrun [OPTIONS] [COMMAND]  can get unwieldy when there are many commands. A\nbetter alternative is to create a bash script that creates our symlinks and\nalso kicks off our main Python script.  If you are not familiar with writing bash scripts, a quick search of the\nInternet can get you up to speed on the basics, but bash scripting is out of\nthe scope of this documentation.", 
            "title": "2. Using a bash script"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#example-1_1", 
            "text": "This bash script should live in the root/top-level directory of your project.\nWe'll call ours  run.sh . Here's an example of what it might look like:  #!/bin/bash  # Create a /data directory \nmkdir /data # Symlink mounted data to their destinations \nln -s /train /data\nln -s /test /data # Execute Python script \npython train_and_eval.py  Let's execute the bash script using  floyd run :  $ floyd run  bash run.sh   Because the last line of our bash script runs our Python script, we can kick\noff our entire job by running only the bash script.  This is a very effective pattern if your jobs require a more complex\nsetup\u2014create a bash script that sets up your environment, and then have the\nbash script call your python script. This keeps your setup separate from your\ncode, and keeps things clean.", 
            "title": "Example 1"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#example-2", 
            "text": "Your data is mounted under  /vgg  using  --datafloydhub/datasets/vgg-ilsvrc-19-layers/1:vgg . However, your code expects the\ndata to be present at  /home/data/vgg/2017 . Let's create a symlink to make\nyour data available at  /home/data/vgg/2017 . Here's an example of a bash\nscript (we'll call it  run.sh ) that takes care of the symlinking and\ncalls our training script.  #!/bin/bash  # Create directory \nmkdir -p /home/data/vgg # Symlink our data at /home/data/vgg/2017 \nln -s /vgg /home/data/vgg/2017 # Call our training script \npython train.py  Now let's tie it all together with our  floyd run command :  $ floyd run --data floydhub/datasets/vgg-ilsvrc-19-layers/1:vgg  bash run.sh", 
            "title": "Example 2"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#3-for-jupyter-notebooks", 
            "text": "Symlinking with Jupyter Notebooks follows the same principles that a regular\njob does. Here's an example:", 
            "title": "3. For Jupyter Notebooks"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#example-1_2", 
            "text": "Your data is mounted under  /vgg  using  --datafloydhub/datasets/vgg-ilsvrc-19-layers/1:vgg , but you want your data to be\npresent at  /home/data/vgg/2017 . The notebook below accomplishes that with a\nsymlink:", 
            "title": "Example 1"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/environments/", 
            "text": "Environments\n\n\nBelow is the list of Deep Learning environments supported by FloydHub. Any of\nthese can be specified in the floyd \nrun\n command using the\n\n--env\n option.\n\n\nIf no \n--env\n is provided, it uses the \nkeras\n image by default, which comes with Python\n3, Keras 2.0.4 and Tensorflow 1.1.0 pre-installed.\n\n\n\n\n\n\n\n\nFramework\n\n\nEnv name (--env parameter)\n\n\nDescription\n\n\nDocker Image\n\n\n\n\n\n\n\n\n\n\nKeras\n\n\nkeras\n\n\nTensorflow 1.1.0 + keras 2.0.4 on Python3.\n\n\n\n\n\n\n\n\n\n\nkeras:py2\n\n\nTensorflow 1.1.0 + keras 2.0.4 on Python2.\n\n\n\n\n\n\n\n\nTensorflow 1.2\n\n\ntensorflow-1.2\n\n\nTensorflow 1.2.0 + Keras 2.0.4 on Python3.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\n\n\ntensorflow-1.2:py2\n\n\nTensorflow 1.2.0 + Keras 2.0.4 on Python2.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\nTensorflow 1.1\n\n\ntensorflow\n\n\nTensorflow 1.1.0 + Keras 2.0.4 on Python3.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\n\n\ntensorflow:py2\n\n\nTensorflow 1.1.0 + Keras 2.0.4 on Python2.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\nTensorflow 1.0\n\n\ntensorflow-1.0\n\n\nTensorflow 1.0.0 + Keras 1.2.2 on Python3.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\n\n\ntensorflow-1.0:py2\n\n\nTensorflow 1.0.0 + Keras 1.2.2 on Python2.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\nTensorflow 0.12\n\n\ntensorflow-0.12\n\n\nTensorflow 0.12.1 + Keras 1.2.2 on Python3.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\n\n\ntensorflow-0.12:py2\n\n\nTensorflow 0.12.1 + Keras 1.2.2 on Python2.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\nPyTorch 0.2\n\n\npytorch-0.2\n\n\nPyTorch 0.2.0 on Python 3.\n\n\nfloydhub/pytorch\n\n\n\n\n\n\n\n\npytorch-0.2:py2\n\n\nPyTorch 0.2.0 on Python 2.\n\n\nfloydhub/pytorch\n\n\n\n\n\n\nPyTorch 0.1\n\n\npytorch-0.1\n\n\nPyTorch 0.1.12 on Python 3.\n\n\nfloydhub/pytorch\n\n\n\n\n\n\n\n\npytorch-0.1:py2\n\n\nPyTorch 0.1.12 on Python 2.\n\n\nfloydhub/pytorch\n\n\n\n\n\n\nTheano 0.8\n\n\ntheano-0.8\n\n\nTheano rel-0.8.2 + Keras 1.2.2 on Python3.\n\n\nfloydhub/theano\n\n\n\n\n\n\n\n\ntheano-0.8:py2\n\n\nTheano rel-0.8.2 + Keras 1.2.2 on Python2.\n\n\nfloydhub/theano\n\n\n\n\n\n\nTheano 0.9\n\n\ntheano-0.9\n\n\nTheano rel-0.8.2 + Keras 2.0.3 on Python3.\n\n\nfloydhub/theano\n\n\n\n\n\n\n\n\ntheano-0.9:py2\n\n\nTheano rel-0.8.2 + Keras 2.0.3 on Python2.\n\n\nfloydhub/theano\n\n\n\n\n\n\nCaffe\n\n\ncaffe\n\n\nCaffe rc4 on Python3.\n\n\nfloydhub/caffe\n\n\n\n\n\n\n\n\ncaffe:py2\n\n\nCaffe rc4 on Python2.\n\n\nfloydhub/caffe\n\n\n\n\n\n\nTorch\n\n\ntorch\n\n\nTorch 7 with Python 3 env.\n\n\nfloydhub/torch\n\n\n\n\n\n\n\n\ntorch:py2\n\n\nTorch 7 with Python 2 env.\n\n\nfloydhub/torch\n\n\n\n\n\n\nChainer 1.23\n\n\nchainer-1.23\n\n\nChainer 1.23.0 on Python 3.\n\n\nfloydhub/chainer\n\n\n\n\n\n\n\n\nchainer-1.23:py2\n\n\nChainer 1.23.0 on Python 2.\n\n\nfloydhub/chainer\n\n\n\n\n\n\nChainer 2.0\n\n\nchainer-2.0\n\n\nChainer 1.23.0 on Python 3.\n\n\nfloydhub/chainer\n\n\n\n\n\n\n\n\nchainer-2.0:py2\n\n\nChainer 1.23.0 on Python 2.\n\n\nfloydhub/chainer\n\n\n\n\n\n\nMxNet (beta)\n\n\nmxnet:py2\n\n\nMxNet 0.9.3a on Python 2.\n\n\nfloydhub/mxnet\n\n\n\n\n\n\nKur\n\n\nkur\n\n\nKur 0.3.0 on Python 3.\n\n\nfloydhub/kur\n\n\n\n\n\n\n\n\nAll environments are available for both CPU and GPU execution. For example,\n\n\nTo run a Python2 Tensorflow job on CPU\n\n$ floyd run --env tensorflow:py2 \npython mnist_cnn.py\n\n\n\n\nTo run a Python2 Tensorflow job on GPU (CUDA, cuDNN, etc. installed)\n\n$ floyd run --env tensorflow:py2 --gpu \npython mnist_cnn.py\n\n\n\n\nThe following software packages (in addition to many other common libraries) are available in all the environments:\n\nh5py, iPython, Jupyter, matplotlib, numpy, OpenCV, Pandas, Pillow, scikit-learn, scipy, sklearn\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "List of Available Environments"
        }, 
        {
            "location": "/guides/environments/#environments", 
            "text": "Below is the list of Deep Learning environments supported by FloydHub. Any of\nthese can be specified in the floyd  run  command using the --env  option.  If no  --env  is provided, it uses the  keras  image by default, which comes with Python\n3, Keras 2.0.4 and Tensorflow 1.1.0 pre-installed.     Framework  Env name (--env parameter)  Description  Docker Image      Keras  keras  Tensorflow 1.1.0 + keras 2.0.4 on Python3.      keras:py2  Tensorflow 1.1.0 + keras 2.0.4 on Python2.     Tensorflow 1.2  tensorflow-1.2  Tensorflow 1.2.0 + Keras 2.0.4 on Python3.  floydhub/tensorflow     tensorflow-1.2:py2  Tensorflow 1.2.0 + Keras 2.0.4 on Python2.  floydhub/tensorflow    Tensorflow 1.1  tensorflow  Tensorflow 1.1.0 + Keras 2.0.4 on Python3.  floydhub/tensorflow     tensorflow:py2  Tensorflow 1.1.0 + Keras 2.0.4 on Python2.  floydhub/tensorflow    Tensorflow 1.0  tensorflow-1.0  Tensorflow 1.0.0 + Keras 1.2.2 on Python3.  floydhub/tensorflow     tensorflow-1.0:py2  Tensorflow 1.0.0 + Keras 1.2.2 on Python2.  floydhub/tensorflow    Tensorflow 0.12  tensorflow-0.12  Tensorflow 0.12.1 + Keras 1.2.2 on Python3.  floydhub/tensorflow     tensorflow-0.12:py2  Tensorflow 0.12.1 + Keras 1.2.2 on Python2.  floydhub/tensorflow    PyTorch 0.2  pytorch-0.2  PyTorch 0.2.0 on Python 3.  floydhub/pytorch     pytorch-0.2:py2  PyTorch 0.2.0 on Python 2.  floydhub/pytorch    PyTorch 0.1  pytorch-0.1  PyTorch 0.1.12 on Python 3.  floydhub/pytorch     pytorch-0.1:py2  PyTorch 0.1.12 on Python 2.  floydhub/pytorch    Theano 0.8  theano-0.8  Theano rel-0.8.2 + Keras 1.2.2 on Python3.  floydhub/theano     theano-0.8:py2  Theano rel-0.8.2 + Keras 1.2.2 on Python2.  floydhub/theano    Theano 0.9  theano-0.9  Theano rel-0.8.2 + Keras 2.0.3 on Python3.  floydhub/theano     theano-0.9:py2  Theano rel-0.8.2 + Keras 2.0.3 on Python2.  floydhub/theano    Caffe  caffe  Caffe rc4 on Python3.  floydhub/caffe     caffe:py2  Caffe rc4 on Python2.  floydhub/caffe    Torch  torch  Torch 7 with Python 3 env.  floydhub/torch     torch:py2  Torch 7 with Python 2 env.  floydhub/torch    Chainer 1.23  chainer-1.23  Chainer 1.23.0 on Python 3.  floydhub/chainer     chainer-1.23:py2  Chainer 1.23.0 on Python 2.  floydhub/chainer    Chainer 2.0  chainer-2.0  Chainer 1.23.0 on Python 3.  floydhub/chainer     chainer-2.0:py2  Chainer 1.23.0 on Python 2.  floydhub/chainer    MxNet (beta)  mxnet:py2  MxNet 0.9.3a on Python 2.  floydhub/mxnet    Kur  kur  Kur 0.3.0 on Python 3.  floydhub/kur     All environments are available for both CPU and GPU execution. For example,  To run a Python2 Tensorflow job on CPU $ floyd run --env tensorflow:py2  python mnist_cnn.py   To run a Python2 Tensorflow job on GPU (CUDA, cuDNN, etc. installed) $ floyd run --env tensorflow:py2 --gpu  python mnist_cnn.py   The following software packages (in addition to many other common libraries) are available in all the environments: h5py, iPython, Jupyter, matplotlib, numpy, OpenCV, Pandas, Pillow, scikit-learn, scipy, sklearn", 
            "title": "Environments"
        }, 
        {
            "location": "/guides/environments/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/", 
            "text": "Floydhub's \nenvironments\n come with many common deep learning and machine learning packages and dependencies preinstalled. Examples of pre-installed packages include \nnumpy\n, \nscipy\n, \nOpenCV\n, \nOpenAI Gym\n, \nSpaCy\n, etc.\n\n\nIf you need additional or custom packages, you can install them before running your job.\n\n\nInstalling Python dependencies\n\n\nIf your code needs additional Python packages at run time, you can add them to a special file named \nfloyd_requirements.txt\n. \n\n\nIt is similar to Python's \nrequirements.txt\n file and should be present in the same directory from where you issue the \nfloyd run\n command. This is a special file that will be read before your job is started and the packages listed here will be installed before running your job.\n\n\n\n\nHere is an \nfloyd_requirements.txt\n example file:\n\n\nredis\n\ntqdm\n==\n4\n.11.2\n\n\n\n\nWhen this file is present in the project's root directory, any job that is run inside this project will have the \nredis\n and \ntqdm\n (version \n4.11.2\n) packages installed and available at runtime.\n\n\nNotes\n\n\n\n\nOnly Python packages\n: This will only install Python packages available in \nPyPi\n. Please ensure that the package you are trying to install is available.\n\n\nOne package per line\n: Ensure that you have only one package per line in \nfloyd_requirements.txt\n\n\nInstalling specific versions\n: You can install specific versions of packages using the \npackage\n==\nversion\n notation. For example, an entry \ntqdm\n will install the latest version of the package, but \ntqdm==4.11.2\n will force install that specific version.\n\n\n\n\nInstalling dependencies inside Jupyter Notebook\n\n\nYou can install packages (Python or otherwise) interactively inside Jupyter Notebooks. To execute a non-Python command inside a Notebook, prepend it with \n!\n.\n\n\nFor example, to install \ntextblob\n, you can execute \n!pip install textblob\n inside your Notebook:\n\n\n\n\nYou can also use this method to install non-Python packages. For example, to install \nOpenAI Universe\n inside your Notebook, you can execute \n!git clone https://github.com/openai/universe.git \n cd universe \n pip install -e .\n\n\n\n\nInstalling other dependencies\n\n\nYou might want to install non-Python packages or other packages that have custom installation steps. If you are using a Jupyter Notebook, you can follow \nthese steps\n to install arbitrary packages interactively.\n\n\nIf you are running a script using the \nfloyd run \ncommand\n command, you can do one of the following:\n\n\n\n\nInclude the installation steps in the \ncommand\n\n\n\n\nFor example, to install \nOpenAI Universe\n before running your actual script \ntrain.py\n:\n\n\n$ floyd run \ngit clone https://github.com/openai/universe.git \n cd universe \n pip install -e . \n python train.py\n\n\n\n\n\nThis will clone the Universe git repo, install it and then execute \npython train.py\n.\n\n\n\n\nCreate an installation script\n:\n\n\n\n\nIncluding the setup instructions in the \nrun\n command can get unwieldy very soon. An alternative would be to create a bash script with the sequence of setup commands (say, \nsetup.sh\n) and then execute this bash script as part of your \nfloyd run\n.\n\n\nsetup.sh\n\n\n#!/bin/bash\n\n\ngit clone https://github.com/openai/universe.git\n\ncd\n universe\npip install -e . \n\n\n\n\nExecute the setup bash script in your \nfloyd run\n command before your actual job:\n\n\n$ floyd run \nbash setup.sh \n python train.py\n\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Install Extra Dependencies"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/#installing-python-dependencies", 
            "text": "If your code needs additional Python packages at run time, you can add them to a special file named  floyd_requirements.txt .   It is similar to Python's  requirements.txt  file and should be present in the same directory from where you issue the  floyd run  command. This is a special file that will be read before your job is started and the packages listed here will be installed before running your job.   Here is an  floyd_requirements.txt  example file:  redis tqdm == 4 .11.2  When this file is present in the project's root directory, any job that is run inside this project will have the  redis  and  tqdm  (version  4.11.2 ) packages installed and available at runtime.", 
            "title": "Installing Python dependencies"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/#notes", 
            "text": "Only Python packages : This will only install Python packages available in  PyPi . Please ensure that the package you are trying to install is available.  One package per line : Ensure that you have only one package per line in  floyd_requirements.txt  Installing specific versions : You can install specific versions of packages using the  package == version  notation. For example, an entry  tqdm  will install the latest version of the package, but  tqdm==4.11.2  will force install that specific version.", 
            "title": "Notes"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/#installing-dependencies-inside-jupyter-notebook", 
            "text": "You can install packages (Python or otherwise) interactively inside Jupyter Notebooks. To execute a non-Python command inside a Notebook, prepend it with  ! .  For example, to install  textblob , you can execute  !pip install textblob  inside your Notebook:   You can also use this method to install non-Python packages. For example, to install  OpenAI Universe  inside your Notebook, you can execute  !git clone https://github.com/openai/universe.git   cd universe   pip install -e .", 
            "title": "Installing dependencies inside Jupyter Notebook"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/#installing-other-dependencies", 
            "text": "You might want to install non-Python packages or other packages that have custom installation steps. If you are using a Jupyter Notebook, you can follow  these steps  to install arbitrary packages interactively.  If you are running a script using the  floyd run  command  command, you can do one of the following:   Include the installation steps in the  command   For example, to install  OpenAI Universe  before running your actual script  train.py :  $ floyd run  git clone https://github.com/openai/universe.git   cd universe   pip install -e .   python train.py   This will clone the Universe git repo, install it and then execute  python train.py .   Create an installation script :   Including the setup instructions in the  run  command can get unwieldy very soon. An alternative would be to create a bash script with the sequence of setup commands (say,  setup.sh ) and then execute this bash script as part of your  floyd run .  setup.sh  #!/bin/bash \n\ngit clone https://github.com/openai/universe.git cd  universe\npip install -e .   Execute the setup bash script in your  floyd run  command before your actual job:  $ floyd run  bash setup.sh   python train.py", 
            "title": "Installing other dependencies"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/basics/using_gpu/", 
            "text": "Running your job on CPU vs. GPU\n\n\nWhen you run a job using the \nfloyd run\n command, it is executed on a CPU instance on FloydHub's servers, by default. \n\n\n$ floyd run \npython mnist_cnn.py\n\n\n\n\n\nYou can also force your job to execute on on a CPU using the \n--cpu\n flag\n\n\n$ floyd run --cpu \npython mnist_cnn.py\n\n\n\n\n\nIf you want to run your job on a GPU, simply add the \n--gpu\n flag. Just make sure your code is optimized to use the available GPU.\n\n\n$ floyd run --gpu \npython mnist_cnn.py\n\n\n\n\n\nChecking GPU stats\n\n\nYou can check the GPU stats by running a dummy job that executes the \nnvidia-smi\n command.\n\n\n$ floyd run --gpu \nnvidia-smi\n\nSyncing code...\n...\n\n$ floyd logs -t \nJOB_NAME\n\n\nMon Jul \n31\n \n22\n:45:14 \n2017\n       \n+-----------------------------------------------------------------------------+\n\n|\n NVIDIA-SMI \n375\n.66                 Driver Version: \n375\n.66                    \n|\n\n\n|\n-------------------------------+----------------------+----------------------+\n\n|\n GPU  Name        Persistence-M\n|\n Bus-Id        Disp.A \n|\n Volatile Uncorr. ECC \n|\n\n\n|\n Fan  Temp  Perf  Pwr:Usage/Cap\n|\n         Memory-Usage \n|\n GPU-Util  Compute M. \n|\n\n\n|\n===============================\n+\n======================\n+\n======================\n|\n\n\n|\n   \n0\n  Tesla K80           Off  \n|\n \n0000\n:00:1E.0     Off \n|\n                    \n0\n \n|\n\n\n|\n N/A   43C    P8    25W / 149W \n|\n      0MiB / 11439MiB \n|\n      \n0\n%      Default \n|\n\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n\n|\n Processes:                                                       GPU Memory \n|\n\n\n|\n  GPU       PID  Type  Process name                               Usage      \n|\n\n\n|\n=============================================================================\n|\n\n\n|\n  No running processes found                                                 \n|\n\n+-----------------------------------------------------------------------------+\n\n\n\n\nIf you are using a Jupyter Notebook, you can also just execute the \n!nvidia-smi\n command inside it. (\nMake note\n of the \n!\n character at the beginning of the command)", 
            "title": "Using CPU vs GPU"
        }, 
        {
            "location": "/guides/basics/using_gpu/#running-your-job-on-cpu-vs-gpu", 
            "text": "When you run a job using the  floyd run  command, it is executed on a CPU instance on FloydHub's servers, by default.   $ floyd run  python mnist_cnn.py   You can also force your job to execute on on a CPU using the  --cpu  flag  $ floyd run --cpu  python mnist_cnn.py   If you want to run your job on a GPU, simply add the  --gpu  flag. Just make sure your code is optimized to use the available GPU.  $ floyd run --gpu  python mnist_cnn.py", 
            "title": "Running your job on CPU vs. GPU"
        }, 
        {
            "location": "/guides/basics/using_gpu/#checking-gpu-stats", 
            "text": "You can check the GPU stats by running a dummy job that executes the  nvidia-smi  command.  $ floyd run --gpu  nvidia-smi \nSyncing code...\n...\n\n$ floyd logs -t  JOB_NAME \n\nMon Jul  31   22 :45:14  2017        \n+-----------------------------------------------------------------------------+ |  NVIDIA-SMI  375 .66                 Driver Version:  375 .66                     |  | -------------------------------+----------------------+----------------------+ |  GPU  Name        Persistence-M |  Bus-Id        Disp.A  |  Volatile Uncorr. ECC  |  |  Fan  Temp  Perf  Pwr:Usage/Cap |          Memory-Usage  |  GPU-Util  Compute M.  |  | =============================== + ====================== + ====================== |  |     0   Tesla K80           Off   |   0000 :00:1E.0     Off  |                      0   |  |  N/A   43C    P8    25W / 149W  |       0MiB / 11439MiB  |        0 %      Default  | \n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+ |  Processes:                                                       GPU Memory  |  |   GPU       PID  Type  Process name                               Usage       |  | ============================================================================= |  |   No running processes found                                                  | \n+-----------------------------------------------------------------------------+  If you are using a Jupyter Notebook, you can also just execute the  !nvidia-smi  command inside it. ( Make note  of the  !  character at the beginning of the command)", 
            "title": "Checking GPU stats"
        }, 
        {
            "location": "/faqs/environments/", 
            "text": "Can I switch between Python 2 and Python 3 environments?\n\n\nMost of our Python based \nenvironments\n support both Python 2 and Python 3.\n\n\nThe default environments are Python 3. But you can use Python 2 by using the \n:py2\n tag with the environment name, when available. For example,\n\n\n\n\n\n\nTensorflow 1.2 with Python 3\n\n$ floyd run --env tensorflow-1.2\n\n\n\n\n\n\n\nTensorflow 1.2 with Python 2\n\n$ floyd run --env tensorflow-1.2:py2\n\n\n\n\n\n\n\nWe currently have Python 2 and 3 environments for Tensorflow, Theano, Caffe, PyTorch and Chainer. Please see our \ncomplete list of environments\n for more details on the \n--env\n name.", 
            "title": "Troubleshooting & FAQs"
        }, 
        {
            "location": "/faqs/environments/#can-i-switch-between-python-2-and-python-3-environments", 
            "text": "Most of our Python based  environments  support both Python 2 and Python 3.  The default environments are Python 3. But you can use Python 2 by using the  :py2  tag with the environment name, when available. For example,    Tensorflow 1.2 with Python 3 $ floyd run --env tensorflow-1.2    Tensorflow 1.2 with Python 2 $ floyd run --env tensorflow-1.2:py2    We currently have Python 2 and 3 environments for Tensorflow, Theano, Caffe, PyTorch and Chainer. Please see our  complete list of environments  for more details on the  --env  name.", 
            "title": "Can I switch between Python 2 and Python 3 environments?"
        }, 
        {
            "location": "/guides/ssh/", 
            "text": "Quick Look\n\n\n$ floyd run --mode jupyter\n\n\n\n\n\n\n\n\nYou can start a bash session into your job's environment by running your job in\nJupyter Notebook mode:\n\n\n$ floyd run --mode jupyter\nCreating project run. Total upload size: \n183\n.0B\nSyncing code ...\n\n[================================]\n \n916\n/916 - \n00\n:00:00\n\nJOB NAME\n--------------------\nmckay/projects/ssh/1\n\nSetting up your instance and waiting \nfor\n Jupyter notebook to become available\n\nPath to jupyter notebook: https://floydhub.com/notebooks/YXau92xMFUshUbMdKqKVVX\n\n\n\n\nWhen you visit your running Jupyter Notebook in the browser, click the \nNew\n\nbutton in the top right corner of the screen and select the \nTerminal\n option\nas shown below:\n\n\n\n\nThat button will launch a terminal session in your running instance with root\naccess:\n\n\n\nCurrently, FloydHub does not offer true SSH access into instances, but the\nmethod described above is sufficient for what most users request SSH for.", 
            "title": "SSH into a Job"
        }, 
        {
            "location": "/guides/data/storing_output/", 
            "text": "Overview\n\n\nSaving information generated during a job is easy.\n\n\nOn a FloydHub deep learning server your code has access to a directory called\n\n/output\n. The \n/output\n directory is a special directory that is used to store\ninformation you want to save for future use after a job finishes. Anything\nsaved in the \n/output\n directory at the time a job finishes will be preserved\nand can be accessed and reused later.\n\n\nThe most common thing users save is model checkpoints, but anything that ends\nup in the \n/output\n directory at the end of a job will be saved (use your\nimagination!).\n\n\nLet's work through a couple of examples to see how to save data during a job.\n\n\nExample 1\n\n\nThis job prints the string \"Hello, world!\", and saves it to a file called\n\nhello.txt\n. Because \nhello.txt\n is located in the \n/output\n directory, it will\nbe saved and available after the job finishes:\n\n\n$ floyd init my_awesome_hello_world_project\n$ floyd run \necho \nHello, world!\n \n /output/hello.txt\n\nSyncing code ...\n\n\n\n\n\n\nNote\n\n\nIf you are not familiar with what\n\necho \nHello, world!\n \n /output/hello.txt\n does, here's a quick\nexplanation:\n\n\n\n\nThe \necho \nHello, world!\n part outputs the string \nHello, world!\n.\n\n\nThe \n part of the command redirects the printed output of \necho \nHello, world!\n\n  (which is, of course, \nHello world!\n) to the file specified after the \n.\n\n\nThe \n/output/hello.txt\n part of the command specifies where the \nHello,world!\n should be written to: \n/output/hello.txt\n. Because \nhello.txt\n is\n  in the \n/output\n directory, it will be preserved for future reference and\n  use.\n\n\n\n\n\n\nExample 2\n\n\nIn this example, we'll use Python to save some data to a file in the \n/output\n\ndirectory. Put this code in a file named \nsave_example.py\n:\n\n\nwith\n \nopen\n(\n/output/myfile.txt\n,\n \na\n)\n \nas\n \nf\n:\n\n    \nf\n.\nwrite\n(\nPlease save me!\n\\n\n)\n\n\n\n\n\nIf you run this code locally on your computer, you'll probably get something\nlike this:\n\n\nTraceback \n(\nmost recent call last\n)\n:\n  File \nsave_example.py\n, line \n1\n, in \nmodule\n\n    with open\n(\n/output/myfile.txt\n, \na\n)\n as f:\nIOError: \n[\nErrno \n2\n]\n No such file or directory: \n/output/myfile.txt\n\n\n\n\n\nThat's because there is no \n/output\n directory on your computer. In contrast,\nevery job on FloydHub runs on a server that has a \n/output\n directory, so the\ncommand won't fail on FloydHub. Let's run it with the following commands:\n\n\n$ floyd init save_example_2\n$ floyd run \npython save_example.py\n\nCreating project run. Total upload size: \n267\n.0B\nSyncing code ...\n\n\nSuccess! We can now view the output, download it, or even use it again in\nfuture jobs.\n\n\nNow that we've completed a couple trivial examples, let's do something more\nuseful and realistic.\n\n\nExample 3\n\n\nHere is a sample Tensorflow example that saves a model checkpoint. Because we\nwrite (save) the data to the \n/output\n directory, we'll be able to use it\nlater. A future job can use this model checkpoint as a starting point.\nConsider this partial code, and note the call to \nsaver.save(sess,\n/output/model.ckpt\n)\n:\n\n\nimport\n \ntensorflow\n \nas\n \ntf\n\n\n\n...\n\n\n\nsaver\n \n=\n \ntf\n.\ntrain\n.\nSaver\n()\n\n\nwith\n \ntf\n.\nSession\n()\n \nas\n \nsess\n:\n\n    \nsess\n.\nrun\n(\ninit\n)\n\n    \n...\n\n    \nsave_path\n \n=\n \nsaver\n.\nsave\n(\nsess\n,\n \n/output/model.ckpt\n)\n\n    \nprint\n(\nModel saved in file: \n%s\n \n%\n \nsave_path\n)\n\n    \n...\n\n\n\n\n\nBecause model is stored under the special \n/output\n directory, it will be saved\neven after your job ends, and can be used again in future jobs.\n\n\nViewing Saved Output Data\n\n\nYou can view the saved output of a job using the \nfloyd output\n command:\n\n\n$ floyd output floydhub/projects/quick-start/1/\nOpening output directory in your browser...\n\n\n\n\nAlternatively, you can browse or download the saved output by visiting the\n\nOutput\n tab of the job on your dashboard as shown in the image below:\n\n\n\n\nUsing output as a data source\n\n\nYou can use the output of one job as the input to your next job. To see how to\nmount output data, please see \nthis guide\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Save Output"
        }, 
        {
            "location": "/guides/data/storing_output/#overview", 
            "text": "Saving information generated during a job is easy.  On a FloydHub deep learning server your code has access to a directory called /output . The  /output  directory is a special directory that is used to store\ninformation you want to save for future use after a job finishes. Anything\nsaved in the  /output  directory at the time a job finishes will be preserved\nand can be accessed and reused later.  The most common thing users save is model checkpoints, but anything that ends\nup in the  /output  directory at the end of a job will be saved (use your\nimagination!).  Let's work through a couple of examples to see how to save data during a job.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/data/storing_output/#example-1", 
            "text": "This job prints the string \"Hello, world!\", and saves it to a file called hello.txt . Because  hello.txt  is located in the  /output  directory, it will\nbe saved and available after the job finishes:  $ floyd init my_awesome_hello_world_project\n$ floyd run  echo  Hello, world!    /output/hello.txt \nSyncing code ...   Note  If you are not familiar with what echo  Hello, world!    /output/hello.txt  does, here's a quick\nexplanation:   The  echo  Hello, world!  part outputs the string  Hello, world! .  The   part of the command redirects the printed output of  echo  Hello, world! \n  (which is, of course,  Hello world! ) to the file specified after the  .  The  /output/hello.txt  part of the command specifies where the  Hello,world!  should be written to:  /output/hello.txt . Because  hello.txt  is\n  in the  /output  directory, it will be preserved for future reference and\n  use.", 
            "title": "Example 1"
        }, 
        {
            "location": "/guides/data/storing_output/#example-2", 
            "text": "In this example, we'll use Python to save some data to a file in the  /output \ndirectory. Put this code in a file named  save_example.py :  with   open ( /output/myfile.txt ,   a )   as   f : \n     f . write ( Please save me! \\n )   If you run this code locally on your computer, you'll probably get something\nlike this:  Traceback  ( most recent call last ) :\n  File  save_example.py , line  1 , in  module \n    with open ( /output/myfile.txt ,  a )  as f:\nIOError:  [ Errno  2 ]  No such file or directory:  /output/myfile.txt   That's because there is no  /output  directory on your computer. In contrast,\nevery job on FloydHub runs on a server that has a  /output  directory, so the\ncommand won't fail on FloydHub. Let's run it with the following commands:  $ floyd init save_example_2\n$ floyd run  python save_example.py \nCreating project run. Total upload size:  267 .0B\nSyncing code ... \nSuccess! We can now view the output, download it, or even use it again in\nfuture jobs.  Now that we've completed a couple trivial examples, let's do something more\nuseful and realistic.", 
            "title": "Example 2"
        }, 
        {
            "location": "/guides/data/storing_output/#example-3", 
            "text": "Here is a sample Tensorflow example that saves a model checkpoint. Because we\nwrite (save) the data to the  /output  directory, we'll be able to use it\nlater. A future job can use this model checkpoint as a starting point.\nConsider this partial code, and note the call to  saver.save(sess, /output/model.ckpt ) :  import   tensorflow   as   tf  ...  saver   =   tf . train . Saver ()  with   tf . Session ()   as   sess : \n     sess . run ( init ) \n     ... \n     save_path   =   saver . save ( sess ,   /output/model.ckpt ) \n     print ( Model saved in file:  %s   %   save_path ) \n     ...   Because model is stored under the special  /output  directory, it will be saved\neven after your job ends, and can be used again in future jobs.", 
            "title": "Example 3"
        }, 
        {
            "location": "/guides/data/storing_output/#viewing-saved-output-data", 
            "text": "You can view the saved output of a job using the  floyd output  command:  $ floyd output floydhub/projects/quick-start/1/\nOpening output directory in your browser...  Alternatively, you can browse or download the saved output by visiting the Output  tab of the job on your dashboard as shown in the image below:", 
            "title": "Viewing Saved Output Data"
        }, 
        {
            "location": "/guides/data/storing_output/#using-output-as-a-data-source", 
            "text": "You can use the output of one job as the input to your next job. To see how to\nmount output data, please see  this guide", 
            "title": "Using output as a data source"
        }, 
        {
            "location": "/guides/data/storing_output/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/delete_output/", 
            "text": "It is not possible to delete just the output of a job. You will have to \ndelete the job itself\n.", 
            "title": "Delete Output"
        }, 
        {
            "location": "/guides/download_output/", 
            "text": "Quick Look\n\n\nOn the job screen on floydhub.com:\n\n\n\n\nOr from the CLI:\n\n$ floyd data clone \nusername\n/\nproject_name\n/\nrun_number\n/output\n\n\n\n\n\nAfter you've \nsaved output\n from a job, you can download\nthe output of the job on floydhub.com or from the CLI.\n\n\nTo learn how to re-use output in a new job, see\n\nthis documentation\n.\n\n\nFrom floydhub.com\n\n\nFrom the job's page on floydhub.com, you can browse and download the output of\nthe job by using the \"Browse\" and \"Download\" icons found on the \"Output\" tab:\n\n\n\n\nFrom the CLI\n\n\nTo open the browsing window from the CLI, use the\n\nfloyd data clone\n command and pass it the path of the\njob's output:\n\n\n$ floyd data clone mckay/projects/my_proj/1/output\nOpening output directory in your browser ...\n\n\n\n\nThe path of the output is \nyour_username\n/projects/\nproject_name\n/\nrun_number\n/output\n.", 
            "title": "Download Saved Output"
        }, 
        {
            "location": "/guides/download_output/#from-floydhubcom", 
            "text": "From the job's page on floydhub.com, you can browse and download the output of\nthe job by using the \"Browse\" and \"Download\" icons found on the \"Output\" tab:", 
            "title": "From floydhub.com"
        }, 
        {
            "location": "/guides/download_output/#from-the-cli", 
            "text": "To open the browsing window from the CLI, use the floyd data clone  command and pass it the path of the\njob's output:  $ floyd data clone mckay/projects/my_proj/1/output\nOpening output directory in your browser ...  The path of the output is  your_username /projects/ project_name / run_number /output .", 
            "title": "From the CLI"
        }, 
        {
            "location": "/guides/browse_output/", 
            "text": "Quick Look\n\n\nOn the job screen on floydhub.com:\n\n\n\n\nOr from the CLI:\n\n$ floyd output \nusername\n/projects/\nproject_name\n/\nrun_number\n\n\n\n\n\n\nAfter you've \nsaved output\n from a job, you can browse the\noutput of the job on floydhub.com. Using the CLI, you can open to the job's\nin-browser data-browsing page.\n\n\nFor downloading output, see \nthis documentation\n.\n\n\nTo learn how to re-use output in a new job, see \nthis\ndocumentation\n.\n\n\nFrom floydhub.com\n\n\nFrom the job's page on floydhub.com, you can browse and download the output of\nthe job by using the \"Browse\" and \"Download\" icons found on the \"Output\" tab:\n\n\n\n\nFrom the CLI\n\n\nTo open the browsing window from the CLI, use the\n\nfloyd data clone\n command and pass it the path of the\njob's output:\n\n\n$ floyd output mckay/projects/my_proj/1\nOpening output directory in your browser ...\n\n\n\n\nThe path of the output is \nyour_username\n/projects/\nproject_name\n/\nrun_number\n/output\n.", 
            "title": "Browse Saved Output"
        }, 
        {
            "location": "/guides/browse_output/#from-floydhubcom", 
            "text": "From the job's page on floydhub.com, you can browse and download the output of\nthe job by using the \"Browse\" and \"Download\" icons found on the \"Output\" tab:", 
            "title": "From floydhub.com"
        }, 
        {
            "location": "/guides/browse_output/#from-the-cli", 
            "text": "To open the browsing window from the CLI, use the floyd data clone  command and pass it the path of the\njob's output:  $ floyd output mckay/projects/my_proj/1\nOpening output directory in your browser ...  The path of the output is  your_username /projects/ project_name / run_number /output .", 
            "title": "From the CLI"
        }, 
        {
            "location": "/guides/reusing_output/", 
            "text": "You can link jobs by mounting the output of one job as the input of a new job.\nThis allows you to iterate on the ouput of a past job. Output is mounted to a\njob in the same way data is. To learn more about mounting data, see \nthis\nguide\n.\n\n\nYou can refer to the output of a job by its name with \n/output\n appended to it.\nFor example: \nfloydhub/projects/handwriting-recognition/12/output\n refers to\nthe output of the job \nfloydhub/projects/handwriting-recognition/12\n\n\nUse the \n--data\n flag in the \nfloyd run\n command, mount past output to a job,\njust as you would to mount a dataset. For example:\n\n\n$ floyd run \n\\\n\n  --data floydhub/projects/handwriting-recognition/12/output:filtered_training_data \n\\\n\n  \npython train.py\n\n\n\n\n\nThis will make the output of \nfloydhub/projects/handwriting-recognition/12\n\navailable at \n/filtered_training_data\n for the new job to use.\n\n\nNote: You need to have access to a job to be able to mount its output.", 
            "title": "Using Previous Output in a New Job"
        }, 
        {
            "location": "/guides/basics/install/", 
            "text": "Floyd CLI is a python based command line tool to interact with FloydHub from your terminal.\n\n\nfloyd-cli\n is available on \npypi\n and\nruns on both Python 2.7 and Python 3.5.\n\n\nUse pip to install the CLI.\n\n\n$ pip install -U floyd-cli\n\n\n\n\nUse pip3 if you only want to install the CLI for python 3:\n\n\n$ pip3 install -U floyd-cli\n\n\n\n\nAfter installation you can view the commands supported by the CLI using the\n\n--help\n option.\n\n\n$ floyd --help\nUsage: floyd \n[\nOPTIONS\n]\n COMMAND \n[\nARGS\n]\n...\n\n  Floyd CLI interacts with Floyd server and executes your commands. More\n  \nhelp\n is available under each \ncommand\n listed below.\n\n...\n\n\n\n\nDetailed documentation for the floyd commands is available in the \ndocumentation\n.\n\n\nHaving trouble installing the CLI?\n\n\nSee the list of \nFAQs related to installation\n.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Install Floyd CLI"
        }, 
        {
            "location": "/guides/basics/install/#having-trouble-installing-the-cli", 
            "text": "See the list of  FAQs related to installation .", 
            "title": "Having trouble installing the CLI?"
        }, 
        {
            "location": "/guides/basics/install/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/basics/login/", 
            "text": "Quick Preparation Checklist\n\n\n\n\nYou must have a \nFloydHub account\n\n\nYou must be \nlogged in to your web dashboard\n\n\nYou must have \nfloyd-cli\n \ninstalled on your computer\n\n\n\n\nYou can use the \nfloyd login\n command to log in to\nyour FloydHub account through your command line\n\n\n$ floyd login\nAuthentication token page will now open in your browser. Continue? \n[\nY/n\n]\n: y\nPlease copy and paste the authentication token.\nThis is an invisible field. Paste token and press ENTER:\n\n\n\n\nThis will open your browser and display your authentication token. You can also\naccess this directly at\n\nfloydhub.com/settings/security\n.\n(\nNote:\n only visible if you are \nlogged in\n to\nyour web dashboard).\n\n\n\n\nCopy the token and paste it in your terminal.\n\n\nLogging in without opening a browser\n\n\nSometimes you may need to log in to Floyd on a computer that can't open a\nbrowser. Using your\n\ntoken from floydhub.com\n, you can\nlog in without opening a browser by passing the \n--token\n flag to\n\nfloyd login\n:\n\n\n$ floyd login --token\nPlease copy and paste the authentication token.\nThis is an invisible field. Paste token and press ENTER:\nLogin Successful\n\n\n\n\nHaving problems with logging in using floyd-cli?\n\n\n\n\nAre you on a Windows machine? Please see our \nFAQs for Windows\n\n\nOther problems? Check out our \nLogin FAQs\n or \ncontact us", 
            "title": "Log In Using Floyd CLI"
        }, 
        {
            "location": "/guides/basics/login/#quick-preparation-checklist", 
            "text": "You must have a  FloydHub account  You must be  logged in to your web dashboard  You must have  floyd-cli   installed on your computer   You can use the  floyd login  command to log in to\nyour FloydHub account through your command line  $ floyd login\nAuthentication token page will now open in your browser. Continue?  [ Y/n ] : y\nPlease copy and paste the authentication token.\nThis is an invisible field. Paste token and press ENTER:  This will open your browser and display your authentication token. You can also\naccess this directly at floydhub.com/settings/security .\n( Note:  only visible if you are  logged in  to\nyour web dashboard).   Copy the token and paste it in your terminal.", 
            "title": "Quick Preparation Checklist"
        }, 
        {
            "location": "/guides/basics/login/#logging-in-without-opening-a-browser", 
            "text": "Sometimes you may need to log in to Floyd on a computer that can't open a\nbrowser. Using your token from floydhub.com , you can\nlog in without opening a browser by passing the  --token  flag to floyd login :  $ floyd login --token\nPlease copy and paste the authentication token.\nThis is an invisible field. Paste token and press ENTER:\nLogin Successful", 
            "title": "Logging in without opening a browser"
        }, 
        {
            "location": "/guides/basics/login/#having-problems-with-logging-in-using-floyd-cli", 
            "text": "Are you on a Windows machine? Please see our  FAQs for Windows  Other problems? Check out our  Login FAQs  or  contact us", 
            "title": "Having problems with logging in using floyd-cli?"
        }, 
        {
            "location": "/commands/", 
            "text": "Floyd Commands\n\n\nBelow are the commands that are part of Floyd CLI.\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nfloyd login\n\n\nLogin to Floyd.\n\n\n\n\n\n\nfloyd init\n\n\nInitialize a Floyd project\n\n\n\n\n\n\nfloyd run\n\n\nRun your project on Floyd\n\n\n\n\n\n\nfloyd data\n\n\nManage data on Floyd\n\n\n\n\n\n\nfloyd logs\n\n\nStream logs of your job\n\n\n\n\n\n\nfloyd status\n\n\nCheck status of your jobs\n\n\n\n\n\n\nfloyd clone\n\n\nClone an existing floyd project\n\n\n\n\n\n\nfloyd output\n\n\nView the output of a job\n\n\n\n\n\n\nfloyd info\n\n\nSee details of a job\n\n\n\n\n\n\nfloyd stop\n\n\nTerminate a job\n\n\n\n\n\n\nfloyd logout\n\n\nLogout from Floyd\n\n\n\n\n\n\nfloyd version\n\n\nSee version of floyd client\n\n\n\n\n\n\nfloyd upgrade\n\n\nUpgrade floyd client", 
            "title": "floyd"
        }, 
        {
            "location": "/commands/#floyd-commands", 
            "text": "Below are the commands that are part of Floyd CLI.     Command  Description      floyd login  Login to Floyd.    floyd init  Initialize a Floyd project    floyd run  Run your project on Floyd    floyd data  Manage data on Floyd    floyd logs  Stream logs of your job    floyd status  Check status of your jobs    floyd clone  Clone an existing floyd project    floyd output  View the output of a job    floyd info  See details of a job    floyd stop  Terminate a job    floyd logout  Logout from Floyd    floyd version  See version of floyd client    floyd upgrade  Upgrade floyd client", 
            "title": "Floyd Commands"
        }, 
        {
            "location": "/commands/login/", 
            "text": "Login to Floyd.\n\n\nUsage\n\n\nfloyd login\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--token\n\n\nFalse\n\n\nIf specified, browser will not open. You can paste your token in the command line. \nNote\n: This is only supported with \nversion 0.7.2+\n of \nfloyd-cli\n. If you get an error, please upgrade using \npip install -U floyd-cli\n and try again\n\n\n\n\n\n\n\n\nDescription\n\n\nYou need to login to Floyd before running any other command. The login flow will require an access token from the Floydhub \nwebsite. You will be prompted to enter you credentials to get your access token. Copy and paste the token on the command line \nto complete login.\n\n\nExample\n\n\nTo automatically open your browser\n\n$ floyd login\nAuthentication token page will now open in your browser. Continue? \n[\nY/n\n]\n:\nPlease paste the token here:\nLogin Successful\n\n\n\nIn case you use remote machines and do not have access to the browser, you can copy the token from the \n\ndashboard\n and use the \n--token\n parameter when you login.\n\n$ floyd login --token\nPlease copy and paste the token here:\nLogin Successful\n\n\n\nAuthentication Tokens\n\n\nFloyd uses \nJson Web Tokens\n for authentication. Your \ntoken will be stored in the \n~/.floydconfig\n file. They are valid for 7 days after \nwhich you need to login again. This file will be removed when you \nlogout\n.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd login"
        }, 
        {
            "location": "/commands/login/#usage", 
            "text": "floyd login", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/login/#options", 
            "text": "Name, shorthand  Default  Description      --token  False  If specified, browser will not open. You can paste your token in the command line.  Note : This is only supported with  version 0.7.2+  of  floyd-cli . If you get an error, please upgrade using  pip install -U floyd-cli  and try again", 
            "title": "Options"
        }, 
        {
            "location": "/commands/login/#description", 
            "text": "You need to login to Floyd before running any other command. The login flow will require an access token from the Floydhub \nwebsite. You will be prompted to enter you credentials to get your access token. Copy and paste the token on the command line \nto complete login.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/login/#example", 
            "text": "To automatically open your browser $ floyd login\nAuthentication token page will now open in your browser. Continue?  [ Y/n ] :\nPlease paste the token here:\nLogin Successful  In case you use remote machines and do not have access to the browser, you can copy the token from the  dashboard  and use the  --token  parameter when you login. $ floyd login --token\nPlease copy and paste the token here:\nLogin Successful", 
            "title": "Example"
        }, 
        {
            "location": "/commands/login/#authentication-tokens", 
            "text": "Floyd uses  Json Web Tokens  for authentication. Your \ntoken will be stored in the  ~/.floydconfig  file. They are valid for 7 days after \nwhich you need to login again. This file will be removed when you  logout .", 
            "title": "Authentication Tokens"
        }, 
        {
            "location": "/commands/login/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/init/", 
            "text": "Initialize a Floyd project.\n\n\nUsage\n\n\nfloyd init PROJECT_NAME\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPROJECT_NAME\n\n\n\n\nName of your project (Pick a name from the projects page in web dashboard)\n\n\n\n\n\n\n\n\nDescription\n\n\nThis command initializes the current directory for a given project name and tracks all the files and subdirectories. \nMake sure that the project name you enter here already exists in Floyd. In case the project name does not exist, \nthe CLI will open the create project page in your browser.\n\n\nTo initialize a new dataset you should use \nfloyd data init\n command.\n\n\nThe init command also creates a \n.floydignore\n file. Any files and directories you do not want Floyd to track can be added \nto this file. When you run your project on Floyd, these files will not be uploaded. More details on how floydignore \nfile works is available \nhere\n.\n\n\nExample\n\n\nInitialize a floyd project in your project directory.\n\n$ \ncd\n /code/project\n$ floyd init style-transfer\nProject \nstyle-transfer\n initialized in current directory\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd init"
        }, 
        {
            "location": "/commands/init/#usage", 
            "text": "floyd init PROJECT_NAME", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/init/#options", 
            "text": "Name, shorthand  Default  Description      PROJECT_NAME   Name of your project (Pick a name from the projects page in web dashboard)", 
            "title": "Options"
        }, 
        {
            "location": "/commands/init/#description", 
            "text": "This command initializes the current directory for a given project name and tracks all the files and subdirectories. \nMake sure that the project name you enter here already exists in Floyd. In case the project name does not exist, \nthe CLI will open the create project page in your browser.  To initialize a new dataset you should use  floyd data init  command.  The init command also creates a  .floydignore  file. Any files and directories you do not want Floyd to track can be added \nto this file. When you run your project on Floyd, these files will not be uploaded. More details on how floydignore \nfile works is available  here .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/init/#example", 
            "text": "Initialize a floyd project in your project directory. $  cd  /code/project\n$ floyd init style-transfer\nProject  style-transfer  initialized in current directory", 
            "title": "Example"
        }, 
        {
            "location": "/commands/init/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/run/", 
            "text": "Run your project on Floyd.\n\n\nUsage\n\n\nfloyd run \n[\nOPTIONS\n]\n \n[\nCOMMAND\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--gpu/--cpu\n\n\ncpu\n\n\nIf specified, runs the job on a GPU (G1) instance or CPU (C1) instance. See instance specifications on the \npricing\n page.\n\n\n\n\n\n\n--data \nID:mount\n\n\n\n\nID\n of the data source to link to. \nmount\n specifies the path to mount it at. You can use this parameter multiple times. See \ndata\n section for more details.\n\n\n\n\n\n\n--mode [jupyter|serve]\n\n\ncommand\n\n\nSpecify the mode you want to run the project. The default behavior executes the command you specify. See \njupyter\n and \nserve\n sections for more info on them.\n\n\n\n\n\n\n--no-open\n\n\n\n\nYou can disable the CLI from opening the jupyter notebook url. It will print the URL instead.\n\n\n\n\n\n\n--env [tensorflow:py3|tensorflow:py2|...]\n\n\nkeras:py3\n\n\nSpecify the environment you want to use for your project. See \nenvironments\n for the full list.\n\n\n\n\n\n\n--message \nmessage_str\n\n\n\n\nAttach a message to the specific run of the project.\n\n\n\n\n\n\n--tensorboard\n\n\n\n\nStarts tensorboard in the environment. Tensorboard URL can be found in the dashboard.\n\n\n\n\n\n\ncommand\n\n\n\n\nCommand to execute when running your project on Floyd.\n\n\n\n\n\n\n\n\nDescription\n\n\nThis command syncs the code tracked by the CLI to the Floyd servers and executes your command. You can see the progress\nwith \nstatus\n command. To view the logs from your code use \nlogs\n command.\n\n\nExample\n\n\n$ floyd run \npython train_tf.py -lr 0.01 -output /output/model.bin\n\nSyncing code ...\nRUN ID                  NAME\n----------------------  -----------------------------\ndTe2cJJrNR2CBD74rSZXPA  floydhub/tensorflow-project/7\n\n...\n$ floyd logs floydhub/tensorflow-project/7\n\n\n\n\nfloyd_requirements.txt\n\n\nFloyd runs standard Docker images for various deep learning frameworks.(See \nenvironments\n for details). If your\ncode requires additional Python dependencies you can specify them in a \nfloyd_requirements.txt\n file and place it at the root\ndirectory of your project. These dependencies will be installed before running your code.\n\n\nExample\n\n\n$ cat floyd_requirements.txt\nPillow\nscipy\n$ floyd run \npython train_tf.py -lr 0.01 -output /output/model.bin\n\n\n\n\n\nJupyter notebook\n\n\nFloyd supports running Jupyter/iPython notebooks on the server. Make sure that the notebook (.ipynb) files are present in the\ncurrent directory. Use \n--mode jupyter\n and you will be presented with a URL to view your Jupyter environment. You do not need\nto specify a command in this mode. See \njupyter\n page for more details.\n\n\nExample\n\n\n$ floyd run --mode jupyter\n...\nPath to jupyter notebook: https://www.floydhub.com/notebooks/g8uGRZFQz85meArJGToEcs\n\n\n\n\nAttaching multiple datasets\n\n\nYou can attach upto 5 datasets when you run a project using the run command. You can specify both\ndatasets you uploaded and output datasets of your previous runs. You can specify the mount point\nalso when you specify the data id to mount.\n\n\nExample:\n\n\n$ floyd run --data floydhub/datasets/cifar/1:training --data floydhub/datasets/faces/21:testing \npython script.py\n\n\n\nThe above datasets will be mounted at \n/training\n and \n/testing\n respectively.\n\n\nServe\n\n\nFloyd can be used to host the model you generated as a REST api. This api can be used to evaluate your model over HTTP.\nUse \n--mode serve\n and you will be presented with a URL to access your API. Floyd currently supports only Flask apps.\nIt runs app.py file and expects the service to run on port 5000. You do not need to specify a command in this mode.\nSee \nserve\n page for more details.\n\n\nExample\n\n\n$ floyd run --mode serve\n...\nPath to service endpoint: https://www.floydhub.com/expose/vbKSKgVYGgZqmM9i3LjLBb\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd run"
        }, 
        {
            "location": "/commands/run/#usage", 
            "text": "floyd run  [ OPTIONS ]   [ COMMAND ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/run/#options", 
            "text": "Name, shorthand  Default  Description      --gpu/--cpu  cpu  If specified, runs the job on a GPU (G1) instance or CPU (C1) instance. See instance specifications on the  pricing  page.    --data  ID:mount   ID  of the data source to link to.  mount  specifies the path to mount it at. You can use this parameter multiple times. See  data  section for more details.    --mode [jupyter|serve]  command  Specify the mode you want to run the project. The default behavior executes the command you specify. See  jupyter  and  serve  sections for more info on them.    --no-open   You can disable the CLI from opening the jupyter notebook url. It will print the URL instead.    --env [tensorflow:py3|tensorflow:py2|...]  keras:py3  Specify the environment you want to use for your project. See  environments  for the full list.    --message  message_str   Attach a message to the specific run of the project.    --tensorboard   Starts tensorboard in the environment. Tensorboard URL can be found in the dashboard.    command   Command to execute when running your project on Floyd.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/run/#description", 
            "text": "This command syncs the code tracked by the CLI to the Floyd servers and executes your command. You can see the progress\nwith  status  command. To view the logs from your code use  logs  command.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/run/#example", 
            "text": "$ floyd run  python train_tf.py -lr 0.01 -output /output/model.bin \nSyncing code ...\nRUN ID                  NAME\n----------------------  -----------------------------\ndTe2cJJrNR2CBD74rSZXPA  floydhub/tensorflow-project/7\n\n...\n$ floyd logs floydhub/tensorflow-project/7", 
            "title": "Example"
        }, 
        {
            "location": "/commands/run/#floyd_requirementstxt", 
            "text": "Floyd runs standard Docker images for various deep learning frameworks.(See  environments  for details). If your\ncode requires additional Python dependencies you can specify them in a  floyd_requirements.txt  file and place it at the root\ndirectory of your project. These dependencies will be installed before running your code.", 
            "title": "floyd_requirements.txt"
        }, 
        {
            "location": "/commands/run/#example_1", 
            "text": "$ cat floyd_requirements.txt\nPillow\nscipy\n$ floyd run  python train_tf.py -lr 0.01 -output /output/model.bin", 
            "title": "Example"
        }, 
        {
            "location": "/commands/run/#jupyter-notebook", 
            "text": "Floyd supports running Jupyter/iPython notebooks on the server. Make sure that the notebook (.ipynb) files are present in the\ncurrent directory. Use  --mode jupyter  and you will be presented with a URL to view your Jupyter environment. You do not need\nto specify a command in this mode. See  jupyter  page for more details.", 
            "title": "Jupyter notebook"
        }, 
        {
            "location": "/commands/run/#example_2", 
            "text": "$ floyd run --mode jupyter\n...\nPath to jupyter notebook: https://www.floydhub.com/notebooks/g8uGRZFQz85meArJGToEcs", 
            "title": "Example"
        }, 
        {
            "location": "/commands/run/#attaching-multiple-datasets", 
            "text": "You can attach upto 5 datasets when you run a project using the run command. You can specify both\ndatasets you uploaded and output datasets of your previous runs. You can specify the mount point\nalso when you specify the data id to mount.", 
            "title": "Attaching multiple datasets"
        }, 
        {
            "location": "/commands/run/#example_3", 
            "text": "$ floyd run --data floydhub/datasets/cifar/1:training --data floydhub/datasets/faces/21:testing  python script.py  \nThe above datasets will be mounted at  /training  and  /testing  respectively.", 
            "title": "Example:"
        }, 
        {
            "location": "/commands/run/#serve", 
            "text": "Floyd can be used to host the model you generated as a REST api. This api can be used to evaluate your model over HTTP.\nUse  --mode serve  and you will be presented with a URL to access your API. Floyd currently supports only Flask apps.\nIt runs app.py file and expects the service to run on port 5000. You do not need to specify a command in this mode.\nSee  serve  page for more details.", 
            "title": "Serve"
        }, 
        {
            "location": "/commands/run/#example_4", 
            "text": "$ floyd run --mode serve\n...\nPath to service endpoint: https://www.floydhub.com/expose/vbKSKgVYGgZqmM9i3LjLBb", 
            "title": "Example"
        }, 
        {
            "location": "/commands/run/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/data/", 
            "text": "Manage your data sets on Floyd. The subcommands are:\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nfloyd data init\n\n\nInitialize a dataset\n\n\n\n\n\n\nfloyd data upload\n\n\nCreate a new dataset version\n\n\n\n\n\n\nfloyd data status\n\n\nList all your datasets\n\n\n\n\n\n\nfloyd data clone\n\n\nClone an existing dataset\n\n\n\n\n\n\nfloyd data delete\n\n\nDelete your datasets\n\n\n\n\n\n\nfloyd data output\n\n\nView contents of a dataset\n\n\n\n\n\n\n\n\nfloyd data init\n\n\nInitialize a Floyd dataset.\n\n\nUsage\n\n\nfloyd data init DATASET_NAME\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDATASET_NAME\n\n\n\n\nName of the dataset (Pick a name from the dataset page in web dashboard)\n\n\n\n\n\n\n\n\nDescription\n\n\nFloyd can manage your experiment datasets and make them available when running your projects.This command initializes the \ncurrent directory and tracks all files and subdirectories. Make sure the dataset name you enter here already \nexists in Floyd. In case the dataset name does not exist, the CLI will open the create dataset page in your browser.\n\n\nThe init command also creates a \n.floydignore\n file. Any files and directories you do not want Floyd to track can be added \nto this file. When you upload this dataset to Floyd, these files will not be uploaded.\n\n\nExample\n\n\nInitialize a floyd dataset in your data directory.\n\n$ \ncd\n /data/mnist\n$ floyd data init mnist-data\nData \nsource\n \nmnist-data\n initialized in current directory\n\n\n\n\n\nfloyd data upload\n\n\nUpload a new version of dataset\n\n\nUsage\n\n\nfloyd data upload\n\n\n\n\nDescription\n\n\nUpload contents of the current directory as a new version of the dataset. This data can now be referred to in the \nrun\n command.\nAt run time the data will be available at the \n/input\n path.\n\n\nFloyd also versions your data so you can choose any specific version to use in your runs.\n\n\nExample\n\n\n$ floyd data upload\nCreating data source. Uploading files ...\nDATA ID                 NAME                \n----------------------  ------------------\nGY3QRFFUA8KpbnqvroTPPW  alice/mnist-data:1  \n\n\nFloyd will generate a data id for the uploaded dataset. This uploaded dataset can be used in your future experiments, if needed,\nusing this data id. See \nhere\n for more details.\n\n\n\n\nfloyd data status\n\n\nView your datasets on Floyd\n\n\nUsage\n\n\nfloyd data status \n[\nNAME or ID\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nName or ID of your data.\n\n\n\n\n\n\n\n\nDescription\n\n\nLists all your datasets on Floyd with more details.\n\n\nExample\n\n\n$ floyd data status\nDATA ID                 CREATED         DISK USAGE    NAME               \n----------------------  --------------  ------------  ------------------\nHYLEc2czGKRpYVm7rGtBoY  \n12\n minutes ago  \n372\n.0 MB      floydhub/mnist:1  \nqNcS5bXHtFdSiMZ35kkEPh  an hour ago     \n456\n.2 MB      floydhub/csr:7    \n\n\n\n\n\n\nfloyd data delete\n\n\nDelete datasets on FloydHub\n\n\nUsage\n\n\nfloyd data delete \n[\nOPTIONS\n]\n \n[\nNAMES or IDS\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNAMES or IDS\n\n\n\n\nOne or more Names or IDs of your data.\n\n\n\n\n\n\n--yes\n, \n-y\n\n\nFalse\n\n\nSkip delete confirmation step\n\n\n\n\n\n\n\n\nDescription\n\n\nDeletes your datasets from FloydHub. This data will no longer \nbe accessible.\n\n\nNote: You do \nnot\n have to be in the project directory to run this command.\n\n\nExample\n\n\n$ floyd data delete floydhub/csr:7\nDelete Data: floydhub/csr:7? \n[\ny/N\n]\n: y\nData deleted\n\n\n\n\n\n\nfloyd data output\n\n\nView datasets\n\n\nUsage\n\n\nfloyd data output \n[\nOPTIONS\n]\n ID\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--url\n, \n-u\n\n\n\n\nOnly print the URL. The data directory can be viewed in the browser.\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nNAME or ID of your data.\n\n\n\n\n\n\n\n\nDescription\n\n\nThe output command gives the url to access a dataset. This command by default opens the data url \nin your default browser.\n\n\nExample\n\n\n$ floyd data output floydhub/csr:11\nOpening output directory in your browser ...\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd data"
        }, 
        {
            "location": "/commands/data/#floyd-data-init", 
            "text": "Initialize a Floyd dataset.", 
            "title": "floyd data init"
        }, 
        {
            "location": "/commands/data/#usage", 
            "text": "floyd data init DATASET_NAME", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#options", 
            "text": "Name, shorthand  Default  Description      DATASET_NAME   Name of the dataset (Pick a name from the dataset page in web dashboard)", 
            "title": "Options"
        }, 
        {
            "location": "/commands/data/#description", 
            "text": "Floyd can manage your experiment datasets and make them available when running your projects.This command initializes the \ncurrent directory and tracks all files and subdirectories. Make sure the dataset name you enter here already \nexists in Floyd. In case the dataset name does not exist, the CLI will open the create dataset page in your browser.  The init command also creates a  .floydignore  file. Any files and directories you do not want Floyd to track can be added \nto this file. When you upload this dataset to Floyd, these files will not be uploaded.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example", 
            "text": "Initialize a floyd dataset in your data directory. $  cd  /data/mnist\n$ floyd data init mnist-data\nData  source   mnist-data  initialized in current directory", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#floyd-data-upload", 
            "text": "Upload a new version of dataset", 
            "title": "floyd data upload"
        }, 
        {
            "location": "/commands/data/#usage_1", 
            "text": "floyd data upload", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#description_1", 
            "text": "Upload contents of the current directory as a new version of the dataset. This data can now be referred to in the  run  command.\nAt run time the data will be available at the  /input  path.  Floyd also versions your data so you can choose any specific version to use in your runs.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example_1", 
            "text": "$ floyd data upload\nCreating data source. Uploading files ...\nDATA ID                 NAME                \n----------------------  ------------------\nGY3QRFFUA8KpbnqvroTPPW  alice/mnist-data:1   \nFloyd will generate a data id for the uploaded dataset. This uploaded dataset can be used in your future experiments, if needed,\nusing this data id. See  here  for more details.", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#floyd-data-status", 
            "text": "View your datasets on Floyd", 
            "title": "floyd data status"
        }, 
        {
            "location": "/commands/data/#usage_2", 
            "text": "floyd data status  [ NAME or ID ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#options_1", 
            "text": "Name, shorthand  Default  Description      NAME or ID   Name or ID of your data.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/data/#description_2", 
            "text": "Lists all your datasets on Floyd with more details.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example_2", 
            "text": "$ floyd data status\nDATA ID                 CREATED         DISK USAGE    NAME               \n----------------------  --------------  ------------  ------------------\nHYLEc2czGKRpYVm7rGtBoY   12  minutes ago   372 .0 MB      floydhub/mnist:1  \nqNcS5bXHtFdSiMZ35kkEPh  an hour ago      456 .2 MB      floydhub/csr:7", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#floyd-data-delete", 
            "text": "Delete datasets on FloydHub", 
            "title": "floyd data delete"
        }, 
        {
            "location": "/commands/data/#usage_3", 
            "text": "floyd data delete  [ OPTIONS ]   [ NAMES or IDS ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#options_2", 
            "text": "Name, shorthand  Default  Description      NAMES or IDS   One or more Names or IDs of your data.    --yes ,  -y  False  Skip delete confirmation step", 
            "title": "Options"
        }, 
        {
            "location": "/commands/data/#description_3", 
            "text": "Deletes your datasets from FloydHub. This data will no longer \nbe accessible.  Note: You do  not  have to be in the project directory to run this command.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example_3", 
            "text": "$ floyd data delete floydhub/csr:7\nDelete Data: floydhub/csr:7?  [ y/N ] : y\nData deleted", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#floyd-data-output", 
            "text": "View datasets", 
            "title": "floyd data output"
        }, 
        {
            "location": "/commands/data/#usage_4", 
            "text": "floyd data output  [ OPTIONS ]  ID", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#options_3", 
            "text": "Name, shorthand  Default  Description      --url ,  -u   Only print the URL. The data directory can be viewed in the browser.    NAME or ID   NAME or ID of your data.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/data/#description_4", 
            "text": "The output command gives the url to access a dataset. This command by default opens the data url \nin your default browser.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example_4", 
            "text": "$ floyd data output floydhub/csr:11\nOpening output directory in your browser ...", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/logs/", 
            "text": "View the logs of your run\n\n\nUsage\n\n\nfloyd logs \n[\nOPTIONS\n]\n ID\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--url\n, \n-u\n\n\n\n\nOnly print the URL. The logs can be viewed in the browser.\n\n\n\n\n\n\n--tail\n, \n-t\n\n\n\n\nStream the output of your code in real-time.\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nName or ID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nAny data sent to STDOUT and STDERR by your code will become available here. Make sure your \nlogs are flushed out if you prefer to view logs in real-time. There will be some information from \nFloyd servers before and after your project logs. They are usually useful for debugging purposes.\n\n\nExample\n\n\n$ floyd logs floydhub/projects/style-transfer/4\nPreparing to run \nStarting container...\n\n\n#################################################\n\n\nRun Output:\n...\n\n\n#################################################\n\n\nWaiting \nfor\n container to complete...\n\n[\nsuccess\n]\n Finishing execution\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd logs"
        }, 
        {
            "location": "/commands/logs/#usage", 
            "text": "floyd logs  [ OPTIONS ]  ID", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/logs/#options", 
            "text": "Name, shorthand  Default  Description      --url ,  -u   Only print the URL. The logs can be viewed in the browser.    --tail ,  -t   Stream the output of your code in real-time.    NAME or ID   Name or ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/logs/#description", 
            "text": "Any data sent to STDOUT and STDERR by your code will become available here. Make sure your \nlogs are flushed out if you prefer to view logs in real-time. There will be some information from \nFloyd servers before and after your project logs. They are usually useful for debugging purposes.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/logs/#example", 
            "text": "$ floyd logs floydhub/projects/style-transfer/4\nPreparing to run \nStarting container... ################################################# \n\nRun Output:\n... ################################################# \n\nWaiting  for  container to complete... [ success ]  Finishing execution", 
            "title": "Example"
        }, 
        {
            "location": "/commands/logs/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/status/", 
            "text": "View status of your jobs.\n\n\nUsage\n\n\nfloyd status \n[\nNAME or ID\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName or ID\n\n\n\n\nID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nShows the status of a run, if the ID is specified. It can also list the status of all \nthe runs in the current project. You need to be in the project directory for this command to work.\n\n\nExample\n\n\n$ floyd status\nRUN ID                  CREATED         STATUS      DURATION\n(\ns\n)\n  NAME                           INSTANCE      VERSION\n----------------------  --------------  --------  -------------  -----------------------------  ----------  ---------\ndTe2cJJrNR2CBD74rSZXPA  \n31\n minutes ago  success             \n108\n  floydhub/tensorflow-project:7  cpu                 \n2\n\nB8wkLbuGs2mtjhe9jqrkYT  \n2\n hours ago     success            \n2349\n  floydhub/tensorflow-project:7  gpu                 \n1\n\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd status"
        }, 
        {
            "location": "/commands/status/#usage", 
            "text": "floyd status  [ NAME or ID ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/status/#options", 
            "text": "Name, shorthand  Default  Description      Name or ID   ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/status/#description", 
            "text": "Shows the status of a run, if the ID is specified. It can also list the status of all \nthe runs in the current project. You need to be in the project directory for this command to work.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/status/#example", 
            "text": "$ floyd status\nRUN ID                  CREATED         STATUS      DURATION ( s )   NAME                           INSTANCE      VERSION\n----------------------  --------------  --------  -------------  -----------------------------  ----------  ---------\ndTe2cJJrNR2CBD74rSZXPA   31  minutes ago  success              108   floydhub/tensorflow-project:7  cpu                  2 \nB8wkLbuGs2mtjhe9jqrkYT   2  hours ago     success             2349   floydhub/tensorflow-project:7  gpu                  1", 
            "title": "Example"
        }, 
        {
            "location": "/commands/status/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/output/", 
            "text": "View the output of a job.\n\n\nUsage\n\n\nfloyd output \n[\nOPTIONS\n]\n ID\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--url\n, \n-u\n\n\n\n\nOnly print the URL. The output directory can be viewed in the browser.\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nName or ID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nMost jobs generate output. Any output that needs to be retained after the job is finished should be send to \n/output\n path.\nThis is the only path Floyd will preserve. The output command gives the url to access this output. This command by default opens the \noutput url in your default browser.\n\n\nExample\n\n\n$ floyd output floydhub/projects/style-transfer/4\nOpening output directory in your browser ...\n\n\nor\n\n$ floyd output dTe2cJJrNR2CBD74rSZXPA\nOpening output directory in your browser ...\n\n\n\nDownloading output\n\n\nTo download the output you can use the data \nclone\n command.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd output"
        }, 
        {
            "location": "/commands/output/#usage", 
            "text": "floyd output  [ OPTIONS ]  ID", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/output/#options", 
            "text": "Name, shorthand  Default  Description      --url ,  -u   Only print the URL. The output directory can be viewed in the browser.    NAME or ID   Name or ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/output/#description", 
            "text": "Most jobs generate output. Any output that needs to be retained after the job is finished should be send to  /output  path.\nThis is the only path Floyd will preserve. The output command gives the url to access this output. This command by default opens the \noutput url in your default browser.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/output/#example", 
            "text": "$ floyd output floydhub/projects/style-transfer/4\nOpening output directory in your browser ... \nor $ floyd output dTe2cJJrNR2CBD74rSZXPA\nOpening output directory in your browser ...", 
            "title": "Example"
        }, 
        {
            "location": "/commands/output/#downloading-output", 
            "text": "To download the output you can use the data  clone  command.", 
            "title": "Downloading output"
        }, 
        {
            "location": "/commands/output/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/clone/", 
            "text": "Clone the code for a specific job.\n\n\nUsage\n\n\nfloyd clone JOB_NAME\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nJOB_NAME or ID\n\n\n\n\nName or ID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nUse this command to clone an existing project on floyd. The code used for the job is downloaded to the \ncurrent directory. This will override any existing file or directory in the process.\n\n\nThis command is a great way to get started on floyd by starting from an existing project.\n\n\nExample\n\n\n$ floyd clone floydhub/projects/style-transfer/4\nDownloading the tar file to the current directory ...\nUntarring the contents of the file ...\nCleaning up the tar file ...\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd clone"
        }, 
        {
            "location": "/commands/clone/#usage", 
            "text": "floyd clone JOB_NAME", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/clone/#options", 
            "text": "Name, shorthand  Default  Description      JOB_NAME or ID   Name or ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/clone/#description", 
            "text": "Use this command to clone an existing project on floyd. The code used for the job is downloaded to the \ncurrent directory. This will override any existing file or directory in the process.  This command is a great way to get started on floyd by starting from an existing project.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/clone/#example", 
            "text": "$ floyd clone floydhub/projects/style-transfer/4\nDownloading the tar file to the current directory ...\nUntarring the contents of the file ...\nCleaning up the tar file ...", 
            "title": "Example"
        }, 
        {
            "location": "/commands/clone/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/info/", 
            "text": "View the details of a job.\n\n\nUsage\n\n\nfloyd info \n[\nOPTIONS\n]\n NAME\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nName or ID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nThis command gives detailed information about your job. Some useful information here:\n\n\nOutput ID\n\n\nOutput ID is the reference to the output generated by your run. \n\n\nUrl\n\n\nIf your job is running in \njupyter\n or \nserve\n mode, you can get their URL here.\n\n\nExample\n\n\n$ floyd info floydhub/jupyter-notebook/1\n-----------  ----------------------------------------------------\nRun ID       Faa2xpokjAfJL5Jd7vCVXo\nName         floydhub/jupyter-notebook/1\nCreated      \n2\n minutes ago\nStatus       running\nDuration\n(\ns\n)\n  \n0\n\nOutput ID    VB9bF6vtyvrHLdUsFbUuvW\nInstance     cpu\nVersion      \n1\n\nMode         jupyter\nUrl          https://www.floydhub.com:8000/VB9bF6vtyvrHLdUsFbUuvW\n-----------  ----------------------------------------------------\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd info"
        }, 
        {
            "location": "/commands/info/#usage", 
            "text": "floyd info  [ OPTIONS ]  NAME", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/info/#options", 
            "text": "Name, shorthand  Default  Description      NAME or ID   Name or ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/info/#description", 
            "text": "This command gives detailed information about your job. Some useful information here:", 
            "title": "Description"
        }, 
        {
            "location": "/commands/info/#output-id", 
            "text": "Output ID is the reference to the output generated by your run.", 
            "title": "Output ID"
        }, 
        {
            "location": "/commands/info/#url", 
            "text": "If your job is running in  jupyter  or  serve  mode, you can get their URL here.", 
            "title": "Url"
        }, 
        {
            "location": "/commands/info/#example", 
            "text": "$ floyd info floydhub/jupyter-notebook/1\n-----------  ----------------------------------------------------\nRun ID       Faa2xpokjAfJL5Jd7vCVXo\nName         floydhub/jupyter-notebook/1\nCreated       2  minutes ago\nStatus       running\nDuration ( s )    0 \nOutput ID    VB9bF6vtyvrHLdUsFbUuvW\nInstance     cpu\nVersion       1 \nMode         jupyter\nUrl          https://www.floydhub.com:8000/VB9bF6vtyvrHLdUsFbUuvW\n-----------  ----------------------------------------------------", 
            "title": "Example"
        }, 
        {
            "location": "/commands/info/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/stop/", 
            "text": "Terminate a queued or running job.\n\n\nUsage\n\n\nfloyd stop NAME\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nName or ID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nSometimes you want to terminate a job before it can finish. The stop command sends a request \nto the server to stop the job. You can view the \nstatus\n of the job to confirm. When you stop \na job, you will be charged only for the duration your job was running.\n\n\nExample\n\n\n$ floyd stop floydhub/projects/tensorflow-example/4\nExperiment shutdown request submitted. Check status to confirm shutdown\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd stop"
        }, 
        {
            "location": "/commands/stop/#usage", 
            "text": "floyd stop NAME", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/stop/#options", 
            "text": "Name, shorthand  Default  Description      NAME or ID   Name or ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/stop/#description", 
            "text": "Sometimes you want to terminate a job before it can finish. The stop command sends a request \nto the server to stop the job. You can view the  status  of the job to confirm. When you stop \na job, you will be charged only for the duration your job was running.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/stop/#example", 
            "text": "$ floyd stop floydhub/projects/tensorflow-example/4\nExperiment shutdown request submitted. Check status to confirm shutdown", 
            "title": "Example"
        }, 
        {
            "location": "/commands/stop/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/delete/", 
            "text": "Delete one or more floyd jobs.\n\n\nUsage\n\n\nfloyd delete \n[\nIDS\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nIDS\n\n\n\n\nOne or more IDs of your job.\n\n\n\n\n\n\n--yes\n, \n-y\n\n\nFalse\n\n\nSkip delete confirmation step\n\n\n\n\n\n\n\n\nDescription\n\n\nDeletes a job from FloydHub. The experiment information is no longer \navailable. You will not be able to access the code uploaded to run the \nexperiment. You need to make sure that the project is currently not \nrunning. If so, you can use the \nstop\n command for that.\n\n\nNote: You do \nnot\n have to be in the project directory to run this command.\n\n\nExample\n\n\n$ floyd delete J2ggstKWTNmL24nQTgi36o qNcS5bXHtFdSiMZ35kkEPh\nDelete Run: floydhub/fastText:1? \n[\ny/N\n]\n: y\nExperiment deleted\nDelete Run: floydhub/cnr:1? \n[\ny/N\n]\n: y\nExperiment deleted\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd delete"
        }, 
        {
            "location": "/commands/delete/#usage", 
            "text": "floyd delete  [ IDS ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/delete/#options", 
            "text": "Name, shorthand  Default  Description      IDS   One or more IDs of your job.    --yes ,  -y  False  Skip delete confirmation step", 
            "title": "Options"
        }, 
        {
            "location": "/commands/delete/#description", 
            "text": "Deletes a job from FloydHub. The experiment information is no longer \navailable. You will not be able to access the code uploaded to run the \nexperiment. You need to make sure that the project is currently not \nrunning. If so, you can use the  stop  command for that.  Note: You do  not  have to be in the project directory to run this command.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/delete/#example", 
            "text": "$ floyd delete J2ggstKWTNmL24nQTgi36o qNcS5bXHtFdSiMZ35kkEPh\nDelete Run: floydhub/fastText:1?  [ y/N ] : y\nExperiment deleted\nDelete Run: floydhub/cnr:1?  [ y/N ] : y\nExperiment deleted", 
            "title": "Example"
        }, 
        {
            "location": "/commands/delete/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/logout/", 
            "text": "Logout from Floyd.\n\n\nDescription\n\n\nLogout the CLI from Floyd\n\n\nUsage\n\n\nfloyd \nlogout\n\n\n\n\n\nDescription\n\n\nLogs you out and expires your current token. You will need to login again to run further commands.\n\n\nExample\n\n\n$ floyd \nlogout\n\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd logout"
        }, 
        {
            "location": "/commands/logout/#logout-from-floyd", 
            "text": "", 
            "title": "Logout from Floyd."
        }, 
        {
            "location": "/commands/logout/#description", 
            "text": "Logout the CLI from Floyd", 
            "title": "Description"
        }, 
        {
            "location": "/commands/logout/#usage", 
            "text": "floyd  logout", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/logout/#description_1", 
            "text": "Logs you out and expires your current token. You will need to login again to run further commands.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/logout/#example", 
            "text": "$ floyd  logout", 
            "title": "Example"
        }, 
        {
            "location": "/commands/logout/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/upgrade/", 
            "text": "Upgrade the floyd client.\n\n\nUsage\n\n\nfloyd upgrade\n\n\n\n\nDescription\n\n\nThis will upgrade the floyd cli to the latest version using pip.\n\n\nExample\n\n\n$ floyd upgrade\nCollecting floyd-cli from ...\n...\nSuccessfully installed floyd-cli-0.9.1\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd upgrade"
        }, 
        {
            "location": "/commands/upgrade/#usage", 
            "text": "floyd upgrade", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/upgrade/#description", 
            "text": "This will upgrade the floyd cli to the latest version using pip.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/upgrade/#example", 
            "text": "$ floyd upgrade\nCollecting floyd-cli from ...\n...\nSuccessfully installed floyd-cli-0.9.1", 
            "title": "Example"
        }, 
        {
            "location": "/commands/upgrade/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/version/", 
            "text": "Get the current version of floyd-cli\n\n\nUsage\n\n\nfloyd version\n\n\n\n\nDescription\n\n\nPrints the current version of floyd-cli\n\n\nExample\n\n\n$ floyd version\n\n0\n.9.1\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd version"
        }, 
        {
            "location": "/commands/version/#usage", 
            "text": "floyd version", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/version/#description", 
            "text": "Prints the current version of floyd-cli", 
            "title": "Description"
        }, 
        {
            "location": "/commands/version/#example", 
            "text": "$ floyd version 0 .9.1", 
            "title": "Example"
        }, 
        {
            "location": "/commands/version/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/examples/style_transfer/", 
            "text": "Neural Style Transfer is an algorithm for combining the content of one image with the style of another image \nusing convolutional neural networks. Here's an example that maps the artistic style of The Starry Night \nonto a night-time photograph of the Stanford campus:\n\n\n\n\n\n\n\n\nWe will use this example to demonstrate how Floyd can be used to deploy your trained model as a REST API endpoint that can be accessed over the web. \nThis feature is very useful if you want to quickly compare models or have others play with your models. This guide will \nwalk you through how to do this.\n\n\nSetup project\n\n\nFor this guide we will be using \nFast Style Transfer\n\nproject.\n\n\n$ git clone https://github.com/floydhub/fast-style-transfer\n$ \ncd\n fast-style-transfer\n$ floyd init fast-style-transfer\nProject \nfast-style-transfer\n initialized in the current directory\n\n\n\n\nTrain a model\n\n\nYou can train your model by running the \nstyle.py\n script in this repo on Floyd. You can specify any style image to use in the command line. Just \ndownload it and keep it in current path. In this example we will be starting from a \n\npre-trained model\n.\n\n\nTraining data\n\n\nThis project also requires access to the imagenet-vgg-verydeep-19 model and image training data. Floyd already has this data source available.\nYou can mount this at runtime using the \n--data\n parameter.\n\n\nTraining\n\n\n$ floyd run --gpu --env tensorflow-0.12:py2 --data narenst/datasets/coco-train-2014/1:images --data narenst/datasets/neural-style-transfer-pre-trained-models/1:models --data floydhub/datasets/imagenet-vgg-verydeep-19/3:vgg \npython style.py --vgg-path /vgg/imagenet-vgg-verydeep-19.mat --train-path /images/train2014 --style examples/style/la_muse.jpg --base-model-path /models/la_muse.ckpt --epoch 1 --total-iterations 10 --checkpoint-dir /output\n\n\n\n\n\nThis will kick off a new job on Floyd. This will take a few minutes to run and will generate the model. You can follow along the progress \nby using the \nlogs\n command. \n\n\n$ floyd logs \nJOB_NAME\n -t\n\n\nNow you need to get the ID of the \nOutput\n generated by your job. Floyd \ninfo\n can give you that information.\n\n\n$ floyd info \nJOB_NAME\n\n\n\n\n\nEvaluate your model\n\n\nYou can evaluate the generated model by running \nevaluate.py\n on sample images. Use the output id from the training step\nas the datasource in this step. Add any image you want to style transfer to the \nimages\n directory. Then run \nevaluate.py\n.\n\n\nfloyd run --env tensorflow-0.12:py2 --data \nREPLACE_WITH_OUTPUT_ID\n:input \npython evaluate.py --allow-different-dimensions  --checkpoint /input/fns.ckpt --in-path ./images/ --out-path /output/\n\n\n\nYou can track the status of the run with the status or logs command.\n\n\n$ floyd status \nJOB_NAME\n\n$ floyd logs \nJOB_NAME\n -t\n\n\n\n\nAfter the job finishes successfully, view the output directory to see the style transferred images. Run the floyd \noutput\n\nfor this.\n\n\n$ floyd output \nJOB_NAME\n\n\n\n\n\nImproving the model\n\n\nYou may notice that the output does not look great. That is because we ran the training for a small number of iterations. To train \na fully working model try the train step again, this time without setting \n--total-iterations\n and increasing the \n--epoch\n to 2.\nIt takes about 8 hours to train a model that works well. You can instead try one of our pre-trained models in the next section.\n\n\nEvaluate pre-trained models\n\n\nIf you want to try out some awesome pre-trained models for various styles, you can use the datasource with models available publicly.\nYou can play with any of these model and style transfer any image you prefer. Just add them to \nimages\n directory. And point to the \nright model in the \n--checkpoint\n parameter.\n\n\nfloyd run --env tensorflow-0.12:py2 --data narenst/datasets/neural-style-transfer-pre-trained-models/1:models \npython evaluate.py --allow-different-dimensions  --checkpoint /models/la_muse.ckpt --in-path ./images/ --out-path /output/\n\n\n\n\n\nYou can track the status of the run with the status command.\n\n\n$ floyd status \nJOB_NAME\n\n\n\n\n\nWhen the experiment is finished, you can see the style transferred images by running:\n\n\n$ floyd output \nJOB_NAME\n\n\n\n\n\n\n\nModel API\n\n\nYou can now host this model as a REST API. This means you can send any image to this API as a HTTP request and it will be style transferred. \n\n\nServe mode\n\n\nFloyd \nrun\n command has a \nserve\n mode. This will upload the files in the current directory and run a special command - \n\npython app.py\n. Floyd expects this file to contain the code to run a web server and listen on port \n5000\n. You can see the \n\napp.py\n file in the sample repository. This file handles the \nincoming request, executes the code in \nevaluate.py\n and returns the output.\n\n\nNote that this feature is in preview mode and is not production ready yet\n\n\n$ floyd run --env tensorflow-0.12:py2 --data narenst/datasets/neural-style-transfer-pre-trained-models/1:input --mode serve\nSyncing code ...\nRUN ID                  NAME                              VERSION\n----------------------  ------------------------------  ---------\nDJSdJAVa3u7AsFEMZMBBL5  floydhub/fast-style-transfer:5          \n5\n\n\nPath to service endpoint: https://www.floydhub.com/expose/t4AdkU6awahkT3ooNazw8c\n\nTo view logs enter:\n    floyd logs DJSdJAVa3u7AsFEMZMBBL5\n\n\n\n\nSending requests to the REST API\n\n\nNow you can send any image file as request to this api and it will return the style transferred image.\n\n\ncurl -o taipei_output.jpg -F \nfile=@./images/taipei101.jpg\n https://www.floydhub.com/expose/t4AdkU6awahkT3ooNazw8c\n\n\n\n\n\n\nYou will see the default style (\nla_muse\n) is applied to the input image.\n\n\nTrying out different models\n\n\nYou can also pass in the name of the checkpoint to use and the image will be style transferred accordingly:\n\n\ncurl -o taipei_udnie.jpg -F \nfile=@./images/taipei101.jpg\n -F \ncheckpoint=udnie.ckpt\n  https://www.floydhub.com/expose/MUDFXViCLArG2drppvU3nm\n\n\n\n\n\n\nThis uses a different style checkpoint to render the image. All the logic for this is present in the \napp.py\n file. You can update it to \nbe as complex as you prefer.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Style Transfer"
        }, 
        {
            "location": "/examples/style_transfer/#setup-project", 
            "text": "For this guide we will be using  Fast Style Transfer \nproject.  $ git clone https://github.com/floydhub/fast-style-transfer\n$  cd  fast-style-transfer\n$ floyd init fast-style-transfer\nProject  fast-style-transfer  initialized in the current directory", 
            "title": "Setup project"
        }, 
        {
            "location": "/examples/style_transfer/#train-a-model", 
            "text": "You can train your model by running the  style.py  script in this repo on Floyd. You can specify any style image to use in the command line. Just \ndownload it and keep it in current path. In this example we will be starting from a  pre-trained model .", 
            "title": "Train a model"
        }, 
        {
            "location": "/examples/style_transfer/#training-data", 
            "text": "This project also requires access to the imagenet-vgg-verydeep-19 model and image training data. Floyd already has this data source available.\nYou can mount this at runtime using the  --data  parameter.", 
            "title": "Training data"
        }, 
        {
            "location": "/examples/style_transfer/#training", 
            "text": "$ floyd run --gpu --env tensorflow-0.12:py2 --data narenst/datasets/coco-train-2014/1:images --data narenst/datasets/neural-style-transfer-pre-trained-models/1:models --data floydhub/datasets/imagenet-vgg-verydeep-19/3:vgg  python style.py --vgg-path /vgg/imagenet-vgg-verydeep-19.mat --train-path /images/train2014 --style examples/style/la_muse.jpg --base-model-path /models/la_muse.ckpt --epoch 1 --total-iterations 10 --checkpoint-dir /output   This will kick off a new job on Floyd. This will take a few minutes to run and will generate the model. You can follow along the progress \nby using the  logs  command.   $ floyd logs  JOB_NAME  -t \nNow you need to get the ID of the  Output  generated by your job. Floyd  info  can give you that information.  $ floyd info  JOB_NAME", 
            "title": "Training"
        }, 
        {
            "location": "/examples/style_transfer/#evaluate-your-model", 
            "text": "You can evaluate the generated model by running  evaluate.py  on sample images. Use the output id from the training step\nas the datasource in this step. Add any image you want to style transfer to the  images  directory. Then run  evaluate.py .  floyd run --env tensorflow-0.12:py2 --data  REPLACE_WITH_OUTPUT_ID :input  python evaluate.py --allow-different-dimensions  --checkpoint /input/fns.ckpt --in-path ./images/ --out-path /output/  \nYou can track the status of the run with the status or logs command.  $ floyd status  JOB_NAME \n$ floyd logs  JOB_NAME  -t  After the job finishes successfully, view the output directory to see the style transferred images. Run the floyd  output \nfor this.  $ floyd output  JOB_NAME", 
            "title": "Evaluate your model"
        }, 
        {
            "location": "/examples/style_transfer/#improving-the-model", 
            "text": "You may notice that the output does not look great. That is because we ran the training for a small number of iterations. To train \na fully working model try the train step again, this time without setting  --total-iterations  and increasing the  --epoch  to 2.\nIt takes about 8 hours to train a model that works well. You can instead try one of our pre-trained models in the next section.", 
            "title": "Improving the model"
        }, 
        {
            "location": "/examples/style_transfer/#evaluate-pre-trained-models", 
            "text": "If you want to try out some awesome pre-trained models for various styles, you can use the datasource with models available publicly.\nYou can play with any of these model and style transfer any image you prefer. Just add them to  images  directory. And point to the \nright model in the  --checkpoint  parameter.  floyd run --env tensorflow-0.12:py2 --data narenst/datasets/neural-style-transfer-pre-trained-models/1:models  python evaluate.py --allow-different-dimensions  --checkpoint /models/la_muse.ckpt --in-path ./images/ --out-path /output/   You can track the status of the run with the status command.  $ floyd status  JOB_NAME   When the experiment is finished, you can see the style transferred images by running:  $ floyd output  JOB_NAME", 
            "title": "Evaluate pre-trained models"
        }, 
        {
            "location": "/examples/style_transfer/#model-api", 
            "text": "You can now host this model as a REST API. This means you can send any image to this API as a HTTP request and it will be style transferred.", 
            "title": "Model API"
        }, 
        {
            "location": "/examples/style_transfer/#serve-mode", 
            "text": "Floyd  run  command has a  serve  mode. This will upload the files in the current directory and run a special command -  python app.py . Floyd expects this file to contain the code to run a web server and listen on port  5000 . You can see the  app.py  file in the sample repository. This file handles the \nincoming request, executes the code in  evaluate.py  and returns the output.  Note that this feature is in preview mode and is not production ready yet  $ floyd run --env tensorflow-0.12:py2 --data narenst/datasets/neural-style-transfer-pre-trained-models/1:input --mode serve\nSyncing code ...\nRUN ID                  NAME                              VERSION\n----------------------  ------------------------------  ---------\nDJSdJAVa3u7AsFEMZMBBL5  floydhub/fast-style-transfer:5           5 \n\nPath to service endpoint: https://www.floydhub.com/expose/t4AdkU6awahkT3ooNazw8c\n\nTo view logs enter:\n    floyd logs DJSdJAVa3u7AsFEMZMBBL5", 
            "title": "Serve mode"
        }, 
        {
            "location": "/examples/style_transfer/#sending-requests-to-the-rest-api", 
            "text": "Now you can send any image file as request to this api and it will return the style transferred image.  curl -o taipei_output.jpg -F  file=@./images/taipei101.jpg  https://www.floydhub.com/expose/t4AdkU6awahkT3ooNazw8c   You will see the default style ( la_muse ) is applied to the input image.", 
            "title": "Sending requests to the REST API"
        }, 
        {
            "location": "/examples/style_transfer/#trying-out-different-models", 
            "text": "You can also pass in the name of the checkpoint to use and the image will be style transferred accordingly:  curl -o taipei_udnie.jpg -F  file=@./images/taipei101.jpg  -F  checkpoint=udnie.ckpt   https://www.floydhub.com/expose/MUDFXViCLArG2drppvU3nm   This uses a different style checkpoint to render the image. All the logic for this is present in the  app.py  file. You can update it to \nbe as complex as you prefer.", 
            "title": "Trying out different models"
        }, 
        {
            "location": "/examples/style_transfer/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/examples/deep_corrector/", 
            "text": "Deep Text Corrector is an Tensorflow project made by \nAlex Paino\n for correcting\ngrammatical errors in short sentences. For example, the message \"I'm going to\nstore\" would be unaffected by typical autocorrection systems, when the user\nmost likely intendend to write \"I'm going to \nthe\n store\".\n\n\nIn this guide we will train a Tensorflow model for correcting sentences and use\nit to evaluate input sentences.  Finally we will deploy the trained model as a\nREST endpoint that can be used to evaluate input sequences in real time.\n\n\nProject setup\n\n\nThe code for this project is available on Floyd's \nGithub page\n. Clone the project and\n\ninitialize\n a floyd project.\n\n\n$ git clone https://github.com/floydhub/deep-text-corrector\n$ \ncd\n deep-text-corrector\n$ floyd init deep-text-corrector\n\n\n\n\nTraining\n\n\nDataset\n\n\nFor this project we will use the Cornel Movie-Dialogs Corpus for training and testing.\nThe dataset should be preprocessed and split into 3 sets: 80% for training, and 10%\neach for validation and testing. This preprocessed dataset is available publicly on\n\nFloydHub\n.\n\n\nTraining\n\n\nYou can train the deep corrector model by running \ncorrect_text.py\n script with required\nparameters. Below is the \ncommand\n to start a training job on Floyd:\n\n\n$ floyd run --gpu --env tensorflow-0.12:py2 --data floydhub/datasets/deep-text-corrector/1 \npython correct_text.py --num_steps 1000 --train_path /input/data/movie_dialog_train.txt --val_path /input/data/movie_dialog_val.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --output_path /output\n\n\n\n\n\nNotes:\n\n\n\n\nThe input dataset is passed using the \n--data\n parameter. This mounts the pre-processed\nCornell Movie Dialog dataset at \n/input\n path. You will notice that other parameters use files\nmounted in this path.\n\n\nThe data name \nfloydhub/datasets/deep-text-corrector/1\n\npoints to the pre-processed dataset on FloydHub.\n\n\nThe job is running on a gpu instance (Because of the \n--gpu\n flag).\n\n\nThis project uses Tensorflow-0.12 installed on Python 2. (See the \n--env\n flag)\n\n\n\n\nThis job takes about 10 minutes to run and generate a model. You can follow along the progress\nby using the \nlogs\n command.\n\n\n$ floyd logs \nJOB_NAME\n -t\n\n\n\n\nFloyd saves any content stored in the \n/output\n directory after the job is\nfinished. This output can be used as a datasource in the next project.  To get\nthe name of the output generated by your job use the\n\ninfo\n command.\n\n\n$ floyd info \nJOB_NAME\n\n\n\n\n\nEvaluating\n\n\nTo evaluate your model you can run the \ncorrect_text.py\n script with the \ndecode\n flag.\nYou need a file containing short messages for evaluation. The \ntest.txt\n file already has some\ninputs. You can update or add more strings to this file - one per line. You also need to\nuse the output from the training step above as the datasource in this step.\n\n\nfloyd run --env tensorflow-0.12:py2 --data \nREPLACE_WITH_JOB_OUTPUT_NAME\n \npython correct_text.py --train_path /input/data/movie_dialog_train.txt --test_path test.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --input_path /input --decode\n\n\n\n\n\nYou can track the status of the run with the status or logs command. The logs should print the\nconcerted messages from the test.txt file.\n\n\n$ floyd status \nJOB_NAME\n\n$ floyd logs \nJOB_NAME\n -t\n\n\n\n\nImproving your model\n\n\nYou may notice that the output does not look great. In fact, the algorithm would've added more\nmistakes into the sentences than correct it. That is because we ran the training for a small number\nof iterations. To train a fully working model try the training step again, this time by setting\nthe flag \nnum_steps\n to a large value. In general, about 20000 steps are necessary to give a\nworking corrector model. (Note: This takes a few hours to run on the GPU instance)\n\n\nEvaluate pre-trained models\n\n\nIf you want to try out a pre-trained model, FloydHub has a public job output for\nthis. You can mount it with job output name:\n\nfloydhub/deep-text-corrector/23/output\n\n.\n\n\nfloyd run --env tensorflow-0.12:py2 --data floydhub/deep-text-corrector/23/output \npython correct_text.py --train_path /input/data/movie_dialog_train.txt --test_path test.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --input_path /input --decode\n\n\n\n\n\nThis model should perform better on the given inputs compared to the previous one.\n\n\nServe model through REST API\n\n\nFloydHub supports seving mode for demo and testing purpose. If you run a job\nwith \n--mode serve\n flag, FloydHub will run the \napp.py\n file in your project\nand attach it to a dynamic service endpoint:\n\n\nfloyd run --mode serve --env tensorflow-0.12:py2 --data floydhub/deep-text-corrector/23/output:input\n\n\n\n\nThe above command will print out a service endpoint for this job in your terminal console.\n\n\nThe service endpoint will take couple minutes to become ready. Once it's up, you can interact with the model by sending text you want to correct:\n\n\ncurl -X POST -d \nI see it tomorrow\n \nREPLACE_WITH_YOUR_SERVICE_ENDPOINT\n\n\n\n\n\nAny job running in serving mode will stay up until it reaches maximum runtime. So\nonce you are done testing, remember to shutdown the job.\n\n\nNote that this feature is in preview mode and is not production ready yet\n\n\nWhat Next?\n\n\nThe model was trained using movie dialogues which are not the greatest sources of gramatically correct\nsentences. An improvement to this approach would be to use other datasources like \nProject Gutenberg\n.\nThis project was also discussed on \nHackerNews\n and you can\nfind lots of interesting alternatives there.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Deep Text Corrector"
        }, 
        {
            "location": "/examples/deep_corrector/#project-setup", 
            "text": "The code for this project is available on Floyd's  Github page . Clone the project and initialize  a floyd project.  $ git clone https://github.com/floydhub/deep-text-corrector\n$  cd  deep-text-corrector\n$ floyd init deep-text-corrector", 
            "title": "Project setup"
        }, 
        {
            "location": "/examples/deep_corrector/#training", 
            "text": "", 
            "title": "Training"
        }, 
        {
            "location": "/examples/deep_corrector/#dataset", 
            "text": "For this project we will use the Cornel Movie-Dialogs Corpus for training and testing.\nThe dataset should be preprocessed and split into 3 sets: 80% for training, and 10%\neach for validation and testing. This preprocessed dataset is available publicly on FloydHub .", 
            "title": "Dataset"
        }, 
        {
            "location": "/examples/deep_corrector/#training_1", 
            "text": "You can train the deep corrector model by running  correct_text.py  script with required\nparameters. Below is the  command  to start a training job on Floyd:  $ floyd run --gpu --env tensorflow-0.12:py2 --data floydhub/datasets/deep-text-corrector/1  python correct_text.py --num_steps 1000 --train_path /input/data/movie_dialog_train.txt --val_path /input/data/movie_dialog_val.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --output_path /output   Notes:   The input dataset is passed using the  --data  parameter. This mounts the pre-processed\nCornell Movie Dialog dataset at  /input  path. You will notice that other parameters use files\nmounted in this path.  The data name  floydhub/datasets/deep-text-corrector/1 \npoints to the pre-processed dataset on FloydHub.  The job is running on a gpu instance (Because of the  --gpu  flag).  This project uses Tensorflow-0.12 installed on Python 2. (See the  --env  flag)   This job takes about 10 minutes to run and generate a model. You can follow along the progress\nby using the  logs  command.  $ floyd logs  JOB_NAME  -t  Floyd saves any content stored in the  /output  directory after the job is\nfinished. This output can be used as a datasource in the next project.  To get\nthe name of the output generated by your job use the info  command.  $ floyd info  JOB_NAME", 
            "title": "Training"
        }, 
        {
            "location": "/examples/deep_corrector/#evaluating", 
            "text": "To evaluate your model you can run the  correct_text.py  script with the  decode  flag.\nYou need a file containing short messages for evaluation. The  test.txt  file already has some\ninputs. You can update or add more strings to this file - one per line. You also need to\nuse the output from the training step above as the datasource in this step.  floyd run --env tensorflow-0.12:py2 --data  REPLACE_WITH_JOB_OUTPUT_NAME   python correct_text.py --train_path /input/data/movie_dialog_train.txt --test_path test.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --input_path /input --decode   You can track the status of the run with the status or logs command. The logs should print the\nconcerted messages from the test.txt file.  $ floyd status  JOB_NAME \n$ floyd logs  JOB_NAME  -t", 
            "title": "Evaluating"
        }, 
        {
            "location": "/examples/deep_corrector/#improving-your-model", 
            "text": "You may notice that the output does not look great. In fact, the algorithm would've added more\nmistakes into the sentences than correct it. That is because we ran the training for a small number\nof iterations. To train a fully working model try the training step again, this time by setting\nthe flag  num_steps  to a large value. In general, about 20000 steps are necessary to give a\nworking corrector model. (Note: This takes a few hours to run on the GPU instance)", 
            "title": "Improving your model"
        }, 
        {
            "location": "/examples/deep_corrector/#evaluate-pre-trained-models", 
            "text": "If you want to try out a pre-trained model, FloydHub has a public job output for\nthis. You can mount it with job output name: floydhub/deep-text-corrector/23/output \n.  floyd run --env tensorflow-0.12:py2 --data floydhub/deep-text-corrector/23/output  python correct_text.py --train_path /input/data/movie_dialog_train.txt --test_path test.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --input_path /input --decode   This model should perform better on the given inputs compared to the previous one.", 
            "title": "Evaluate pre-trained models"
        }, 
        {
            "location": "/examples/deep_corrector/#serve-model-through-rest-api", 
            "text": "FloydHub supports seving mode for demo and testing purpose. If you run a job\nwith  --mode serve  flag, FloydHub will run the  app.py  file in your project\nand attach it to a dynamic service endpoint:  floyd run --mode serve --env tensorflow-0.12:py2 --data floydhub/deep-text-corrector/23/output:input  The above command will print out a service endpoint for this job in your terminal console.  The service endpoint will take couple minutes to become ready. Once it's up, you can interact with the model by sending text you want to correct:  curl -X POST -d  I see it tomorrow   REPLACE_WITH_YOUR_SERVICE_ENDPOINT   Any job running in serving mode will stay up until it reaches maximum runtime. So\nonce you are done testing, remember to shutdown the job.  Note that this feature is in preview mode and is not production ready yet", 
            "title": "Serve model through REST API"
        }, 
        {
            "location": "/examples/deep_corrector/#what-next", 
            "text": "The model was trained using movie dialogues which are not the greatest sources of gramatically correct\nsentences. An improvement to this approach would be to use other datasources like  Project Gutenberg .\nThis project was also discussed on  HackerNews  and you can\nfind lots of interesting alternatives there.", 
            "title": "What Next?"
        }, 
        {
            "location": "/examples/deep_corrector/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/faqs/installation/", 
            "text": "Using virtualenv to install floyd-cli\n\n\nWe highly recommend using \nvirtualenv\n for installing and using \nfloyd-cli\n. This helps avoid any library version conflicts and results in a smoother installation process.\n\n\nsudo pip install virtualenv\n\n\n\n\nTo create a virtualenv, you need to pass a path to store the installed packages.\n\n\nvirtualenv ~/floyd\n\n\n\n\nYou can now activate and start using the virtualenv by running:\n\nsource\n ~/floyd/bin/activate\n\n\n\nTo install floyd-cli in this virtualenv:\n\n\npip install -U floyd-cli\n\n\n\n\nYou are now ready to use the \nfloyd commands\n. Note: You need to activate your virtualenv using the \nsource\n command above each time you open a new terminal and want to use \nfloyd-cli\n in it.\n\n\nUsing conda to install floyd-cli\n\n\nIf you are using Anaconda Python, you can also use \nconda\n to install \nfloyd-cli\n, instead of \nvirtualenv\n.\n\n\nconda create -n \ninsert-your-env-name-here\n\n\nsource\n activate \ninsert-your-env-name-here\n\npip install -U floyd-cli\n\n\n\n\nPlease see \nthis guide\n on creating virtual environments for Python with conda.\n\n\nUsing sudo to install floyd-cli\n\n\nTry this if you see a permission error, such as \nPermission denied\n or \nAccess is denied\n. If you are not using virtualenv and you are installing \nfloyd-cli\n globally you may need to use \nsudo\n:\n\n\nsudo pip install -U floyd-cli\n\n\n\n\nDealing with missing dependencies when installing floyd-cli\n\n\nNot all python environments are installed the same way. So sometimes you may run\ninto install issues. If \npip\n cannot install dependencies itself, you may see errors like:\n\n\n...\nFailed building wheel \nfor\n scandir\n...\n\n\n\n\nor\n\n\n...\nNo distributions matching the version \nfor\n backports.tempfile \n(\nfrom floyd-cli\n)\n\n...\n\n\n\n\nIn such cases, you can install the dependencies directly:\n\n\npip install -U scandir\npip install -U backports.tempfile\n\n\n\n\nand then try installing \nfloyd-cli\n.\n\n\nPython.h: No such file or directory\n\n\nIf you get this error in a linux environment:\n\n\n...\nPython.h: No such file or directory\n\n\n\n\nyou need to install \npython-dev\n package\n\n\nsudo apt-get install python-dev\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Installation"
        }, 
        {
            "location": "/faqs/installation/#using-virtualenv-to-install-floyd-cli", 
            "text": "We highly recommend using  virtualenv  for installing and using  floyd-cli . This helps avoid any library version conflicts and results in a smoother installation process.  sudo pip install virtualenv  To create a virtualenv, you need to pass a path to store the installed packages.  virtualenv ~/floyd  You can now activate and start using the virtualenv by running: source  ~/floyd/bin/activate  To install floyd-cli in this virtualenv:  pip install -U floyd-cli  You are now ready to use the  floyd commands . Note: You need to activate your virtualenv using the  source  command above each time you open a new terminal and want to use  floyd-cli  in it.", 
            "title": "Using virtualenv to install floyd-cli"
        }, 
        {
            "location": "/faqs/installation/#using-conda-to-install-floyd-cli", 
            "text": "If you are using Anaconda Python, you can also use  conda  to install  floyd-cli , instead of  virtualenv .  conda create -n  insert-your-env-name-here  source  activate  insert-your-env-name-here \npip install -U floyd-cli  Please see  this guide  on creating virtual environments for Python with conda.", 
            "title": "Using conda to install floyd-cli"
        }, 
        {
            "location": "/faqs/installation/#using-sudo-to-install-floyd-cli", 
            "text": "Try this if you see a permission error, such as  Permission denied  or  Access is denied . If you are not using virtualenv and you are installing  floyd-cli  globally you may need to use  sudo :  sudo pip install -U floyd-cli", 
            "title": "Using sudo to install floyd-cli"
        }, 
        {
            "location": "/faqs/installation/#dealing-with-missing-dependencies-when-installing-floyd-cli", 
            "text": "Not all python environments are installed the same way. So sometimes you may run\ninto install issues. If  pip  cannot install dependencies itself, you may see errors like:  ...\nFailed building wheel  for  scandir\n...  or  ...\nNo distributions matching the version  for  backports.tempfile  ( from floyd-cli ) \n...  In such cases, you can install the dependencies directly:  pip install -U scandir\npip install -U backports.tempfile  and then try installing  floyd-cli .", 
            "title": "Dealing with missing dependencies when installing floyd-cli"
        }, 
        {
            "location": "/faqs/installation/#pythonh-no-such-file-or-directory", 
            "text": "If you get this error in a linux environment:  ...\nPython.h: No such file or directory  you need to install  python-dev  package  sudo apt-get install python-dev", 
            "title": "Python.h: No such file or directory"
        }, 
        {
            "location": "/faqs/installation/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/faqs/authentication/", 
            "text": "Login\n\n\nWindows\n\n\nI get \"Invalid Token\" error on my Windows 10 machine when I run floyd login.\n\n\nIf you are using Windows command shell, there is an issue with pasting the token using the \nstandard \nCtrl + V\n shortcut. You need to use the Shell's Edit menu to paste the token. After copying the token from the browser, right click on the top bar of the command shell and select Edit -\n Paste. See image below:\n\n\n\n\nI still get the \"Invalid Token\" error after trying the above suggestion.\n\n\nIn some windows shells (like Git Bash) there is an extra space added to the token field\nbefore you paste the token. So you need to hit Backspace and clear out the field before pasting \nthe token. So the steps are:\n\n\n\n\nType \nfloyd login\n in the console.\n\n\nFrom the FloydHub web page, select the token and click on the \"Copy to clipboard\" button.\n\n\n\nIn the console, hit \"backspace\" a few times to remove the extra characters from the token login prompt request.\n\n\nRight click on the menu bar, and select \"Edit\", and then \"Paste\"\n\n\nThen press \"Enter\"\n\n\n\n\nYou should be able to login successfully now. If it's still not working, please give it a try on powershell.\n\n\nSignup\n\n\nHow does the free CPU / GPU hours work?\n\n\nEvery one who signups to Floydhub will receive 2 hours of free CPU / GPU time\nfor running your projects. We hope this will give you enough time to evaluate\nFloydhub for your needs. We are working on a new free plan right now to better\nhelp new users explore the platform.\n\n\nEmail Verification\n\n\nAfter you signup on FloydHub, you have to verify your email address. You will receive an automated email from Floyd with a link that you can click to verify.\n\n\nI did not receive my verification email\n\n\nAs soon as you sign up on FloydHub, you should receive an automated email in your inbox with instructions to verify your email address. \n\n\nIf you do not receive an email within a few minutes:\n\n\n\n\nPlease check your spam folder. If the email is there, please \"Mark as not Spam\" to avoid this happening in the future\n\n\nIf you still don't receive an email, please try resending the verification email by clicking on \"Resend Verification Email\" at \nfloydhub.com/settings/security\n\n\n\n\n\n\n\n\nIf this still doesn't work, it is likely that your mail server (e.g. your work email server) is filtering out our emails. Please check with your email adminstrator to allow emails from the \nfloydhub.com\n domain.\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Signup and Login"
        }, 
        {
            "location": "/faqs/authentication/#login", 
            "text": "", 
            "title": "Login"
        }, 
        {
            "location": "/faqs/authentication/#windows", 
            "text": "", 
            "title": "Windows"
        }, 
        {
            "location": "/faqs/authentication/#i-get-invalid-token-error-on-my-windows-10-machine-when-i-run-floyd-login", 
            "text": "If you are using Windows command shell, there is an issue with pasting the token using the \nstandard  Ctrl + V  shortcut. You need to use the Shell's Edit menu to paste the token. After copying the token from the browser, right click on the top bar of the command shell and select Edit -  Paste. See image below:", 
            "title": "I get \"Invalid Token\" error on my Windows 10 machine when I run floyd login."
        }, 
        {
            "location": "/faqs/authentication/#i-still-get-the-invalid-token-error-after-trying-the-above-suggestion", 
            "text": "In some windows shells (like Git Bash) there is an extra space added to the token field\nbefore you paste the token. So you need to hit Backspace and clear out the field before pasting \nthe token. So the steps are:   Type  floyd login  in the console.  From the FloydHub web page, select the token and click on the \"Copy to clipboard\" button.  In the console, hit \"backspace\" a few times to remove the extra characters from the token login prompt request.  Right click on the menu bar, and select \"Edit\", and then \"Paste\"  Then press \"Enter\"   You should be able to login successfully now. If it's still not working, please give it a try on powershell.", 
            "title": "I still get the \"Invalid Token\" error after trying the above suggestion."
        }, 
        {
            "location": "/faqs/authentication/#signup", 
            "text": "", 
            "title": "Signup"
        }, 
        {
            "location": "/faqs/authentication/#how-does-the-free-cpu-gpu-hours-work", 
            "text": "Every one who signups to Floydhub will receive 2 hours of free CPU / GPU time\nfor running your projects. We hope this will give you enough time to evaluate\nFloydhub for your needs. We are working on a new free plan right now to better\nhelp new users explore the platform.", 
            "title": "How does the free CPU / GPU hours work?"
        }, 
        {
            "location": "/faqs/authentication/#email-verification", 
            "text": "After you signup on FloydHub, you have to verify your email address. You will receive an automated email from Floyd with a link that you can click to verify.", 
            "title": "Email Verification"
        }, 
        {
            "location": "/faqs/authentication/#i-did-not-receive-my-verification-email", 
            "text": "As soon as you sign up on FloydHub, you should receive an automated email in your inbox with instructions to verify your email address.   If you do not receive an email within a few minutes:   Please check your spam folder. If the email is there, please \"Mark as not Spam\" to avoid this happening in the future  If you still don't receive an email, please try resending the verification email by clicking on \"Resend Verification Email\" at  floydhub.com/settings/security     If this still doesn't work, it is likely that your mail server (e.g. your work email server) is filtering out our emails. Please check with your email adminstrator to allow emails from the  floydhub.com  domain.", 
            "title": "I did not receive my verification email"
        }, 
        {
            "location": "/faqs/authentication/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/faqs/job/", 
            "text": "Why does \nfloyd status\n return an empty list even though I have several\n\n\nruns in my account?\n\n\nFloyd CLI uses the project directory to store the run information (similar to git). So you need\nto be in the directory where you initialized the project and you should be able to see all your\nruns. You can also use the \nweb dashboard\n to view all your\nprojects in one place.\n\n\nWhat do I do when I get \"What do you do when you get \u201cYou are over the allowed limits for this operation. Consider upgrading your account\u201d?\n\n\nFloydhub currently allows only 1 active job per user for trial plan and 3 active jobs for individual plan. If you require more concurrency, contact\nus from the \npricing\n page.\n\n\nI get \"Too many open files\" error when I run my project.\n\n\nFloyd CLI throws this error when you have too many files in your current directory that needs to be uploaded.\nThe actual limit depends on your OS / machine specs.\n\n\nYou can either:\n\n\n\n\nRemove unnecessary files from the directory (like build directory, docs etc.)\n\n\nAdd them to \n.floydignore\n file. Floyd CLI will just ignore these directories.\nSee the \nfloydignore\n documentation to understand how this can be configured.\n\n\nTar them into a single file and untar them at runtime.\n\n\n\n\nAlternatively, instead of uploading files from your local machine, you can also\n\ndownload files\n from a remote URL\ndirectly into Floyd servers.\n\n\nWhy do I get an \"Experiments limit reached\" error when I run a job?\n\n\nFloydHub currently allows only 1 active job in the free Trial plan and 3 active jobs in the Individual plan. \nIf you see an \nError\n:\n \nExperiments\n \nlimit\n \nreached\n message when you run a job, it means you have maxed out your \nconcurrency limits. Please stop you running job(s) or wait for them to finish, and try again.\n\n\nWe have to enforce this concurrency constraint because we have a finite number of GPU machines and have to ensure that no single user is starving the group. In the near future, we will support queueing of jobs so that you can queue multiple jobs to be run as slots become available.\n\n\nI ran my project in Jupyter mode but the url does not seem to work.\n\n\nJupyter notebook server takes a couple of minutes to start. Until then you will get a \"Bad Gateway\"\nor similar error when you access the URL. You can check the status of the Jupyter notebook\nby running the \nlogs\n command.\n\n\nAm I using the GPU instance by default?\n\n\nJobs are run on CPU instances by default. You can specify \n--gpu\n to run them on GPU instances.\n\n\nMy job is taking a while to \"sync changes\". How do I make it go faster?\n\n\nFloyd CLI uploads \nall\n the files in your current directory before starting your experiment.\nThere are a few ways to make this go faster:\n\n\n\n\nRemove unnecessary files from the directory (like build directory, docs etc.)\n\n\nAdd sub-directories to \n.floydignore\n file. Floyd CLI will ignore and not upload these sub-directories.\nSee the \ninit\n command and \nignore files guide\n to understand how this can be configured.\n\n\nIf you have large data files consider uploading them separately as a \ndata source\n.\nYou can then \nrefer\n to them in your project.\n\n\n\n\nMy job finished but how I do I see my output?\n\n\nYou can use the floyd \noutput\n command to view the output of your\nproject. If you want to use this output in your next run view \nthis guide\n.\n\n\nDo I have to pay for the entire time my Jupyter Notebook is running?\n\n\nUnfortunately, yes. As much as we would like to, we are unable to charge you only for the \ncomputation time\n.\n\n\nThis is an engineering challenge. When you start a Jupyter Notebook instance and start executing commands,\nyour state is maintained in memory (both CPU and GPU memory). So the instance has to be alive for the\nentire duration of your notebook, not just when you are executing commands.\n\n\nFor example, when you execute \nimport\n \ntensorflow\n\ncommand, Tensorflow will allocate the entire GPU memory to the current session and waits for the next command. This makes the instance unusable by anyone else, so we have to charge you for the duration your Notebook is alive.\n\n\nCan I view my Jupyter Notebook after my job has stopped?\n\n\nYes. When you use a Jupyter Notebook on FloydHub, your Notebook is saved periodically in the \n/output\n dir. So, your work is not lost after your job has ended, shutdown or timed out.\n\n\nYou can view your saved Notebook using the \nfloyd output\n command. Example:\n\n\n$ floyd output saip/projects/mnist-pytorch/3\n\n\n\n\nOr in the \nOutput\n tab of your job on the web dashboard, example: \nwww.floydhub.com/saip/projects/mnist-pytorch/3/output\n\n\nCan I restart a stopped or timed out job?\n\n\nUnfortunately, not directly. We will be implementing a single command to do this soon!\n\n\nIn the meanwhile, you can follow these steps to do this manually:\n\n\n\n\nJupyter Notebook\n: Your Notebook is \nsaved periodically\n. To restart your Notebook after your job has stopped, please download the output of your stopped job to your machine and start another job. Example:\n\n\n\n\n# Download the saved Notebook from previous job\n\n\n# NOTE: This will overwrite the contents of your current dir\n\n$ floyd data clone saip/projects/mnist-pytorch/3/output\n\n\n# Start a new job\n\n$ floyd run --mode jupyter\n\n\n\n\n\n\nScript\n: If you are running a script/command, you will have to start a new job using the \nfloyd run \ncommand\n command.\n\n\n\n\nWhy is my job in the \"Queued\" state for several minutes?\n\n\nThis means that a machine is being prepared to run your job. \n\n\nMost times, we have several CPU and GPU machines that are ready and your job can start execution in a few seconds. During high traffic periods, we may not have a machine ready for you and have to spin up a new instance for your job on-demand (details below). This might take up to 10 minutes in some cases. We are actively working on reducing this wait time.\n\n\nDetails\n: When you execute a \nfloyd run\n command, Floyd does several things in the background:\n\n\n\n\nProvision a CPU or GPU instance on the cloud\n\n\nSet up a deep learning environment with GPU drivers and the correct environment (as specified by \n--env\n) installed using Docker\n\n\nMount any data you specify using the \n--data\n flag\n\n\nSpin up a Jupyter server, if \n--mode jupyter\n flag\n\n\n\n\nEach of these steps can take up to a couple of minutes. Usually Steps 1 and 2 are already done, but during peak usage hours, we might have to do this on-demand.\n\n\nWhy do I see \"Setting up your instance...\" for several minutes when running a Jupyter Notebook?\n\n\nThe \nSetting up your instance...\n message is displayed when a machine is being prepared to run your Jupyter Notebook.\n\n\n\n\nWhen you execute a \nfloyd run --mode jupyter\n command, the CLI waits for a CPU or GPU machine to be ready before it opens up an interactive Jupyter Notebook for your work on. Usually, this takes a few seconds, but during high traffic periods it can take up to 10 minutes in some cases.\n\n\nFor more details on why it takes time, please see \nWhy is my job in the \"Queued\" state for several minutes?\n\n\nWhy are my logs not displayed in real-time?\n\n\nYou can stream your logs from the CLI using the \nfloyd logs -t \nJOB_NAME\n command. However, sometimes you may notice that the logs are not displayed in real-time. This is because of output buffering. Please make sure that your logs are flushed out if you prefer to view real-time logs.\n\n\nFor example, in Python:\n\n\nimport\n \nsys\n\n\n...\n\n\nprint\n(\nHello world\n)\n\n\nsys\n.\nstdout\n.\nflush\n()\n\n\n\n\n\nWhy did my job timeout after 1 hour?\n\n\nYou are likely in the Free Trial Plan. Jobs run in the trial plan have a maximum runtime of 1 hour. It will automatically timeout after that. \n\n\n\n\nYou can upgrade to the \nPaid Plan\n to overcome these limits.\n\n\nWhy was my CPU job Killed without warning?\n\n\nOccasionally, you may notice that your CPU job died without warning. The output logs just display \nKilled\n. For example,\n\n\n################################################################################\n\n\n\n2017\n-07-24 \n03\n:33:42,530 INFO - Run Output:\n...\n\n2017\n-07-24 \n03\n:33:52,920 INFO - Using TensorFlow backend.\n\n2017\n-07-24 \n03\n:34:04,381 INFO - \n loading UNet of size 1152x256...\n\n2017\n-07-24 \n03\n:34:10,942 INFO - Epoch \n1\n/100\n\n2017\n-07-24 \n03\n:35:17,221 INFO - Killed\n\n2017\n-07-24 \n03\n:35:18,680 INFO - \n\n################################################################################\n\n\n\n\n\nThis happens when your machine runs out of memory (OOM). i.e. your job consumes more memory than is available on our CPU machines.\n\n\nAll jobs run on FloydHub are executed inside a Docker container. Our current CPU machines have \n7GB memory\n. When memory used by your job exceeds 7GB, Docker automatically kills the job to protect the system and avoid abuse.\n\n\nThe resolution is to optimize your code to consume less memory. For example, read less data into your in-memory datastructures or reduce your batch size. We will be introducing more powerful CPUs, which higher memory in the near future.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Job"
        }, 
        {
            "location": "/faqs/job/#why-does-floyd-status-return-an-empty-list-even-though-i-have-several", 
            "text": "runs in my account?  Floyd CLI uses the project directory to store the run information (similar to git). So you need\nto be in the directory where you initialized the project and you should be able to see all your\nruns. You can also use the  web dashboard  to view all your\nprojects in one place.", 
            "title": "Why does floyd status return an empty list even though I have several"
        }, 
        {
            "location": "/faqs/job/#what-do-i-do-when-i-get-what-do-you-do-when-you-get-you-are-over-the-allowed-limits-for-this-operation-consider-upgrading-your-account", 
            "text": "Floydhub currently allows only 1 active job per user for trial plan and 3 active jobs for individual plan. If you require more concurrency, contact\nus from the  pricing  page.", 
            "title": "What do I do when I get \"What do you do when you get \u201cYou are over the allowed limits for this operation. Consider upgrading your account\u201d?"
        }, 
        {
            "location": "/faqs/job/#i-get-too-many-open-files-error-when-i-run-my-project", 
            "text": "Floyd CLI throws this error when you have too many files in your current directory that needs to be uploaded.\nThe actual limit depends on your OS / machine specs.  You can either:   Remove unnecessary files from the directory (like build directory, docs etc.)  Add them to  .floydignore  file. Floyd CLI will just ignore these directories.\nSee the  floydignore  documentation to understand how this can be configured.  Tar them into a single file and untar them at runtime.   Alternatively, instead of uploading files from your local machine, you can also download files  from a remote URL\ndirectly into Floyd servers.", 
            "title": "I get \"Too many open files\" error when I run my project."
        }, 
        {
            "location": "/faqs/job/#why-do-i-get-an-experiments-limit-reached-error-when-i-run-a-job", 
            "text": "FloydHub currently allows only 1 active job in the free Trial plan and 3 active jobs in the Individual plan. \nIf you see an  Error :   Experiments   limit   reached  message when you run a job, it means you have maxed out your \nconcurrency limits. Please stop you running job(s) or wait for them to finish, and try again.  We have to enforce this concurrency constraint because we have a finite number of GPU machines and have to ensure that no single user is starving the group. In the near future, we will support queueing of jobs so that you can queue multiple jobs to be run as slots become available.", 
            "title": "Why do I get an \"Experiments limit reached\" error when I run a job?"
        }, 
        {
            "location": "/faqs/job/#i-ran-my-project-in-jupyter-mode-but-the-url-does-not-seem-to-work", 
            "text": "Jupyter notebook server takes a couple of minutes to start. Until then you will get a \"Bad Gateway\"\nor similar error when you access the URL. You can check the status of the Jupyter notebook\nby running the  logs  command.", 
            "title": "I ran my project in Jupyter mode but the url does not seem to work."
        }, 
        {
            "location": "/faqs/job/#am-i-using-the-gpu-instance-by-default", 
            "text": "Jobs are run on CPU instances by default. You can specify  --gpu  to run them on GPU instances.", 
            "title": "Am I using the GPU instance by default?"
        }, 
        {
            "location": "/faqs/job/#my-job-is-taking-a-while-to-sync-changes-how-do-i-make-it-go-faster", 
            "text": "Floyd CLI uploads  all  the files in your current directory before starting your experiment.\nThere are a few ways to make this go faster:   Remove unnecessary files from the directory (like build directory, docs etc.)  Add sub-directories to  .floydignore  file. Floyd CLI will ignore and not upload these sub-directories.\nSee the  init  command and  ignore files guide  to understand how this can be configured.  If you have large data files consider uploading them separately as a  data source .\nYou can then  refer  to them in your project.", 
            "title": "My job is taking a while to \"sync changes\". How do I make it go faster?"
        }, 
        {
            "location": "/faqs/job/#my-job-finished-but-how-i-do-i-see-my-output", 
            "text": "You can use the floyd  output  command to view the output of your\nproject. If you want to use this output in your next run view  this guide .", 
            "title": "My job finished but how I do I see my output?"
        }, 
        {
            "location": "/faqs/job/#do-i-have-to-pay-for-the-entire-time-my-jupyter-notebook-is-running", 
            "text": "Unfortunately, yes. As much as we would like to, we are unable to charge you only for the  computation time .  This is an engineering challenge. When you start a Jupyter Notebook instance and start executing commands,\nyour state is maintained in memory (both CPU and GPU memory). So the instance has to be alive for the\nentire duration of your notebook, not just when you are executing commands.  For example, when you execute  import   tensorflow \ncommand, Tensorflow will allocate the entire GPU memory to the current session and waits for the next command. This makes the instance unusable by anyone else, so we have to charge you for the duration your Notebook is alive.", 
            "title": "Do I have to pay for the entire time my Jupyter Notebook is running?"
        }, 
        {
            "location": "/faqs/job/#can-i-view-my-jupyter-notebook-after-my-job-has-stopped", 
            "text": "Yes. When you use a Jupyter Notebook on FloydHub, your Notebook is saved periodically in the  /output  dir. So, your work is not lost after your job has ended, shutdown or timed out.  You can view your saved Notebook using the  floyd output  command. Example:  $ floyd output saip/projects/mnist-pytorch/3  Or in the  Output  tab of your job on the web dashboard, example:  www.floydhub.com/saip/projects/mnist-pytorch/3/output", 
            "title": "Can I view my Jupyter Notebook after my job has stopped?"
        }, 
        {
            "location": "/faqs/job/#can-i-restart-a-stopped-or-timed-out-job", 
            "text": "Unfortunately, not directly. We will be implementing a single command to do this soon!  In the meanwhile, you can follow these steps to do this manually:   Jupyter Notebook : Your Notebook is  saved periodically . To restart your Notebook after your job has stopped, please download the output of your stopped job to your machine and start another job. Example:   # Download the saved Notebook from previous job  # NOTE: This will overwrite the contents of your current dir \n$ floyd data clone saip/projects/mnist-pytorch/3/output # Start a new job \n$ floyd run --mode jupyter   Script : If you are running a script/command, you will have to start a new job using the  floyd run  command  command.", 
            "title": "Can I restart a stopped or timed out job?"
        }, 
        {
            "location": "/faqs/job/#why-is-my-job-in-the-queued-state-for-several-minutes", 
            "text": "This means that a machine is being prepared to run your job.   Most times, we have several CPU and GPU machines that are ready and your job can start execution in a few seconds. During high traffic periods, we may not have a machine ready for you and have to spin up a new instance for your job on-demand (details below). This might take up to 10 minutes in some cases. We are actively working on reducing this wait time.  Details : When you execute a  floyd run  command, Floyd does several things in the background:   Provision a CPU or GPU instance on the cloud  Set up a deep learning environment with GPU drivers and the correct environment (as specified by  --env ) installed using Docker  Mount any data you specify using the  --data  flag  Spin up a Jupyter server, if  --mode jupyter  flag   Each of these steps can take up to a couple of minutes. Usually Steps 1 and 2 are already done, but during peak usage hours, we might have to do this on-demand.", 
            "title": "Why is my job in the \"Queued\" state for several minutes?"
        }, 
        {
            "location": "/faqs/job/#why-do-i-see-setting-up-your-instance-for-several-minutes-when-running-a-jupyter-notebook", 
            "text": "The  Setting up your instance...  message is displayed when a machine is being prepared to run your Jupyter Notebook.   When you execute a  floyd run --mode jupyter  command, the CLI waits for a CPU or GPU machine to be ready before it opens up an interactive Jupyter Notebook for your work on. Usually, this takes a few seconds, but during high traffic periods it can take up to 10 minutes in some cases.  For more details on why it takes time, please see  Why is my job in the \"Queued\" state for several minutes?", 
            "title": "Why do I see \"Setting up your instance...\" for several minutes when running a Jupyter Notebook?"
        }, 
        {
            "location": "/faqs/job/#why-are-my-logs-not-displayed-in-real-time", 
            "text": "You can stream your logs from the CLI using the  floyd logs -t  JOB_NAME  command. However, sometimes you may notice that the logs are not displayed in real-time. This is because of output buffering. Please make sure that your logs are flushed out if you prefer to view real-time logs.  For example, in Python:  import   sys  ...  print ( Hello world )  sys . stdout . flush ()", 
            "title": "Why are my logs not displayed in real-time?"
        }, 
        {
            "location": "/faqs/job/#why-did-my-job-timeout-after-1-hour", 
            "text": "You are likely in the Free Trial Plan. Jobs run in the trial plan have a maximum runtime of 1 hour. It will automatically timeout after that.    You can upgrade to the  Paid Plan  to overcome these limits.", 
            "title": "Why did my job timeout after 1 hour?"
        }, 
        {
            "location": "/faqs/job/#why-was-my-cpu-job-killed-without-warning", 
            "text": "Occasionally, you may notice that your CPU job died without warning. The output logs just display  Killed . For example,  ################################################################################  2017 -07-24  03 :33:42,530 INFO - Run Output:\n... 2017 -07-24  03 :33:52,920 INFO - Using TensorFlow backend. 2017 -07-24  03 :34:04,381 INFO -   loading UNet of size 1152x256... 2017 -07-24  03 :34:10,942 INFO - Epoch  1 /100 2017 -07-24  03 :35:17,221 INFO - Killed 2017 -07-24  03 :35:18,680 INFO -  ################################################################################   This happens when your machine runs out of memory (OOM). i.e. your job consumes more memory than is available on our CPU machines.  All jobs run on FloydHub are executed inside a Docker container. Our current CPU machines have  7GB memory . When memory used by your job exceeds 7GB, Docker automatically kills the job to protect the system and avoid abuse.  The resolution is to optimize your code to consume less memory. For example, read less data into your in-memory datastructures or reduce your batch size. We will be introducing more powerful CPUs, which higher memory in the near future.", 
            "title": "Why was my CPU job Killed without warning?"
        }, 
        {
            "location": "/faqs/job/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/faqs/plans/", 
            "text": "Please see here for \nBilling FAQs\n\n\nPlans\n\n\nWhich Plan is right for me?\n\n\nFree Plan\n\n\nIf you're just exploring, the Free plan is for you! You are automatically enrolled in the Free plan when you sign up on FloydHub. It includes \n20 hours of free CPU every month. You cannot, however, use a GPU or run multiple jobs \nin parallel.\n\n\nData Scientist Plan\n\n\nOur Data Scientist plans offer varying levels of \njob concurrency\n, GPU computing hours \nand storage. You can also purchase \nPowerups\n to add more compute hours to supplement your plan.\n\n\nData Scientist Base Plan\n: If you are getting started with deep learning and primarily use Jupyter Notebooks, the Base plan should work well for you. You can run 2 concurrent jobs on GPU and get 100 GB of storage included in the plan. \n\n\nData Scientist Plus and Pro Plans\n: If you are a more advanced user, you may need more job concurrency to run multiple parallel experiments and more storage for your data. The Plus and Pro plans will serve you better. \n\n\nPlease see the \nfeature comparison table\n for a full list of features.\n\n\nWhat is in the Trial plan?\n\n\nAll users that sign up on FloydHub are automatically enrolled in the Free plan. Additionally, we offer 2 hours of free GPU credits to try out FloydHub. The GPU credits expire in 14 days, but you will be able to use all the features of the Free plan forever.\n\n\nTake FloydHub for a pin with our \nQuick Start Guide\n or \nJupyter Notebook Guide\n!\n\n\nWhat is included in the Free plan?\n\n\nThe free plan comes with the following:\n\n\n\n\n20 hours of CPU compute / month\n\n\n10 GB free storage\n\n\nUnlimited public projects and datasets\n\n\nConcurrent jobs: You can only run one job at a time\n\n\n6 hour job timeout: The maximum runtime of a job on the Trial Plan is 6 hours. It will automatically timeout after that\n\n\n\n\nYou can upgrade to one of the \nData Scientist Plan\n to overcome these limits.\n\n\nDo the plans come with preemptible or dedicated instances?\n\n\nThe GPU and CPU compute hours included in your plan (Free or Data Scientist) are \n\npreemptible instances\n. This means that there is a small chance that your job will be terminated without notice. In practice, this happens infrequently and this is perfect for most users. If you need \ndedicated instances\n for your jobs, you \ncan buy the GPU+ or CPU+ Powerups.\n\n\nDo my remaining compute credits roll over each month if I don't use them all?\n\n\nNo, your monthly Plan compute credits are not rolled over. However, your Powerup credits will remain valid for one year from purchase date.\n\n\nWhat happened to the old Pay-as-you-go Individual Plan?\n\n\nWe are transitioning from the Individual Plan, which offered a pay-as-you-go payment method, to our current pricing plan. The Individual Plan is no longer available for new users. \n\n\nI am in the Pay-as-you-go Individual Plan. What will happen to me?\n\n\nIf you signed up for the Individual Plan before August 20\nth\n 2017, you will be grandfathered till October 1\nst\n 2017. After this, you will be automatically enrolled in the Free plan. Please note that any remaining promotional credits will also expire on this date.\n\n\nPlease \nupgrade\n to one of the Data Scientist plans to continue using FloydHub without interruption. We will also be reaching out to you with more information about this transition.\n\n\nWhy did I not get 100 free GPU hours when I signed up?\n\n\nWe offered 100 hours of free GPU for all users during our promotional period. This has ended.\n\n\nWill my free credits expire?\n\n\nThe current Trial plan includes 2 hours of free GPU credits. These will expire 14 days from the day you sign up. After 14 days, you will be transitioned to our Free plan.\n\n\nAre there any academic discounts for students?\n\n\nWe don't have discounts. However, a lot of students create content for us. If you are willing to contribute high quality content to FLoydHub, we will give you \nfree GPU credits\n in exchange! \n\n\nContent we are looking for: \n\n\n\n\nTechnical blogs on deep learning and AI\n\n\nFloydHub tutorials, text or video\n\n\nPort popular deep learning projects to FloydHub\n\n\nCreate interesting datasets\n\n\nInsert your own idea here\n\n\n\n\nIf this is interesting to you, please let us know about it \nhere\n.\n\n\nCompute\n\n\nWhat is job concurrency?\n\n\nJob concurrency is the number of jobs you can run in parallel. Each plan has a limit \non the number of concurrent jobs you can run. For example, in the Free plan, \nyou can only run 1 job at a time. In the Data Scientist Pro plan, you can run up \nto 8 jobs in parallel.\n\n\nHaving a higher concurrency is useful when you want to parallelize your training, for \nexample while hyperparameter sweeping.\n\n\nWhat will happen to my running job when I run out of computing credits?\n\n\nYou job will be shutdown immediately when you run out of computing credits. \n\n\nIf you run long-running jobs and expect them to exceed the computing hours offered by your plan, you can purchase \nPowerups\n.\n\n\nYou can also enable auto-refresh on your Powerups to ensure your long-running jobs are never killed because you ran out of computing hours. We'll automatically refresh your selected Powerup so that your job can continue running.\n\n\nPreemptible Instances\n\n\nPreemptible instances have medium job uptime SLA of 98%. This means that there is a small chance that your job can be terminated (preempted) at any point during its runtime by FloydHub if it requires access to those resources for other, higher priority tasks. \n\n\nPreemptible instances (CPU / GPU) offer top notch compute at affordable prices, in exchange for fault tolerance.\n\n\nNote that SLA refers to what we can guarantee. In practice, this happens infrequently. Historically, less than 0.1% of jobs run on FloydHub have encountered interruption. However, you need to be aware that there is the possibility.\n\n\nDedicated Instances\n\n\nDedicated instances have high job uptime SLA of 99.95%. Use dedicated instances for your jobs if they are critical or not fault tolerant. You can purchase '+' \nPowerups\n (CPU+ / GPU+) to utilize dedicated instances.\n\n\nWhy do you use preemptible instances?\n\n\nTo be able to offer you compute at a much lower cost.\n\n\nWe have a fixed pool of resources that we have to allocate amongst all our users. Some of our users require dedicated instances and are willing to pay the premium for uninterrupted access. But, the majority of our users can tolerate a 98% job uptime SLA for the significant price savings that preemptible instances offer.\n\n\nShould I use Dedicated instances?\n\n\nIf your job is not fault tolerant and cannot withstand a small (\n2%) chance \nof your job being shutdown without notice, you should use our \n+\n Dedicated instances. \nPrice sensitivity also plays a factor - dedicated instances are more expensive than \npremptible instances.\n\n\nGiven that deep learning models typically train over long periods of time, it \nis good practice to build your application to be fault tolerant by regularly checkpointing your training.\n\n\nWhat is the difference between GPU vs. GPU+ and CPU vs. CPU+?\n\n\nGPU and CPU are preemptible instances. GPU+ and CPU+ are dedicated instances.\n\n\nWhat is the SLA of Preemptible instances and Dedicated instances?\n\n\nPreemptible instances have 98% job up time SLA. Dedicated instances have 99.95% job up time SLA.\n\n\nWill I get a refund if my job is preempted?\n\n\nNo. \n\n\nOur preemptible instances have a 98% job uptime SLA. By using them, you are accepting a small chance \nof your job being terminated without notice, in exchange for paying a much lower price than dedicated instances.\n\n\nHow will I know when my job is preempted?\n\n\nYour job's state will turn from \nRunning\n to \nShutdown\n. We will send you a notification informing you about this. Unfortunately, we are currently unable to warn your ahead of time of an impending preemption.\n\n\nPowerups\n\n\nWhat are Powerups?\n\n\nYour subscription plan comes with a monthly quota of CPU and GPU computing hours. \nIf you need more computing hours, you can buy Powerups to supplement your plan.\n\n\nWhat Powerups should I buy?\n\n\nThis depends on your computing needs. We offer multiple tiers of Powerups:\n\n\n\n\nPreemptible vs. Dedicated\n: CPU / GPU are affordable \npreemptible instances\n, CPU+ / GPU+ are high-reliability \ndedicated instances\n.\n\n\n10 vs. 50 vs. 100 hours\n: Purchase a pack that suits your computing needs. Note that the larger packs offer compute at a much cheaper rate/hour than smaller packs.\n\n\nAuto-refresh\n: You can enable auto-refresh on any pack.\n\n\n\n\nIf you are \njust starting out\n and need more computing hours than your plan offers, you can start with the \nGPU10 Powerup\n. \n\n\nIf you run \nlong-running jobs\n, you should purchase the \nGPU100 with auto-refresh enabled\n, to ensure that you never run out of computing credits.\n\n\nIf you run \ncritical jobs\n that are not fault-tolerant, you should purchase the \nGPU+ Powerup\n.\n\n\nHow can I buy Powerups?\n\n\nYou can purchase them from your \nPowerups Dashboard\n\n\nWhy would I enable auto-refresh?\n\n\nAuto-refresh ensures your long-running jobs are never killed because you \nran out of computing hours\n. We'll automatically refresh your selected Powerup so that your Job can continue running.\n\n\nCan I buy a Powerup if I am in the Free Plan?\n\n\nNo. You have to be enrolled in one of the Data Scientist plans to be eligible for \npurchasing Powerups.\n\n\nDo Powerups expire?\n\n\nYes. Powerups are valid for 1 year from the date of purchase.\n\n\nHow will my Powerups be used?\n\n\nYour compute hours will be consumed in the following order:\n\n\n\n\nHours from your subscription plan\n\n\nHours from free credits\n\n\nHours from Powerups\n\n\n\n\nStorage\n\n\nHow much storage do I get?\n\n\nEach plan comes with its own storage limit. For example, the Free plan has \n10GB storage, while the Data Scientist Pro plan has 400GB. Please see the \n\nfeature comparison table\n for details.\n\n\nWhat counts against my storage?\n\n\nStorage is consumed by the datasets that you upload, your code and the data that your jobs output.\n\n\nNote that you are only responsible for the data that you own. For example, if you use a public dataset in your job, you won't be charged for it.\n\n\nCan I buy more storage than my plan offers?\n\n\nThe Data Scientist Pro plan includes 400GB of storage. If this doesn't fit your needs, \nplease contact us at \n.\n\n\nWe will soon have a storage Powerup that can you buy to add more storage to your base plan. \n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Plans"
        }, 
        {
            "location": "/faqs/plans/#plans", 
            "text": "", 
            "title": "Plans"
        }, 
        {
            "location": "/faqs/plans/#which-plan-is-right-for-me", 
            "text": "", 
            "title": "Which Plan is right for me?"
        }, 
        {
            "location": "/faqs/plans/#free-plan", 
            "text": "If you're just exploring, the Free plan is for you! You are automatically enrolled in the Free plan when you sign up on FloydHub. It includes \n20 hours of free CPU every month. You cannot, however, use a GPU or run multiple jobs \nin parallel.", 
            "title": "Free Plan"
        }, 
        {
            "location": "/faqs/plans/#data-scientist-plan", 
            "text": "Our Data Scientist plans offer varying levels of  job concurrency , GPU computing hours \nand storage. You can also purchase  Powerups  to add more compute hours to supplement your plan.  Data Scientist Base Plan : If you are getting started with deep learning and primarily use Jupyter Notebooks, the Base plan should work well for you. You can run 2 concurrent jobs on GPU and get 100 GB of storage included in the plan.   Data Scientist Plus and Pro Plans : If you are a more advanced user, you may need more job concurrency to run multiple parallel experiments and more storage for your data. The Plus and Pro plans will serve you better.   Please see the  feature comparison table  for a full list of features.", 
            "title": "Data Scientist Plan"
        }, 
        {
            "location": "/faqs/plans/#what-is-in-the-trial-plan", 
            "text": "All users that sign up on FloydHub are automatically enrolled in the Free plan. Additionally, we offer 2 hours of free GPU credits to try out FloydHub. The GPU credits expire in 14 days, but you will be able to use all the features of the Free plan forever.  Take FloydHub for a pin with our  Quick Start Guide  or  Jupyter Notebook Guide !", 
            "title": "What is in the Trial plan?"
        }, 
        {
            "location": "/faqs/plans/#what-is-included-in-the-free-plan", 
            "text": "The free plan comes with the following:   20 hours of CPU compute / month  10 GB free storage  Unlimited public projects and datasets  Concurrent jobs: You can only run one job at a time  6 hour job timeout: The maximum runtime of a job on the Trial Plan is 6 hours. It will automatically timeout after that   You can upgrade to one of the  Data Scientist Plan  to overcome these limits.", 
            "title": "What is included in the Free plan?"
        }, 
        {
            "location": "/faqs/plans/#do-the-plans-come-with-preemptible-or-dedicated-instances", 
            "text": "The GPU and CPU compute hours included in your plan (Free or Data Scientist) are  preemptible instances . This means that there is a small chance that your job will be terminated without notice. In practice, this happens infrequently and this is perfect for most users. If you need  dedicated instances  for your jobs, you \ncan buy the GPU+ or CPU+ Powerups.", 
            "title": "Do the plans come with preemptible or dedicated instances?"
        }, 
        {
            "location": "/faqs/plans/#do-my-remaining-compute-credits-roll-over-each-month-if-i-dont-use-them-all", 
            "text": "No, your monthly Plan compute credits are not rolled over. However, your Powerup credits will remain valid for one year from purchase date.", 
            "title": "Do my remaining compute credits roll over each month if I don't use them all?"
        }, 
        {
            "location": "/faqs/plans/#what-happened-to-the-old-pay-as-you-go-individual-plan", 
            "text": "We are transitioning from the Individual Plan, which offered a pay-as-you-go payment method, to our current pricing plan. The Individual Plan is no longer available for new users.", 
            "title": "What happened to the old Pay-as-you-go Individual Plan?"
        }, 
        {
            "location": "/faqs/plans/#i-am-in-the-pay-as-you-go-individual-plan-what-will-happen-to-me", 
            "text": "If you signed up for the Individual Plan before August 20 th  2017, you will be grandfathered till October 1 st  2017. After this, you will be automatically enrolled in the Free plan. Please note that any remaining promotional credits will also expire on this date.  Please  upgrade  to one of the Data Scientist plans to continue using FloydHub without interruption. We will also be reaching out to you with more information about this transition.", 
            "title": "I am in the Pay-as-you-go Individual Plan. What will happen to me?"
        }, 
        {
            "location": "/faqs/plans/#why-did-i-not-get-100-free-gpu-hours-when-i-signed-up", 
            "text": "We offered 100 hours of free GPU for all users during our promotional period. This has ended.", 
            "title": "Why did I not get 100 free GPU hours when I signed up?"
        }, 
        {
            "location": "/faqs/plans/#will-my-free-credits-expire", 
            "text": "The current Trial plan includes 2 hours of free GPU credits. These will expire 14 days from the day you sign up. After 14 days, you will be transitioned to our Free plan.", 
            "title": "Will my free credits expire?"
        }, 
        {
            "location": "/faqs/plans/#are-there-any-academic-discounts-for-students", 
            "text": "We don't have discounts. However, a lot of students create content for us. If you are willing to contribute high quality content to FLoydHub, we will give you  free GPU credits  in exchange!   Content we are looking for:    Technical blogs on deep learning and AI  FloydHub tutorials, text or video  Port popular deep learning projects to FloydHub  Create interesting datasets  Insert your own idea here   If this is interesting to you, please let us know about it  here .", 
            "title": "Are there any academic discounts for students?"
        }, 
        {
            "location": "/faqs/plans/#compute", 
            "text": "", 
            "title": "Compute"
        }, 
        {
            "location": "/faqs/plans/#what-is-job-concurrency", 
            "text": "Job concurrency is the number of jobs you can run in parallel. Each plan has a limit \non the number of concurrent jobs you can run. For example, in the Free plan, \nyou can only run 1 job at a time. In the Data Scientist Pro plan, you can run up \nto 8 jobs in parallel.  Having a higher concurrency is useful when you want to parallelize your training, for \nexample while hyperparameter sweeping.", 
            "title": "What is job concurrency?"
        }, 
        {
            "location": "/faqs/plans/#what-will-happen-to-my-running-job-when-i-run-out-of-computing-credits", 
            "text": "You job will be shutdown immediately when you run out of computing credits.   If you run long-running jobs and expect them to exceed the computing hours offered by your plan, you can purchase  Powerups .  You can also enable auto-refresh on your Powerups to ensure your long-running jobs are never killed because you ran out of computing hours. We'll automatically refresh your selected Powerup so that your job can continue running.", 
            "title": "What will happen to my running job when I run out of computing credits?"
        }, 
        {
            "location": "/faqs/plans/#preemptible-instances", 
            "text": "Preemptible instances have medium job uptime SLA of 98%. This means that there is a small chance that your job can be terminated (preempted) at any point during its runtime by FloydHub if it requires access to those resources for other, higher priority tasks.   Preemptible instances (CPU / GPU) offer top notch compute at affordable prices, in exchange for fault tolerance.  Note that SLA refers to what we can guarantee. In practice, this happens infrequently. Historically, less than 0.1% of jobs run on FloydHub have encountered interruption. However, you need to be aware that there is the possibility.", 
            "title": "Preemptible Instances"
        }, 
        {
            "location": "/faqs/plans/#dedicated-instances", 
            "text": "Dedicated instances have high job uptime SLA of 99.95%. Use dedicated instances for your jobs if they are critical or not fault tolerant. You can purchase '+'  Powerups  (CPU+ / GPU+) to utilize dedicated instances.", 
            "title": "Dedicated Instances"
        }, 
        {
            "location": "/faqs/plans/#why-do-you-use-preemptible-instances", 
            "text": "To be able to offer you compute at a much lower cost.  We have a fixed pool of resources that we have to allocate amongst all our users. Some of our users require dedicated instances and are willing to pay the premium for uninterrupted access. But, the majority of our users can tolerate a 98% job uptime SLA for the significant price savings that preemptible instances offer.", 
            "title": "Why do you use preemptible instances?"
        }, 
        {
            "location": "/faqs/plans/#should-i-use-dedicated-instances", 
            "text": "If your job is not fault tolerant and cannot withstand a small ( 2%) chance \nof your job being shutdown without notice, you should use our  +  Dedicated instances. \nPrice sensitivity also plays a factor - dedicated instances are more expensive than \npremptible instances.  Given that deep learning models typically train over long periods of time, it \nis good practice to build your application to be fault tolerant by regularly checkpointing your training.", 
            "title": "Should I use Dedicated instances?"
        }, 
        {
            "location": "/faqs/plans/#what-is-the-difference-between-gpu-vs-gpu-and-cpu-vs-cpu", 
            "text": "GPU and CPU are preemptible instances. GPU+ and CPU+ are dedicated instances.", 
            "title": "What is the difference between GPU vs. GPU+ and CPU vs. CPU+?"
        }, 
        {
            "location": "/faqs/plans/#what-is-the-sla-of-preemptible-instances-and-dedicated-instances", 
            "text": "Preemptible instances have 98% job up time SLA. Dedicated instances have 99.95% job up time SLA.", 
            "title": "What is the SLA of Preemptible instances and Dedicated instances?"
        }, 
        {
            "location": "/faqs/plans/#will-i-get-a-refund-if-my-job-is-preempted", 
            "text": "No.   Our preemptible instances have a 98% job uptime SLA. By using them, you are accepting a small chance \nof your job being terminated without notice, in exchange for paying a much lower price than dedicated instances.", 
            "title": "Will I get a refund if my job is preempted?"
        }, 
        {
            "location": "/faqs/plans/#how-will-i-know-when-my-job-is-preempted", 
            "text": "Your job's state will turn from  Running  to  Shutdown . We will send you a notification informing you about this. Unfortunately, we are currently unable to warn your ahead of time of an impending preemption.", 
            "title": "How will I know when my job is preempted?"
        }, 
        {
            "location": "/faqs/plans/#powerups", 
            "text": "", 
            "title": "Powerups"
        }, 
        {
            "location": "/faqs/plans/#what-are-powerups", 
            "text": "Your subscription plan comes with a monthly quota of CPU and GPU computing hours. \nIf you need more computing hours, you can buy Powerups to supplement your plan.", 
            "title": "What are Powerups?"
        }, 
        {
            "location": "/faqs/plans/#what-powerups-should-i-buy", 
            "text": "This depends on your computing needs. We offer multiple tiers of Powerups:   Preemptible vs. Dedicated : CPU / GPU are affordable  preemptible instances , CPU+ / GPU+ are high-reliability  dedicated instances .  10 vs. 50 vs. 100 hours : Purchase a pack that suits your computing needs. Note that the larger packs offer compute at a much cheaper rate/hour than smaller packs.  Auto-refresh : You can enable auto-refresh on any pack.   If you are  just starting out  and need more computing hours than your plan offers, you can start with the  GPU10 Powerup .   If you run  long-running jobs , you should purchase the  GPU100 with auto-refresh enabled , to ensure that you never run out of computing credits.  If you run  critical jobs  that are not fault-tolerant, you should purchase the  GPU+ Powerup .", 
            "title": "What Powerups should I buy?"
        }, 
        {
            "location": "/faqs/plans/#how-can-i-buy-powerups", 
            "text": "You can purchase them from your  Powerups Dashboard", 
            "title": "How can I buy Powerups?"
        }, 
        {
            "location": "/faqs/plans/#why-would-i-enable-auto-refresh", 
            "text": "Auto-refresh ensures your long-running jobs are never killed because you  ran out of computing hours . We'll automatically refresh your selected Powerup so that your Job can continue running.", 
            "title": "Why would I enable auto-refresh?"
        }, 
        {
            "location": "/faqs/plans/#can-i-buy-a-powerup-if-i-am-in-the-free-plan", 
            "text": "No. You have to be enrolled in one of the Data Scientist plans to be eligible for \npurchasing Powerups.", 
            "title": "Can I buy a Powerup if I am in the Free Plan?"
        }, 
        {
            "location": "/faqs/plans/#do-powerups-expire", 
            "text": "Yes. Powerups are valid for 1 year from the date of purchase.", 
            "title": "Do Powerups expire?"
        }, 
        {
            "location": "/faqs/plans/#how-will-my-powerups-be-used", 
            "text": "Your compute hours will be consumed in the following order:   Hours from your subscription plan  Hours from free credits  Hours from Powerups", 
            "title": "How will my Powerups be used?"
        }, 
        {
            "location": "/faqs/plans/#storage", 
            "text": "", 
            "title": "Storage"
        }, 
        {
            "location": "/faqs/plans/#how-much-storage-do-i-get", 
            "text": "Each plan comes with its own storage limit. For example, the Free plan has \n10GB storage, while the Data Scientist Pro plan has 400GB. Please see the  feature comparison table  for details.", 
            "title": "How much storage do I get?"
        }, 
        {
            "location": "/faqs/plans/#what-counts-against-my-storage", 
            "text": "Storage is consumed by the datasets that you upload, your code and the data that your jobs output.  Note that you are only responsible for the data that you own. For example, if you use a public dataset in your job, you won't be charged for it.", 
            "title": "What counts against my storage?"
        }, 
        {
            "location": "/faqs/plans/#can-i-buy-more-storage-than-my-plan-offers", 
            "text": "The Data Scientist Pro plan includes 400GB of storage. If this doesn't fit your needs, \nplease contact us at  .  We will soon have a storage Powerup that can you buy to add more storage to your base plan.", 
            "title": "Can I buy more storage than my plan offers?"
        }, 
        {
            "location": "/faqs/plans/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/faqs/billing/", 
            "text": "Please see here for \nPlan FAQs\n\n\nPayment Questions\n\n\nWhat forms of payments do you accept?\n\n\nWe accept Visa, MasterCard, American Express and Discover credit and debit cards. We do not accept prepaid cards at the moment. \n\n\nWe are a US based company. If you are outside the US, please ensure your card has \ninternational transactions enabled.\n\n\nDo you keep my credit card information?\n\n\nNo, we do not retain any credit card information. We use\n\nStripe\n to process payments.\n\n\nPayment processing failed. Whay am I unable to add my payment method?\n\n\nWhen you submit your payment details, you may received a \"Error: Payment processing failed\" error notification.\n\n\n\n\nSome common causes are:\n\n\n\n\nCredit and Debit cards only\n: We currently accept only Visa, Mastercard and AmEx credit and debit cards. We do not accept prepaid cards. Please ensure you are using a valid credit or debit card.\n\n\nBalance\n: We issue a small $0-1 transaction on your card as a pending authorization request between our billing system and the bank that issued your credit or debit card. If this authorization fails, you won't be able to add your card. Please ensure you have enough balance and are using a valid card. \nNote\n: This is an authorization request only, not an actual charge.\n\n\nFraud Detection\n: We use Stripe for managing all our payments. They have their own fraud detection algorithm which can decline some cards. Please try a different payment method.\n\n\nInternational Cards\n: We are a US based company. If you are outside the US please ensure your card has international transactions enabled.\n\n\n\n\nIf you are still unable to add your payment, please contact us directly at \n.\n\n\nBilling Questions\n\n\nWhat am I billed for?\n\n\nYour usage includes compute (CPU / GPU) usage and storage consumption. \n\n\n\n\n\n\nCompute\n: You will be billed exactly for the duration that your job runs, rounded off to the nearest second.\n\n\nNote that you are only charged for compute when your job is in the \nRunning\n state. You will \nnot\n be charged when your job is in any other state, including \nQueued\n and \nShutdown\n.\n\n\n\n\n\n\nStorage\n: You will also be charged for the storage you consume, rounded off to the nearest kB. \n\n\nStorage is consumed by the datasets that you upload, your code and the data that your jobs output.\n\n\n\n\n\n\nWhen will my card be charged?\n\n\nFor subscription plans, your card will be charged every month on the day you upgraded to the plan. For example, if you upgraded from the Free to the Data Scientist Pro plan on the 22\nnd\n of August, you will be charged immediately. You will be then billed on the 22\nnd\n of every month.\n\n\nWhen you purchase Powerups, either directly from the \nPowerups Dashboard\n or via auto-refresh, you will be charged at the time of purchase.\n\n\nCan I upgrade or downgrade my plan?\n\n\nYou can upgrade or downgrade your subscriptions at anytime from your \nPlans page\n under Settings on your dashboard.\n\n\nWhen you upgrade, you will be immediately elevated to the new plan. When you downgrade, your new plan will start at the end of your billing cycle (since you will have already paid for the month in advance).\n\n\nUpgrades and downgrades inside the Data Scientist plans do not affect any Powerups you \nmay have purchased.\n\n\nHow do upgrades work?\n\n\nWhen you upgrade from the Free plan to a paid plan, you will be charged immediately. You will then be billed on the 1 month anniversary of your subscription date every month. For example, if you upgrade on August 20\nth\n, 2017, you will be charged on that day and on the 20\nth\n of every subsequent month.\n\n\nWhen you upgrade from one paid plan to another, you will be charged for your new plan on a pro-rated basis. Lets say your billing cycle is on the 20\nth\n of every month. If you upgrade from the Data Scientist Base to the Pro plan on the 5\nth\n, you will be charged for the new plan on a pro-rated basis (5\nth\n to 20\nth\n).\n\n\nHow do downgrades work?\n\n\nYou can downgrade at any time to a lower or Free plan. \n\n\nIf you are over the usage limits for the plan you are downgrading to, you will have to handle that first. For example, if you are downgrading from the Data Scientist Pro to Base plan, but have 300GB data, you will have to delete some of your data before downgrading since the Base plan only offers 100 GB.\n\n\nIf you downgrade to the Free plan, you will no longer have access to any Powerups that you may have purchased.\n\n\nHow do I remove my credit card?\n\n\nPlease downgrade to the Free plan. We will remove your credit card at the end of your billing cycle.\n\n\nDo my remaining compute credits roll over each month if I don't use them all?\n\n\nNo, your monthly Plan compute credits are not rolled over. However, your Powerup credits will remain valid for one year from purchase date.\n\n\nDo you offer refunds?\n\n\nNo, we do not offer refunds. If there are extenuating circumstances, please open a ticket by contacting our support team.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Billing"
        }, 
        {
            "location": "/faqs/billing/#payment-questions", 
            "text": "", 
            "title": "Payment Questions"
        }, 
        {
            "location": "/faqs/billing/#what-forms-of-payments-do-you-accept", 
            "text": "We accept Visa, MasterCard, American Express and Discover credit and debit cards. We do not accept prepaid cards at the moment.   We are a US based company. If you are outside the US, please ensure your card has \ninternational transactions enabled.", 
            "title": "What forms of payments do you accept?"
        }, 
        {
            "location": "/faqs/billing/#do-you-keep-my-credit-card-information", 
            "text": "No, we do not retain any credit card information. We use Stripe  to process payments.", 
            "title": "Do you keep my credit card information?"
        }, 
        {
            "location": "/faqs/billing/#payment-processing-failed-whay-am-i-unable-to-add-my-payment-method", 
            "text": "When you submit your payment details, you may received a \"Error: Payment processing failed\" error notification.   Some common causes are:   Credit and Debit cards only : We currently accept only Visa, Mastercard and AmEx credit and debit cards. We do not accept prepaid cards. Please ensure you are using a valid credit or debit card.  Balance : We issue a small $0-1 transaction on your card as a pending authorization request between our billing system and the bank that issued your credit or debit card. If this authorization fails, you won't be able to add your card. Please ensure you have enough balance and are using a valid card.  Note : This is an authorization request only, not an actual charge.  Fraud Detection : We use Stripe for managing all our payments. They have their own fraud detection algorithm which can decline some cards. Please try a different payment method.  International Cards : We are a US based company. If you are outside the US please ensure your card has international transactions enabled.   If you are still unable to add your payment, please contact us directly at  .", 
            "title": "Payment processing failed. Whay am I unable to add my payment method?"
        }, 
        {
            "location": "/faqs/billing/#billing-questions", 
            "text": "", 
            "title": "Billing Questions"
        }, 
        {
            "location": "/faqs/billing/#what-am-i-billed-for", 
            "text": "Your usage includes compute (CPU / GPU) usage and storage consumption.     Compute : You will be billed exactly for the duration that your job runs, rounded off to the nearest second.  Note that you are only charged for compute when your job is in the  Running  state. You will  not  be charged when your job is in any other state, including  Queued  and  Shutdown .    Storage : You will also be charged for the storage you consume, rounded off to the nearest kB.   Storage is consumed by the datasets that you upload, your code and the data that your jobs output.", 
            "title": "What am I billed for?"
        }, 
        {
            "location": "/faqs/billing/#when-will-my-card-be-charged", 
            "text": "For subscription plans, your card will be charged every month on the day you upgraded to the plan. For example, if you upgraded from the Free to the Data Scientist Pro plan on the 22 nd  of August, you will be charged immediately. You will be then billed on the 22 nd  of every month.  When you purchase Powerups, either directly from the  Powerups Dashboard  or via auto-refresh, you will be charged at the time of purchase.", 
            "title": "When will my card be charged?"
        }, 
        {
            "location": "/faqs/billing/#can-i-upgrade-or-downgrade-my-plan", 
            "text": "You can upgrade or downgrade your subscriptions at anytime from your  Plans page  under Settings on your dashboard.  When you upgrade, you will be immediately elevated to the new plan. When you downgrade, your new plan will start at the end of your billing cycle (since you will have already paid for the month in advance).  Upgrades and downgrades inside the Data Scientist plans do not affect any Powerups you \nmay have purchased.", 
            "title": "Can I upgrade or downgrade my plan?"
        }, 
        {
            "location": "/faqs/billing/#how-do-upgrades-work", 
            "text": "When you upgrade from the Free plan to a paid plan, you will be charged immediately. You will then be billed on the 1 month anniversary of your subscription date every month. For example, if you upgrade on August 20 th , 2017, you will be charged on that day and on the 20 th  of every subsequent month.  When you upgrade from one paid plan to another, you will be charged for your new plan on a pro-rated basis. Lets say your billing cycle is on the 20 th  of every month. If you upgrade from the Data Scientist Base to the Pro plan on the 5 th , you will be charged for the new plan on a pro-rated basis (5 th  to 20 th ).", 
            "title": "How do upgrades work?"
        }, 
        {
            "location": "/faqs/billing/#how-do-downgrades-work", 
            "text": "You can downgrade at any time to a lower or Free plan.   If you are over the usage limits for the plan you are downgrading to, you will have to handle that first. For example, if you are downgrading from the Data Scientist Pro to Base plan, but have 300GB data, you will have to delete some of your data before downgrading since the Base plan only offers 100 GB.  If you downgrade to the Free plan, you will no longer have access to any Powerups that you may have purchased.", 
            "title": "How do downgrades work?"
        }, 
        {
            "location": "/faqs/billing/#how-do-i-remove-my-credit-card", 
            "text": "Please downgrade to the Free plan. We will remove your credit card at the end of your billing cycle.", 
            "title": "How do I remove my credit card?"
        }, 
        {
            "location": "/faqs/billing/#do-my-remaining-compute-credits-roll-over-each-month-if-i-dont-use-them-all", 
            "text": "No, your monthly Plan compute credits are not rolled over. However, your Powerup credits will remain valid for one year from purchase date.", 
            "title": "Do my remaining compute credits roll over each month if I don't use them all?"
        }, 
        {
            "location": "/faqs/billing/#do-you-offer-refunds", 
            "text": "No, we do not offer refunds. If there are extenuating circumstances, please open a ticket by contacting our support team.", 
            "title": "Do you offer refunds?"
        }, 
        {
            "location": "/faqs/billing/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }
    ]
}