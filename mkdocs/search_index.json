{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nWelcome to \nFloydHub\n! Here you'll find comprehensive information for training and deploying your deep learning and AI applications with our platform. We do our best to make this documentation clear and user friendly, but if you have unanswered questions, please visit the \ncommunity forum\n or \ncontact us\n.\n\n\nThe fastest way to get up and running is to use our \nquickstart guide\n, which walks through an entire FloydHub training job step-by-step. You'll create a new Project on the FloydHub web dashboard, connect it to a local directory on your computer, and then kick-off a job using the FloydHub CLI to train your deep learning model on FloydHub's GPU servers.\n\n\nDeep learning without the DevOps:\n\n\nFrictionless data science\n\n\nWhy worry about provisioning GPUs, installing drivers, or managing software dependency hell? With FloydHub, we take care of your entire deep learning DevOps workflow - so you can focus on the science.\n\n\nTraining a TensorFlow model using GPUs on the cloud is as simple as executing this command on your terminal: \n\nfloyd run --gpu --env tensorflow \npython train.py\n. Try it now with our \nquickstart guide\n.\n\n\nPowerful workflow tools\n\n\nWhether you're using our web dashboard or our command line interface, our tools make your work easier and your team more productive:\n\n\n\n\nInteractive Jupyter Notebook support\n\n\nEnd-to-end version control for data science\n\n\nFull reproducibility of jobs\n\n\nDeploy models as REST endpoints to integrate with your apps\n\n\n\n\nDeep learning community\n\n\nFloydHub hosts open source Projects and Datasets that you can discover, clone, and reproduce (or reconfigure with your own Dataset).\n\n\nTry it now with the \nNeural Style Transfer\n.\n\n\n\n\n\n\n\n\nWe're here to help!\n\n\nWe're always happy to help with any questions you might have! \nSearch\n our documentation or check answers to \nfrequently asked questions\n. The \nFloydHub community forum\n is another place to ask questions, request features, or share cool Projects. For more help, \nsend us an email\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#introduction", 
            "text": "Welcome to  FloydHub ! Here you'll find comprehensive information for training and deploying your deep learning and AI applications with our platform. We do our best to make this documentation clear and user friendly, but if you have unanswered questions, please visit the  community forum  or  contact us .  The fastest way to get up and running is to use our  quickstart guide , which walks through an entire FloydHub training job step-by-step. You'll create a new Project on the FloydHub web dashboard, connect it to a local directory on your computer, and then kick-off a job using the FloydHub CLI to train your deep learning model on FloydHub's GPU servers.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#deep-learning-without-the-devops", 
            "text": "", 
            "title": "Deep learning without the DevOps:"
        }, 
        {
            "location": "/#frictionless-data-science", 
            "text": "Why worry about provisioning GPUs, installing drivers, or managing software dependency hell? With FloydHub, we take care of your entire deep learning DevOps workflow - so you can focus on the science.  Training a TensorFlow model using GPUs on the cloud is as simple as executing this command on your terminal:  floyd run --gpu --env tensorflow  python train.py . Try it now with our  quickstart guide .", 
            "title": "Frictionless data science"
        }, 
        {
            "location": "/#powerful-workflow-tools", 
            "text": "Whether you're using our web dashboard or our command line interface, our tools make your work easier and your team more productive:   Interactive Jupyter Notebook support  End-to-end version control for data science  Full reproducibility of jobs  Deploy models as REST endpoints to integrate with your apps", 
            "title": "Powerful workflow tools"
        }, 
        {
            "location": "/#deep-learning-community", 
            "text": "FloydHub hosts open source Projects and Datasets that you can discover, clone, and reproduce (or reconfigure with your own Dataset).  Try it now with the  Neural Style Transfer .", 
            "title": "Deep learning community"
        }, 
        {
            "location": "/#were-here-to-help", 
            "text": "We're always happy to help with any questions you might have!  Search  our documentation or check answers to  frequently asked questions . The  FloydHub community forum  is another place to ask questions, request features, or share cool Projects. For more help,  send us an email .", 
            "title": "We're here to help!"
        }, 
        {
            "location": "/getstarted/core_concepts/", 
            "text": "Core Concepts\n\n\nThe essential points for understanding and effectively using FloydHub can be\ngrouped into the following categories:\n\n\n\n\nProjects\n\n\nJobs\n\n\nDatasets\n\n\nEnvironments\n\n\nOutput\n\n\n\n\nThis document serves as an introduction to each of these categories. When\nyou're ready to dive deeper, each category has its own section in our\ndocumentation that you can explore.\n\n\nProjects\n\n\n\n\nQuick Look\n\n\nA project is a group of \njobs\n aimed at accomplishing the same goal. A\nproject keeps track of each job, along with its \noutput\n and logs.\n\n\n\n\n\n\nKey Points of Understanding\n\n\n\n\nCreating a project\n\n\n\n\n\n\nThe project is the most central construct of the FloydHub platform.\nUnderstanding what a project is on FloydHub isn't too difficult because it\ndirectly correlates with what you'd think of as a deep learning project outside\nof FloydHub: You have a problem you need to solve with a deep learning model.\nYou get on your computer, create a new directory with a name like \nmnist-cnn\n\nand boom, you've started a new project. In that directory, you'll write some\ncode, run some experiments, and iterate until you have created a deep learning\nmodel that meets your needs.\n\n\nOn FloydHub, a project is a collection of all the work and iterations you\nperform when developing a deep learning model. In contrast to a typical deep\nlearning workflow, your experiments and iterations (we call them\n\njobs\n) will be versioned and kept organized for you to reference in the\nfuture.\n\n\nJobs\n\n\n\n\nQuick Look\n\n\nA job is an execution of your code on FloydHub's deep-learning servers.\n\n\n\n\n\n\nKey Points of Understanding\n\n\n\n\nRunning a job\n\n\n\n\n\n\nA job is what pulls together your \ncode\n and \ndataset(s)\n, sends them to a\ndeep-learning server configured with the right \nenvironment\n, and\nactually kicks off the necessary code to get the data science done.\n\n\nAfter a job is completed, you'll be able to go back and reference/review it on\nfloydhub.com. For each job, you'll be able to see:\n\n\n\n\nA snapshot of the code used for the job\n\n\nA record of which \nDataset(s)\n you used for the job\n\n\nA record of what \nEnvironment\n was used for the job\n\n\nThe \nOutput\n and logs of the job\n\n\n\n\nYou run jobs using Floyd CLI's \nfloyd run\n command. The command has various\nflags and parameters that let you customize how the job runs: What dataset do\nyou want to use? Where should the dataset be mounted on the server? What\nenvironment do you want to use? Should the server run your job using a CPU or a\nGPU? What command(s) should the server use to run your code? Any other commands\nyou'd like to run on the server to set it up before running your job? Etc.\n\n\nDatasets\n\n\n\n\nQuick Look\n\n\nDatasets are securely uploaded to FloydHub. They are versioned and can be\nattached to any job.\n\n\n\n\n\n\nKey Points of Understanding\n\n\n\n\nHow to upload a dataset\n\n\nHow to attach, or \"mount\", a dataset to a job so your code can use it\n\n\nMaking sure your code references your data in the correct location\n\n\n\n\n\n\nFloydHub's dataset workflow is one of two things that tend to feel a bit\nforeign to users who are used to local development (the other being\n\noutput\n). When working on your local machine, you might have your dataset in\nthe same directory as your code, or in a directory where you keep many\ndifferent datasets. When using FloydHub, datasets are always kept separate from\ncode.\n\n\nWhy keep datasets separate from code?\n\n\nAs a data scientist you tweak your code often during the process of creating a\ndeep-learning model. However, you don't change the underlying data nearly\nas often, if at all.\n\n\nEach time you run a job on FloydHub, a copy of your code is uploaded to\nFloydHub and run on one of FloydHub's powerful deep-learning servers. Because\nyour data isn't changing from job to job, it wouldn't make sense to keep your\ndata with your code and upload it with each job. Instead, we upload a dataset\nonce, and attach, or \"mount\", it to each job. This saves a significant amount\nof time on each job.\n\n\nBeyond that, keeping data separate from code allows you to collaborate more\neasily with others. A dataset can be used by any user who has access to it, so\nteams and communities can work on solving problems together using the same\nunderlying data.\n\n\nConnecting code and datasets\n\n\nYou'll still be writing your code locally when using FloydHub, but when you run\na job, your code will be uploaded to FloydHub and executed on a powerful\ndeep-learning server that has your datasets mounted to it.\n\n\nYou can specify the places where your datasets will be\n\nmounted\n on the server, but you'll have to make\nsure that your code references your datasets with the file paths of the data on\nthe \nserver\n, not where you might have them locally.\n\n\nEnvironments\n\n\n\n\nQuick Look\n\n\nAn environment is what defines the software packages that will be available\nto your code during a job\n\n\n\n\n\n\nKey Points of Understanding\n\n\n\n\nHow to select an environment for your job\n\n\nHow to customize your environment\n\n\n\n\n\n\nFloydHub has a bunch of different \ndeep learning environments to choose\nfrom\n. When you run a job on FloydHub, you'll be able\nto specify the environment you want use, straight from the command line. You'll\nalso be able to specify whether you want the job run using a \nGPU or a\nCPU\n.\n\n\nIf FloydHub's stock deep learning environments don't meet your needs, you can\ncreate a custom environment for your job. See \nthis\nguide\n for instructions on that.\n\n\nOutput\n\n\n\n\nQuick Look\n\n\nOutput is any data, logs, or files from a job you want to save for future\nreference and use.\n\n\n\n\n\n\nKey Points of Understanding\n\n\n\n\nHow to capture/save output from a job\n\n\nHow to use it again in a future job\n\n\n\n\n\n\nOutput is anything from a job that you want to save for future use. The most\ncommon form of output is model checkpoints (the weights and biases of your\nmodel) that you developed during a job. If you save these checkpoints (or\nanything else you'd like to preserve) during a job, you'll have them to\nreference, download, and reuse in the future.\n\n\nOutput is the way that you can link jobs together: You run a job to test an\nidea you have. If it works, you may want to start where you left off and run\nanother job. If going down that path leads to a dead end, you may want to go\nback to a previous output and start again from there. Knowing how to store\noutput is key to optimizing your deep learning workflow.\n\n\nSaving\n and \nreusing\n\noutput on FloydHub can feel foriegn to users who are used to working on their\nown machines. But once you learn how to do it, it becomes very simple and is\none of the most valuable parts of the FloydHub workflow.", 
            "title": "Core Concepts"
        }, 
        {
            "location": "/getstarted/core_concepts/#core-concepts", 
            "text": "The essential points for understanding and effectively using FloydHub can be\ngrouped into the following categories:   Projects  Jobs  Datasets  Environments  Output   This document serves as an introduction to each of these categories. When\nyou're ready to dive deeper, each category has its own section in our\ndocumentation that you can explore.", 
            "title": "Core Concepts"
        }, 
        {
            "location": "/getstarted/core_concepts/#projects", 
            "text": "Quick Look  A project is a group of  jobs  aimed at accomplishing the same goal. A\nproject keeps track of each job, along with its  output  and logs.    Key Points of Understanding   Creating a project    The project is the most central construct of the FloydHub platform.\nUnderstanding what a project is on FloydHub isn't too difficult because it\ndirectly correlates with what you'd think of as a deep learning project outside\nof FloydHub: You have a problem you need to solve with a deep learning model.\nYou get on your computer, create a new directory with a name like  mnist-cnn \nand boom, you've started a new project. In that directory, you'll write some\ncode, run some experiments, and iterate until you have created a deep learning\nmodel that meets your needs.  On FloydHub, a project is a collection of all the work and iterations you\nperform when developing a deep learning model. In contrast to a typical deep\nlearning workflow, your experiments and iterations (we call them jobs ) will be versioned and kept organized for you to reference in the\nfuture.", 
            "title": "Projects"
        }, 
        {
            "location": "/getstarted/core_concepts/#jobs", 
            "text": "Quick Look  A job is an execution of your code on FloydHub's deep-learning servers.    Key Points of Understanding   Running a job    A job is what pulls together your  code  and  dataset(s) , sends them to a\ndeep-learning server configured with the right  environment , and\nactually kicks off the necessary code to get the data science done.  After a job is completed, you'll be able to go back and reference/review it on\nfloydhub.com. For each job, you'll be able to see:   A snapshot of the code used for the job  A record of which  Dataset(s)  you used for the job  A record of what  Environment  was used for the job  The  Output  and logs of the job   You run jobs using Floyd CLI's  floyd run  command. The command has various\nflags and parameters that let you customize how the job runs: What dataset do\nyou want to use? Where should the dataset be mounted on the server? What\nenvironment do you want to use? Should the server run your job using a CPU or a\nGPU? What command(s) should the server use to run your code? Any other commands\nyou'd like to run on the server to set it up before running your job? Etc.", 
            "title": "Jobs"
        }, 
        {
            "location": "/getstarted/core_concepts/#datasets", 
            "text": "Quick Look  Datasets are securely uploaded to FloydHub. They are versioned and can be\nattached to any job.    Key Points of Understanding   How to upload a dataset  How to attach, or \"mount\", a dataset to a job so your code can use it  Making sure your code references your data in the correct location    FloydHub's dataset workflow is one of two things that tend to feel a bit\nforeign to users who are used to local development (the other being output ). When working on your local machine, you might have your dataset in\nthe same directory as your code, or in a directory where you keep many\ndifferent datasets. When using FloydHub, datasets are always kept separate from\ncode.", 
            "title": "Datasets"
        }, 
        {
            "location": "/getstarted/core_concepts/#why-keep-datasets-separate-from-code", 
            "text": "As a data scientist you tweak your code often during the process of creating a\ndeep-learning model. However, you don't change the underlying data nearly\nas often, if at all.  Each time you run a job on FloydHub, a copy of your code is uploaded to\nFloydHub and run on one of FloydHub's powerful deep-learning servers. Because\nyour data isn't changing from job to job, it wouldn't make sense to keep your\ndata with your code and upload it with each job. Instead, we upload a dataset\nonce, and attach, or \"mount\", it to each job. This saves a significant amount\nof time on each job.  Beyond that, keeping data separate from code allows you to collaborate more\neasily with others. A dataset can be used by any user who has access to it, so\nteams and communities can work on solving problems together using the same\nunderlying data.", 
            "title": "Why keep datasets separate from code?"
        }, 
        {
            "location": "/getstarted/core_concepts/#connecting-code-and-datasets", 
            "text": "You'll still be writing your code locally when using FloydHub, but when you run\na job, your code will be uploaded to FloydHub and executed on a powerful\ndeep-learning server that has your datasets mounted to it.  You can specify the places where your datasets will be mounted  on the server, but you'll have to make\nsure that your code references your datasets with the file paths of the data on\nthe  server , not where you might have them locally.", 
            "title": "Connecting code and datasets"
        }, 
        {
            "location": "/getstarted/core_concepts/#environments", 
            "text": "Quick Look  An environment is what defines the software packages that will be available\nto your code during a job    Key Points of Understanding   How to select an environment for your job  How to customize your environment    FloydHub has a bunch of different  deep learning environments to choose\nfrom . When you run a job on FloydHub, you'll be able\nto specify the environment you want use, straight from the command line. You'll\nalso be able to specify whether you want the job run using a  GPU or a\nCPU .  If FloydHub's stock deep learning environments don't meet your needs, you can\ncreate a custom environment for your job. See  this\nguide  for instructions on that.", 
            "title": "Environments"
        }, 
        {
            "location": "/getstarted/core_concepts/#output", 
            "text": "Quick Look  Output is any data, logs, or files from a job you want to save for future\nreference and use.    Key Points of Understanding   How to capture/save output from a job  How to use it again in a future job    Output is anything from a job that you want to save for future use. The most\ncommon form of output is model checkpoints (the weights and biases of your\nmodel) that you developed during a job. If you save these checkpoints (or\nanything else you'd like to preserve) during a job, you'll have them to\nreference, download, and reuse in the future.  Output is the way that you can link jobs together: You run a job to test an\nidea you have. If it works, you may want to start where you left off and run\nanother job. If going down that path leads to a dead end, you may want to go\nback to a previous output and start again from there. Knowing how to store\noutput is key to optimizing your deep learning workflow.  Saving  and  reusing \noutput on FloydHub can feel foriegn to users who are used to working on their\nown machines. But once you learn how to do it, it becomes very simple and is\none of the most valuable parts of the FloydHub workflow.", 
            "title": "Output"
        }, 
        {
            "location": "/getstarted/quick_start/", 
            "text": "Let's train our first deep learning model on Floydhub! Follow this guide for a\nquick look at how easy it is to use FloydHub.\n\n\nQuick Preparation Checklist\n\n\n\n\nCreate a FloydHub account\n\n\nInstall \nfloyd-cli\n on your computer\n\n\nLog in to FloydHub through \nfloyd-cli\n\n\n\n\nQuick Start\n\n\n\n\n\n\nVisit \nhttps://www.floydhub.com/projects/create\n and create a FloydHub\n    project. We'll call it \nmnist-cnn\n:\n    \n\n\n\n\n\n\nIn your terminal, use \ngit\n to clone FloydHub's\n    \nquick-start repository\n. It\n    contains the code we'll use to train our CNN. Clone it and switch\n    directories into it:\n\n\n$ git clone https://github.com/floydhub/quick-start.git\nCloning into \nquick-start\n...\n...\n$ \ncd\n quick-start\n$ ls\neval.py  LICENSE  mnist_cnn.ipynb  README.md  train_and_eval.py  train.py\n\n\n\n\n\n\n\n\nIn your terminal, use Floyd CLI to initialize the project (be sure to use\n   the name you gave the project in step one):\n\n\n$ floyd init mnist-cnn\n\nProject \nmnist-cnn\n initialized in current directory\n\n\n\n\n\n\n\n\nThen, to run the training script on one of  FloydHub's deep-learning GPU\n    servers, we'll use the following command:\n\n\n$ floyd run --gpu --env tensorflow-1.3 \npython train_and_eval.py\n\n\nCreating project run. Total upload size: \n25\n.4KiB\nSyncing code ...\n\n[================================]\n \n27316\n/27316 - \n00\n:00:00\n\nJOB NAME\n----------------------\nmckay/projects/mnist-cnn/1\n\nTo view logs enter:\n   floyd logs mckay/projects/mnist-cnn/1\n\n\n\n\nHere's what Floyd did when you ran that command:\n\n\n\n\nSynced your local code to FloydHub's servers\n\n\nProvisioned a GPU instance on the cloud with TensorFlow 1.3 installed\n\n\nExecuted the command \npython train_and_eval.py\n on the GPU server\n\n\nStored the output logs and generated output data\n\n\nTerminated the GPU instance once the command finished executing\n\n\n\n\n\n\n\n\nView your job's logs in real time using the \nfloyd logs -t\n command:\n\n\n$ floyd logs -t\n\n2017\n-09-27 \n14\n:14:40,364 INFO - Starting attempt \n1\n at \n2017\n-09-27 \n14\n:14:40.358414\n\n2017\n-09-27 \n14\n:14:40,399 INFO - Downloading and setting up data sources\n\n2017\n-09-27 \n14\n:14:40,534 INFO - Pulling Docker image: floydhub/tensorflow:1.3.0-py3_aws.12\n\n2017\n-09-27 \n14\n:14:41,977 INFO - Starting container...\n\n2017\n-09-27 \n14\n:14:42,489 INFO -\n\n################################################################################\n\n\n2017\n-09-27 \n14\n:14:42,489 INFO - Run Output:\n...\n\n\n\n\n\n\n\n\nCongratulations! You've trained and tested your first model on FloydHub \ud83c\udf89\n\n\nYeah, that was pretty high level. To go a bit more in depth and learn more\nabout what you actually just did, check out the \nGetting Started\nTutorial\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/getstarted/quick_start/#quick-preparation-checklist", 
            "text": "Create a FloydHub account  Install  floyd-cli  on your computer  Log in to FloydHub through  floyd-cli", 
            "title": "Quick Preparation Checklist"
        }, 
        {
            "location": "/getstarted/quick_start/#quick-start", 
            "text": "Visit  https://www.floydhub.com/projects/create  and create a FloydHub\n    project. We'll call it  mnist-cnn :\n        In your terminal, use  git  to clone FloydHub's\n     quick-start repository . It\n    contains the code we'll use to train our CNN. Clone it and switch\n    directories into it:  $ git clone https://github.com/floydhub/quick-start.git\nCloning into  quick-start ...\n...\n$  cd  quick-start\n$ ls\neval.py  LICENSE  mnist_cnn.ipynb  README.md  train_and_eval.py  train.py    In your terminal, use Floyd CLI to initialize the project (be sure to use\n   the name you gave the project in step one):  $ floyd init mnist-cnn\n\nProject  mnist-cnn  initialized in current directory    Then, to run the training script on one of  FloydHub's deep-learning GPU\n    servers, we'll use the following command:  $ floyd run --gpu --env tensorflow-1.3  python train_and_eval.py \n\nCreating project run. Total upload size:  25 .4KiB\nSyncing code ... [================================]   27316 /27316 -  00 :00:00\n\nJOB NAME\n----------------------\nmckay/projects/mnist-cnn/1\n\nTo view logs enter:\n   floyd logs mckay/projects/mnist-cnn/1  Here's what Floyd did when you ran that command:   Synced your local code to FloydHub's servers  Provisioned a GPU instance on the cloud with TensorFlow 1.3 installed  Executed the command  python train_and_eval.py  on the GPU server  Stored the output logs and generated output data  Terminated the GPU instance once the command finished executing     View your job's logs in real time using the  floyd logs -t  command:  $ floyd logs -t 2017 -09-27  14 :14:40,364 INFO - Starting attempt  1  at  2017 -09-27  14 :14:40.358414 2017 -09-27  14 :14:40,399 INFO - Downloading and setting up data sources 2017 -09-27  14 :14:40,534 INFO - Pulling Docker image: floydhub/tensorflow:1.3.0-py3_aws.12 2017 -09-27  14 :14:41,977 INFO - Starting container... 2017 -09-27  14 :14:42,489 INFO - ################################################################################  2017 -09-27  14 :14:42,489 INFO - Run Output:\n...    Congratulations! You've trained and tested your first model on FloydHub \ud83c\udf89  Yeah, that was pretty high level. To go a bit more in depth and learn more\nabout what you actually just did, check out the  Getting Started\nTutorial", 
            "title": "Quick Start"
        }, 
        {
            "location": "/getstarted/quick_start/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/", 
            "text": "Follow this guide to learn how to spin up a Jupyter Notebook on FloydHub's deep-learning servers.\n\n\nQuick Preparation Checklist\n\n\n\n\nYou must have a \nFloydHub account\n\n\nYou must have \nfloyd-cli\n \ninstalled on your computer\n\n\nYou must \nlog in to FloydHub through the CLI\n\n\n\n\nQuick Start\n\n\n\n\n\n\nVisit \nhttps://www.floydhub.com/projects/create\n and create a FloydHub\n   project:\n   \n\n\n\n\n\n\nIn your terminal, use Floyd CLI to initialize the project (be sure to use\n   the name you gave the project in step one):\n\n\n$ floyd init my_jupyter_project\n\nProject \nmy_jupyter_project\n initialized in current directory\n\n\n\n\n\n\n\n\nThen, kick off your first Jupyter Notebook with\n    \nfloyd run --gpu --mode jupyter\n\n\n$ floyd run --mode jupyter\n\nCreating project run. Total upload size: \n198\n.0B\nSyncing code ...\n\n[================================]\n \n946\n/946 - \n00\n:00:00\n\nJOB NAME\n-----------------------------------\nmckay/projects/my_jupyter_project/1\n\nSetting up your instance and waiting \nfor\n Jupyter notebook to become available .............\n\nPath to jupyter notebook: https://floydlabs.com/notebooks/gaftzXTdaPtQtQ9NvEieNg\n\n\n\n\nThis will open up a Jupyter Notebook in your browser. The notebook is\nrunning on FloyHub's GPU servers. Just like that, you're up and running!\n\n\n\n\n\n\nCongratulations! You've just started your first Jupyter Notebook on FloydHub \ud83c\udf89\n\n\nTo go a bit more in depth and learn more about using Jupyter Notebooks on FloydHub, check out the \nGetting Started Tutorial - Jupyter Notebook", 
            "title": "Quick Start - Jupyter Notebook"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#quick-preparation-checklist", 
            "text": "You must have a  FloydHub account  You must have  floyd-cli   installed on your computer  You must  log in to FloydHub through the CLI", 
            "title": "Quick Preparation Checklist"
        }, 
        {
            "location": "/getstarted/quick_start_jupyter/#quick-start", 
            "text": "Visit  https://www.floydhub.com/projects/create  and create a FloydHub\n   project:\n       In your terminal, use Floyd CLI to initialize the project (be sure to use\n   the name you gave the project in step one):  $ floyd init my_jupyter_project\n\nProject  my_jupyter_project  initialized in current directory    Then, kick off your first Jupyter Notebook with\n     floyd run --gpu --mode jupyter  $ floyd run --mode jupyter\n\nCreating project run. Total upload size:  198 .0B\nSyncing code ... [================================]   946 /946 -  00 :00:00\n\nJOB NAME\n-----------------------------------\nmckay/projects/my_jupyter_project/1\n\nSetting up your instance and waiting  for  Jupyter notebook to become available .............\n\nPath to jupyter notebook: https://floydlabs.com/notebooks/gaftzXTdaPtQtQ9NvEieNg  This will open up a Jupyter Notebook in your browser. The notebook is\nrunning on FloyHub's GPU servers. Just like that, you're up and running!    Congratulations! You've just started your first Jupyter Notebook on FloydHub \ud83c\udf89  To go a bit more in depth and learn more about using Jupyter Notebooks on FloydHub, check out the  Getting Started Tutorial - Jupyter Notebook", 
            "title": "Quick Start"
        }, 
        {
            "location": "/getstarted/get_started/", 
            "text": "Introduction\n\n\nWith this tutorial, you'll get acquainted with FloydHub and learn the\nbasics necessary to get up and running on the platform.\n\n\nWe'll start with an overview of FloydHub and then jump into training your first\ndeep learning model on FloydHub using TensorFlow and the MNIST dataset (better\nknown as the \"Hello, world!\" of data science). We'll be training a\nconvolutional neural network (CNN) model to recognize hand-written digits using\nFloydHub's GPU servers. For more details on the data and the model, please\nrefer to the \nTensorFlow\ndocumentation\n.\n\n\nQuick Preparation Checklist\n\n\n\n\nCreate a FloydHub account\n\n\nInstall \nfloyd-cli\n on your computer\n\n\nLog in to FloydHub through \nfloyd-cli\n\n\n\n\nCreate a New Project\n\n\nFirst, we'll need to \ncreate a project\n. All the\n\njobs\n we run while training our model will\nbe grouped under this project, and we'll be able to go back and review each job\nlater. For more information about projects, check out our \nCore Concepts\npage\n.\n\n\nTo create the project, visit \nwww.floydhub.com/projects\n, and click the\n\nNew Project\n button in the top right corner of the screen. Complete the form\nto create a new project. (Note that private projects are only available on our\n\nData Scientist Plans\n.)\n\n\nIf you're a new user, then you should already see a default project named\n\nquick-start\n in your \nprojects dashboard\n. If you don't see it, go ahead and create a project named 'quick-start'.\n\n\nWhen you visit \nwww.floydhub.com/projects\n you should see your quick-start\nproject, as shown below:\n\n\n\n\nInitialize the Project on Your Machine\n\n\nNow that we've created the project on FloydHub, we can use Floyd CLI to start\ngetting some deep learning done. We'll start by initializing our quick-start\nproject on our local machine.\n\n\nClone the quick-start Repo from GitHub\n\n\nIf you were writing code from scratch, you would create a new directory on your\ncomputer and initialize the FloydHub project in there. In this quick start,\nwe'll clone an existing GitHub repository and use its code. Clone the\n\nquick-start repository\n from GitHub\nonto your computer, and change directories into it:\n\n\n$ git clone https://github.com/floydhub/quick-start.git\nCloning into \nquick-start\n...\n...\n$ \ncd\n quick-start\n$ ls\neval.py  LICENSE  mnist_cnn.ipynb  README.md  train_and_eval.py  train.py\n\n\n\n\nThis repository contains a file we'll use to get started: \ntrain.py\n.\nIt's a Python script that trains a convolutional neural network model against\nthe MNIST dataset. Feel free to look through the file if you'd like, but you\ndon't need to.\n\n\nInitialize the Project Locally\n\n\nTo \"initialize\" a FloydHub project on your machine\nmeans to run the \nfloyd init \nname_of_project\n command in your project's\ndirectory. This will create some files in the directory that Floyd CLI uses to\nkeep track of your jobs and sync with floydhub.com.\n\n\nLet's initialize our project inside of the directory we just cloned from\nGitHub:\n\n\n$ floyd init quick-start\nProject \nquick-start\n initialized in the current directory\n\n\nSuccess!\n\n\nGet the Dataset\n\n\nTo run our deep-learning script, we'll need to give it access to the MNIST\ndataset. You probably know that the MNIST dataset is actually available within\nthe TensorFlow package itself, but for the purposes of this tutorial we have\nseparated out the dataset so you can get a feel for what it's like to work\nwith datasets on FloydHub. We have the MNIST dataset pulicly available on\nFloydHub \nhere\n.\n\n\n\n\nNote\n\n\nOn FloydHub, datasets are kept separate from your project/code. This\napproach serves two main purposes:\n\n\n\n\nUpload data only once\n: Datasets are usually big, so we don't want to\n  have to upload them each time we run our code.\n\n\nEnable collaboration\n: When datasets are kept separate from code, team\n  members and communities can more easily work on projects together.\n\n\n\n\nIf you'd like more information on keeping data separate from code, check out\n\nthis section\n\nof our Core Concepts page.\n\n\n\n\nWhen you start working on your own projects, you'll eventually want to upload\nyour own dataset. Check out\n\nthis article\n to learn how to do that.\n\n\nRunning Your First Job\n\n\nNow that we have our project created and our dataset ready, let's run our first\njob and train our model! A \njob\n is an\nexecution of your code on FloydHub's deep-learning servers. To kick off a job,\nwe use the \nfloyd run\n command.\n\n\nRun the command below to kick off the training job. (Don't worry, we'll explain\neach piece of the command.)\n\n\n$ floyd run \n\\\n\n--gpu \n\\\n\n--data mckay/datasets/mnist/1:/mnist \n\\\n\n--env tensorflow-1.3 \n\\\n\n\npython train.py\n\n\nCreating project run. Total upload size: \n25\n.4KiB\nSyncing code ...\n\n[================================]\n \n27316\n/27316 - \n00\n:00:00\n\nJOB NAME\n----------------------\nmckay/projects/quick-start/1\n\nTo view logs enter:\n   floyd logs mckay/projects/quick-start/1\n\n\n\n\nCongratulations! Your first job is now running on FloydHub's GPU servers.\nBehind the scenes, FloydHub does the following:\n\n\n\n\nSyncs your local code to FloydHub's servers\n\n\nProvisions a GPU instance on the cloud (because you set the \n--gpu\n flag)\n\n\nSets up a deep learning environment with GPU drivers and TensorFlow 1.3\n  installed (because you set the enviroment flag to \n--env tensorflow-1.3\n)\n\n\nExecutes the command \npython train.py\n inside this environment\n\n\nStores the output logs and generated output data\n\n\nTerminates the GPU instance once the command finishes execution\n\n\n\n\nHere is quick explanation of each part of the command you just ran:\n\n\nfloyd run\n\n\nTells the CLI we want to run a job.  You'll use this command to run all of your\njobs. \nMore info here\n.\n\n\n--gpu\n\n\nSpecifies that we want our job run on a GPU server. More info\non instance types \nhere\n.\n\n\n--data mckay/datasets/mnist/1:/mnist\n\n\nThe \ntrain.py\n script expects the \nMNIST\ndata\n to be located at\n\n/mnist\n on the computer where the script runs. We can use the \n--data\n flag to\nensure that our dataset is available to our code at \n/mnist\n.\n\n\nWe pass the \n--data\n flag the name of our dataset (\nmckay/datasets/mnist/1\n)\nand the location on the server where we want our dataset to be available\n(\n/mnist\n), separated by a colon (\n:\n).\n\n\nPutting that all together, we get:\n\n\n--data mckay/datasets/mnist/1:/mnist\n\n\n\n\nWe have an entire article in our docs about mounting datasets to your jobs.\nWhen you're ready to dive in, take a read through it \nhere\n\n\n--env tensorflow-1.3\n\n\nEnsures our job is run on a server that has TensorFlow 1.3 installed. More info\non job environments \nhere\n\n\n\"python train.py\"\n\n\nThe command we want the server to execute to kick off our code. More info\non how to specify commands \nhere\n.\n\n\nMonitoring Your Job\n\n\nYou can view the status of your job from your terminal using the\n\nfloyd status\n command. You can specify a single job\nname (e.g. \nfloyd status alice/quick-start/1\n) to get its status, or the\n\nfloyd-cli\n will show the status of all jobs in the current project.\n\n\n$ floyd status\nJOB NAME             CREATED        STATUS    DURATION\n(\ns\n)\n  INSTANCE    DESCRIPTION\n-------------------  ---------      --------  -----------  ---------   -----------\nalice/quick-start:1  just now       running            \n15\n  gpu\n\n\n\n\nYou can also view the status of your job in your browser by visiting the \nJobURL\n printed by the \nfloyd run\n command. For example,\n\nhttps://www.floydhub.com/alice/quick-start/1\n. The page should look something\nlike this:\n\n\n\n\nViewing Your Job's Logs\n\n\nIt's easy to view the logs generated by the job from your terminal with the\n\nfloyd logs\n command. If you don't specify the job name, the most recent job of your current project is used.\n\n\n$ floyd logs -t\n...\n\n2017\n-07-12 \n16\n:00:07,446 INFO - Starting attempt \n1\n at \n2017\n-07-12 \n16\n:00:07.436349\n\n2017\n-07-12 \n16\n:00:09,088 INFO - Starting container...\n\n2017\n-07-12 \n16\n:00:09,297 INFO -\n...\n\n##############################################################################\n\n\n2017\n-07-12 \n16\n:00:09,297 INFO - Run Output:\n\n2017\n-07-12 \n16\n:01:46,154 INFO - Successfully downloaded train-images-idx3-ubyte.gz \n9912422\n bytes.\n\n2017\n-07-12 \n16\n:01:46,158 INFO - Iter \n1280\n, Minibatch \nLoss\n=\n \n39855\n.289062, Training \nAccuracy\n=\n \n0\n.17969\n\n2017\n-07-12 \n16\n:01:46,159 INFO - Iter \n2560\n, Minibatch \nLoss\n=\n \n14964\n.132812, Training \nAccuracy\n=\n \n0\n.42969\n...\n\n##############################################################################\n\n...\n\n\n\n\nThe output of your code is printed in the \nRun Output\n section of the logs,\nbetween the \n#########\n lines. Anything you log or print in your code will\nappear here, so this is a great way to monitor the progress of your model\ntraining command. In our \nquick-start\n project, we're logging the training\naccuracy of our model.\n\n\nUsing the \n-t\n (tail) flag will stream the logs as they are generated.\n\n\nYou can also view the logs in your browser using your \nJob URL\n. (The \nJob URL\n\nwill look something like\n\nhttps://www.floydhub.com/\nusername\n/projects/quick-start/\njob_number\n.)\n\n\nStoring Your Model for Future Use\n\n\nWe want to save the model we trained so we can use it later, maybe to iterate\non it or to check its accuracy using an evaluation script.\n\n\nTo save something during our job that we want to save for later, we just need\nto make sure it gets saved at \n/output\n during our job. Anything in \n/output\n\nat the end of a job will be saved for us and we can reuse it later.  Take a\nlook at \nline\n108\n of our\n\ntrain.py\n script. Here's the line:\n\n\nbuilder = tf.saved_model.builder.SavedModelBuilder(\n/output/cnn_model\n)\n\n\nWe're setting up our model to be saved under \n/output\n. This ensures that\nFloydHub will save it for us to use later.\n\n\nFor more details on how to save and reuse job output, see\n\nthis article\n.\n\n\nNow let's evaluate our model by checking it against our evaluation script.\n\n\nEvaluate Your Model\n\n\nTo finish off this tutorial, we'll evaluate the model we trained in our first\njob. The repository we cloned early has a script we can use to do this:\n\neval.py\n.\n\n\nThe script expects our model to be located at \n/model\n on the machine where the\nscript runs. Somehow we've got to make sure that the model we saved in our\nfirst job ends up at that location during our second job. FloydHub allows us to\nreuse a job's output in another job, and to specify the place the data will be\nlocated during the second job. The method to accomplish this is the same one we\nused to mount our dataset to our first job. We'll demonstrate how to mount our\nmodel in the next section.\n\n\nHow to Reuse the Output of Your Previous Job\n\n\nOutput from previous jobs can be attached to a new job using the same approach\nas mounting a dataset. We just use the name of the output instead of the name\nof a dataset when we use the \n--data\n flag:\n\n\n--data mckay/projects/quick-start/1/output:/model\n\n\n\n\nNotice that we specify \n/model\n as the mountpoint because we know our\nevaluation script expects our model to be at \n/model\n.\n\n\nFor more information on reusing output, check out\n\nthis article\n\n\nRun Your Second Job\n\n\nFollow this command to run your second job. Note that we are mounting our\ndataset again at \n/mnist\n and also mounting our model at \n/model\n. Be sure to\nreplace \nmckay/projects/quick-start/1/output\n with the name of the output you\nwant to mount (something like \nusername\n/projects/quick-start/\nrun_number\n/output\n)\n\n\n$\n \nfloyd\n \nrun\n \n\\\n\n\n--\ngpu\n \n\\\n\n\n--\nenv\n \ntensorflow-1\n.\n3\n \n\\\n\n\n--\ndata\n \nmckay/datasets/mnist/1\n:\n/\nmnist\n \n\\\n\n\n--\ndata\n \nmckay/projects/quick-start/1/output\n:\n/\nmodel\n \n\\\n\n\npython eval.py\n\n\n\nCreating\n \nproject\n \nrun\n.\n \nTotal\n \nupload\n \nsize\n:\n \n26.3\nKiB\n\n\nSyncing\n \ncode\n \n...\n\n\n[================================]\n \n28620\n/\n28620\n \n-\n \n00\n:\n00\n:\n01\n\n\n\nJOB\n \nNAME\n\n\n---------------------------\n\n\nmckay/projects/quick-start/2\n\n\n\nTo\n \nview\n \nlogs\n \nenter\n:\n\n\n   \nfloyd\n \nlogs\n \nmckay/projects/quick-start/2\n\n\n\n\n\nYou know how to check the logs, so go ahead and check the logs of your second\njob and see how accurate your model is!\n\n\nIf you forgot how to check the logs, take a look at \nthis\nsection\n of the page.\n\n\nIterating on Your Model\n\n\nCongratulations! You've trained and tested your first model on FloydHub \ud83c\udf89\n\n\nAt this point, you can edit your Python code locally to make improvements or\nadjustments to your training script, and then kick off a new job with the\n\nfloyd run\n command. The \nfloyd-cli\n will upload the newest\nversions of your code and submit another job to the FloydHub servers. Along the\nway, FloydHub will be managing and tracking of all the iterations of jobs\nwithin your project.\n\n\nYou can always view details on all of the jobs in your current project with the\n\nfloyd status\n command from your terminal, or by visiting the \nProject URL\n in\nyour browser.\n\n\nExample: \nwww.floydhub.com/mckay/quick-start\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Getting Started Tutorial"
        }, 
        {
            "location": "/getstarted/get_started/#introduction", 
            "text": "With this tutorial, you'll get acquainted with FloydHub and learn the\nbasics necessary to get up and running on the platform.  We'll start with an overview of FloydHub and then jump into training your first\ndeep learning model on FloydHub using TensorFlow and the MNIST dataset (better\nknown as the \"Hello, world!\" of data science). We'll be training a\nconvolutional neural network (CNN) model to recognize hand-written digits using\nFloydHub's GPU servers. For more details on the data and the model, please\nrefer to the  TensorFlow\ndocumentation .", 
            "title": "Introduction"
        }, 
        {
            "location": "/getstarted/get_started/#quick-preparation-checklist", 
            "text": "Create a FloydHub account  Install  floyd-cli  on your computer  Log in to FloydHub through  floyd-cli", 
            "title": "Quick Preparation Checklist"
        }, 
        {
            "location": "/getstarted/get_started/#create-a-new-project", 
            "text": "First, we'll need to  create a project . All the jobs  we run while training our model will\nbe grouped under this project, and we'll be able to go back and review each job\nlater. For more information about projects, check out our  Core Concepts\npage .  To create the project, visit  www.floydhub.com/projects , and click the New Project  button in the top right corner of the screen. Complete the form\nto create a new project. (Note that private projects are only available on our Data Scientist Plans .)  If you're a new user, then you should already see a default project named quick-start  in your  projects dashboard . If you don't see it, go ahead and create a project named 'quick-start'.  When you visit  www.floydhub.com/projects  you should see your quick-start\nproject, as shown below:", 
            "title": "Create a New Project"
        }, 
        {
            "location": "/getstarted/get_started/#initialize-the-project-on-your-machine", 
            "text": "Now that we've created the project on FloydHub, we can use Floyd CLI to start\ngetting some deep learning done. We'll start by initializing our quick-start\nproject on our local machine.", 
            "title": "Initialize the Project on Your Machine"
        }, 
        {
            "location": "/getstarted/get_started/#clone-the-quick-start-repo-from-github", 
            "text": "If you were writing code from scratch, you would create a new directory on your\ncomputer and initialize the FloydHub project in there. In this quick start,\nwe'll clone an existing GitHub repository and use its code. Clone the quick-start repository  from GitHub\nonto your computer, and change directories into it:  $ git clone https://github.com/floydhub/quick-start.git\nCloning into  quick-start ...\n...\n$  cd  quick-start\n$ ls\neval.py  LICENSE  mnist_cnn.ipynb  README.md  train_and_eval.py  train.py  This repository contains a file we'll use to get started:  train.py .\nIt's a Python script that trains a convolutional neural network model against\nthe MNIST dataset. Feel free to look through the file if you'd like, but you\ndon't need to.", 
            "title": "Clone the quick-start Repo from GitHub"
        }, 
        {
            "location": "/getstarted/get_started/#initialize-the-project-locally", 
            "text": "To \"initialize\" a FloydHub project on your machine\nmeans to run the  floyd init  name_of_project  command in your project's\ndirectory. This will create some files in the directory that Floyd CLI uses to\nkeep track of your jobs and sync with floydhub.com.  Let's initialize our project inside of the directory we just cloned from\nGitHub:  $ floyd init quick-start\nProject  quick-start  initialized in the current directory \nSuccess!", 
            "title": "Initialize the Project Locally"
        }, 
        {
            "location": "/getstarted/get_started/#get-the-dataset", 
            "text": "To run our deep-learning script, we'll need to give it access to the MNIST\ndataset. You probably know that the MNIST dataset is actually available within\nthe TensorFlow package itself, but for the purposes of this tutorial we have\nseparated out the dataset so you can get a feel for what it's like to work\nwith datasets on FloydHub. We have the MNIST dataset pulicly available on\nFloydHub  here .   Note  On FloydHub, datasets are kept separate from your project/code. This\napproach serves two main purposes:   Upload data only once : Datasets are usually big, so we don't want to\n  have to upload them each time we run our code.  Enable collaboration : When datasets are kept separate from code, team\n  members and communities can more easily work on projects together.   If you'd like more information on keeping data separate from code, check out this section \nof our Core Concepts page.   When you start working on your own projects, you'll eventually want to upload\nyour own dataset. Check out this article  to learn how to do that.", 
            "title": "Get the Dataset"
        }, 
        {
            "location": "/getstarted/get_started/#running-your-first-job", 
            "text": "Now that we have our project created and our dataset ready, let's run our first\njob and train our model! A  job  is an\nexecution of your code on FloydHub's deep-learning servers. To kick off a job,\nwe use the  floyd run  command.  Run the command below to kick off the training job. (Don't worry, we'll explain\neach piece of the command.)  $ floyd run  \\ \n--gpu  \\ \n--data mckay/datasets/mnist/1:/mnist  \\ \n--env tensorflow-1.3  \\  python train.py \n\nCreating project run. Total upload size:  25 .4KiB\nSyncing code ... [================================]   27316 /27316 -  00 :00:00\n\nJOB NAME\n----------------------\nmckay/projects/quick-start/1\n\nTo view logs enter:\n   floyd logs mckay/projects/quick-start/1  Congratulations! Your first job is now running on FloydHub's GPU servers.\nBehind the scenes, FloydHub does the following:   Syncs your local code to FloydHub's servers  Provisions a GPU instance on the cloud (because you set the  --gpu  flag)  Sets up a deep learning environment with GPU drivers and TensorFlow 1.3\n  installed (because you set the enviroment flag to  --env tensorflow-1.3 )  Executes the command  python train.py  inside this environment  Stores the output logs and generated output data  Terminates the GPU instance once the command finishes execution   Here is quick explanation of each part of the command you just ran:", 
            "title": "Running Your First Job"
        }, 
        {
            "location": "/getstarted/get_started/#floyd-run", 
            "text": "Tells the CLI we want to run a job.  You'll use this command to run all of your\njobs.  More info here .", 
            "title": "floyd run"
        }, 
        {
            "location": "/getstarted/get_started/#-gpu", 
            "text": "Specifies that we want our job run on a GPU server. More info\non instance types  here .", 
            "title": "--gpu"
        }, 
        {
            "location": "/getstarted/get_started/#-data-mckaydatasetsmnist1mnist", 
            "text": "The  train.py  script expects the  MNIST\ndata  to be located at /mnist  on the computer where the script runs. We can use the  --data  flag to\nensure that our dataset is available to our code at  /mnist .  We pass the  --data  flag the name of our dataset ( mckay/datasets/mnist/1 )\nand the location on the server where we want our dataset to be available\n( /mnist ), separated by a colon ( : ).  Putting that all together, we get:  --data mckay/datasets/mnist/1:/mnist  We have an entire article in our docs about mounting datasets to your jobs.\nWhen you're ready to dive in, take a read through it  here", 
            "title": "--data mckay/datasets/mnist/1:/mnist"
        }, 
        {
            "location": "/getstarted/get_started/#-env-tensorflow-13", 
            "text": "Ensures our job is run on a server that has TensorFlow 1.3 installed. More info\non job environments  here", 
            "title": "--env tensorflow-1.3"
        }, 
        {
            "location": "/getstarted/get_started/#python-trainpy", 
            "text": "The command we want the server to execute to kick off our code. More info\non how to specify commands  here .", 
            "title": "\"python train.py\""
        }, 
        {
            "location": "/getstarted/get_started/#monitoring-your-job", 
            "text": "You can view the status of your job from your terminal using the floyd status  command. You can specify a single job\nname (e.g.  floyd status alice/quick-start/1 ) to get its status, or the floyd-cli  will show the status of all jobs in the current project.  $ floyd status\nJOB NAME             CREATED        STATUS    DURATION ( s )   INSTANCE    DESCRIPTION\n-------------------  ---------      --------  -----------  ---------   -----------\nalice/quick-start:1  just now       running             15   gpu  You can also view the status of your job in your browser by visiting the  JobURL  printed by the  floyd run  command. For example, https://www.floydhub.com/alice/quick-start/1 . The page should look something\nlike this:", 
            "title": "Monitoring Your Job"
        }, 
        {
            "location": "/getstarted/get_started/#viewing-your-jobs-logs", 
            "text": "It's easy to view the logs generated by the job from your terminal with the floyd logs  command. If you don't specify the job name, the most recent job of your current project is used.  $ floyd logs -t\n... 2017 -07-12  16 :00:07,446 INFO - Starting attempt  1  at  2017 -07-12  16 :00:07.436349 2017 -07-12  16 :00:09,088 INFO - Starting container... 2017 -07-12  16 :00:09,297 INFO -\n... ##############################################################################  2017 -07-12  16 :00:09,297 INFO - Run Output: 2017 -07-12  16 :01:46,154 INFO - Successfully downloaded train-images-idx3-ubyte.gz  9912422  bytes. 2017 -07-12  16 :01:46,158 INFO - Iter  1280 , Minibatch  Loss =   39855 .289062, Training  Accuracy =   0 .17969 2017 -07-12  16 :01:46,159 INFO - Iter  2560 , Minibatch  Loss =   14964 .132812, Training  Accuracy =   0 .42969\n... ############################################################################## \n...  The output of your code is printed in the  Run Output  section of the logs,\nbetween the  #########  lines. Anything you log or print in your code will\nappear here, so this is a great way to monitor the progress of your model\ntraining command. In our  quick-start  project, we're logging the training\naccuracy of our model.  Using the  -t  (tail) flag will stream the logs as they are generated.  You can also view the logs in your browser using your  Job URL . (The  Job URL \nwill look something like https://www.floydhub.com/ username /projects/quick-start/ job_number .)", 
            "title": "Viewing Your Job's Logs"
        }, 
        {
            "location": "/getstarted/get_started/#storing-your-model-for-future-use", 
            "text": "We want to save the model we trained so we can use it later, maybe to iterate\non it or to check its accuracy using an evaluation script.  To save something during our job that we want to save for later, we just need\nto make sure it gets saved at  /output  during our job. Anything in  /output \nat the end of a job will be saved for us and we can reuse it later.  Take a\nlook at  line\n108  of our train.py  script. Here's the line:  builder = tf.saved_model.builder.SavedModelBuilder( /output/cnn_model ) \nWe're setting up our model to be saved under  /output . This ensures that\nFloydHub will save it for us to use later.  For more details on how to save and reuse job output, see this article .  Now let's evaluate our model by checking it against our evaluation script.", 
            "title": "Storing Your Model for Future Use"
        }, 
        {
            "location": "/getstarted/get_started/#evaluate-your-model", 
            "text": "To finish off this tutorial, we'll evaluate the model we trained in our first\njob. The repository we cloned early has a script we can use to do this: eval.py .  The script expects our model to be located at  /model  on the machine where the\nscript runs. Somehow we've got to make sure that the model we saved in our\nfirst job ends up at that location during our second job. FloydHub allows us to\nreuse a job's output in another job, and to specify the place the data will be\nlocated during the second job. The method to accomplish this is the same one we\nused to mount our dataset to our first job. We'll demonstrate how to mount our\nmodel in the next section.", 
            "title": "Evaluate Your Model"
        }, 
        {
            "location": "/getstarted/get_started/#how-to-reuse-the-output-of-your-previous-job", 
            "text": "Output from previous jobs can be attached to a new job using the same approach\nas mounting a dataset. We just use the name of the output instead of the name\nof a dataset when we use the  --data  flag:  --data mckay/projects/quick-start/1/output:/model  Notice that we specify  /model  as the mountpoint because we know our\nevaluation script expects our model to be at  /model .  For more information on reusing output, check out this article", 
            "title": "How to Reuse the Output of Your Previous Job"
        }, 
        {
            "location": "/getstarted/get_started/#run-your-second-job", 
            "text": "Follow this command to run your second job. Note that we are mounting our\ndataset again at  /mnist  and also mounting our model at  /model . Be sure to\nreplace  mckay/projects/quick-start/1/output  with the name of the output you\nwant to mount (something like  username /projects/quick-start/ run_number /output )  $   floyd   run   \\  -- gpu   \\  -- env   tensorflow-1 . 3   \\  -- data   mckay/datasets/mnist/1 : / mnist   \\  -- data   mckay/projects/quick-start/1/output : / model   \\  python eval.py  Creating   project   run .   Total   upload   size :   26.3 KiB  Syncing   code   ...  [================================]   28620 / 28620   -   00 : 00 : 01  JOB   NAME  ---------------------------  mckay/projects/quick-start/2  To   view   logs   enter :      floyd   logs   mckay/projects/quick-start/2   You know how to check the logs, so go ahead and check the logs of your second\njob and see how accurate your model is!  If you forgot how to check the logs, take a look at  this\nsection  of the page.", 
            "title": "Run Your Second Job"
        }, 
        {
            "location": "/getstarted/get_started/#iterating-on-your-model", 
            "text": "Congratulations! You've trained and tested your first model on FloydHub \ud83c\udf89  At this point, you can edit your Python code locally to make improvements or\nadjustments to your training script, and then kick off a new job with the floyd run  command. The  floyd-cli  will upload the newest\nversions of your code and submit another job to the FloydHub servers. Along the\nway, FloydHub will be managing and tracking of all the iterations of jobs\nwithin your project.  You can always view details on all of the jobs in your current project with the floyd status  command from your terminal, or by visiting the  Project URL  in\nyour browser.  Example:  www.floydhub.com/mckay/quick-start", 
            "title": "Iterating on Your Model"
        }, 
        {
            "location": "/getstarted/get_started/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/", 
            "text": "In this tutorial, we will run a Python \nJupyter Notebook\n on FloydHub. Notebooks allow you to create and share documents that contain live code, visualizations and explanatory texts. This is an \nexample Notebook\n. It is great for interactively writing and debugging your code and visualizing your results and data.\n\n\nSimilar to the \nQuick Start guide\n, we will train a CNN model for handwritten digit recognition using PyTorch and the MNIST database.\n\n\nIf you are new to FloydHub, please ensure you have followed the \nQuick Start guide\n first. It introduces some important concepts used in this tutorial.\n\n\nWhat we will accomplish in this guide\n\n\n\n\nLearn how to create a new project on FloydHub\n\n\nStart a Jupyter notebook on FloydHub's GPU server\n\n\nInteractively run and debug your code\n\n\nMount datasets to use in your code\n\n\n\n\nQuick preparation checklist\n\n\n\n\nYou must have a \nFloydHub account\n\n\nYou must have \nfloyd-cli\n \ninstalled on your computer\n\n\nYou must \nlog in to FloydHub through the CLI\n\n\n\n\nSetup\n\n\nCreate a new project\n\n\nFor this tutorial, we will create a new Project. This project will be a collection of the jobs you run and their data, logs and results.\n\n\nTo create a new Project, visit \nwww.floydhub.com/projects\n and click on the \"New Project\" button on the top right hand corner.\n\n\n\n\nWe will name this project \nmnist-pytorch\n. Feel free to provide an apt description.\n\n\nThe \nVisibility\n field indicates who can see your project. If you set it to \nPublic\n, anyone can see your project, your code and data. If you are working on an open source project, this is a great way to share and contribute to the FloydHub community. If your code or data is proprietary, please select \nPrivate\n. This will ensure that only you and your team will have access to this project.\n\n\nGet the code\n\n\nWe will clone the \nquick-start repository\n from Github to your local machine and run it on FloydHub. Run the \ngit clone\n command in a brand new directory on your computer:\n\n\n$ git clone https://github.com/floydhub/quick-start-pytorch.git\nCloning into \nquick-start-pytorch\n...\n$ \ncd\n quick-start-pytorch\n\n\n\n\n$ ls\n$ README.md mnist.ipynb\n\n\n\n\nIn this guide, we will use the \nmnist.ipynb\n Jupyter Notebook.\n\n\nInitialize new project\n\n\nNow that we have the code, we want to associate this directory with the new project you just created on FloydHub. Ensure that you are inside the \nquick-start-pytorch\n directory and execute:\n\n\n$ floyd init mnist-pytorch\nProject \nmnist-pytorch\n initialized in the current directory\n\n\n\n\nThis tells Floyd that all the jobs run from this directory belong to the same project.\n\n\nRunning Jupyter Notebook on FloydHub\n\n\nStarting a Jupyter Notebook on FloydHub is very simple. Use the \nfloyd run\n command with \n--mode jupyter\n flag.\n\n\nExecute the following command from the command line:\n\n\n$ floyd run --mode jupyter --gpu --env pytorch-0.2\nCreating project run. Total upload size: \n21\n.9KiB\nSyncing code ...\n\n[================================]\n \n23333\n/23333 - \n00\n:00:00\nNAME\n--------------------\nmckay/projects/mnist-pytorch/2\n\nSetting up your instance and waiting \nfor\n Jupyter notebook to become available ..............\n\nPath to jupyter notebook: https://www.floydhub.com/notebooks/pCoPyzZtYeo6mE9PpSWsmY\n\n\n\n\nThis will take a little bit. As it executes, Floyd is doing the following behind the scenes:\n\n\n\n\nSync your local code to FloydHub's server\n\n\nProvision a GPU instance on the cloud (if you want CPU, drop the \n--gpu\n flag)\n\n\nSet up an deep learning environment with PyTorch installed (because \n--env pytorch\n)\n\n\nStart a Jupyter server on the cloud, and open the url in your browser\n\n\n\n\nYou can also open the link to the your Jupyter dashboard using the displayed URL. For example:\n\n\n\n\nOpen the \nmnist.ipynb\n Notebook and start training your model interactively!\n\n\nNext steps\n\n\nCheck the status of your job\n\n\nYou can view the job's status by going to the job's page in the web dashboard:\n\n\n\n\nAlternatively, you can view the status from your terminal using the\n\nfloyd status\n command:\n\n\n$ floyd status mckay/mnist-pytorch/projects/2\nJOB NAME                          CREATED         STATUS      DURATION\n(\ns\n)\n  INSTANCE    DESCRIPTION\n----------------------            --------------  --------  -------------  ----------  -------------\nmckay/mnist-pytorch/projects/2    \n16\n minutes ago  running               \n0\n  gpu\n\n\n\n\nStopping your Notebook\n\n\nOn the project page, click the \nCancel\n button below the icon that shows the status of your job, as shown in the picture below:\n\n\n\n\nThen click the \nConfirm\n button in the modal that pops up:\n\n\n\n\n\nWarning\n\n\nJupyter Notebooks are designed for interactive development. Your job starts running on FloydHub's server when you execute the \nfloyd run --mode jupyter\n command and it continues to be active till you explicitly stop your job.\n\n\nHence, even if you are not actively executing code inside your Notebook, the Jupyter server is still active on FloydHub and you are billed for the time.\n\n\n\n\nLearn More\n\n\nFor a more in-depth tutorial about FloydHub that uses FloydHub's \"command\nmode\", check out \nthis tutorial\n.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Getting Started Tutorial - Jupyter Notebook"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#what-we-will-accomplish-in-this-guide", 
            "text": "Learn how to create a new project on FloydHub  Start a Jupyter notebook on FloydHub's GPU server  Interactively run and debug your code  Mount datasets to use in your code", 
            "title": "What we will accomplish in this guide"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#quick-preparation-checklist", 
            "text": "You must have a  FloydHub account  You must have  floyd-cli   installed on your computer  You must  log in to FloydHub through the CLI", 
            "title": "Quick preparation checklist"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#setup", 
            "text": "", 
            "title": "Setup"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#create-a-new-project", 
            "text": "For this tutorial, we will create a new Project. This project will be a collection of the jobs you run and their data, logs and results.  To create a new Project, visit  www.floydhub.com/projects  and click on the \"New Project\" button on the top right hand corner.   We will name this project  mnist-pytorch . Feel free to provide an apt description.  The  Visibility  field indicates who can see your project. If you set it to  Public , anyone can see your project, your code and data. If you are working on an open source project, this is a great way to share and contribute to the FloydHub community. If your code or data is proprietary, please select  Private . This will ensure that only you and your team will have access to this project.", 
            "title": "Create a new project"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#get-the-code", 
            "text": "We will clone the  quick-start repository  from Github to your local machine and run it on FloydHub. Run the  git clone  command in a brand new directory on your computer:  $ git clone https://github.com/floydhub/quick-start-pytorch.git\nCloning into  quick-start-pytorch ...\n$  cd  quick-start-pytorch  $ ls\n$ README.md mnist.ipynb  In this guide, we will use the  mnist.ipynb  Jupyter Notebook.", 
            "title": "Get the code"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#initialize-new-project", 
            "text": "Now that we have the code, we want to associate this directory with the new project you just created on FloydHub. Ensure that you are inside the  quick-start-pytorch  directory and execute:  $ floyd init mnist-pytorch\nProject  mnist-pytorch  initialized in the current directory  This tells Floyd that all the jobs run from this directory belong to the same project.", 
            "title": "Initialize new project"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#running-jupyter-notebook-on-floydhub", 
            "text": "Starting a Jupyter Notebook on FloydHub is very simple. Use the  floyd run  command with  --mode jupyter  flag.  Execute the following command from the command line:  $ floyd run --mode jupyter --gpu --env pytorch-0.2\nCreating project run. Total upload size:  21 .9KiB\nSyncing code ... [================================]   23333 /23333 -  00 :00:00\nNAME\n--------------------\nmckay/projects/mnist-pytorch/2\n\nSetting up your instance and waiting  for  Jupyter notebook to become available ..............\n\nPath to jupyter notebook: https://www.floydhub.com/notebooks/pCoPyzZtYeo6mE9PpSWsmY  This will take a little bit. As it executes, Floyd is doing the following behind the scenes:   Sync your local code to FloydHub's server  Provision a GPU instance on the cloud (if you want CPU, drop the  --gpu  flag)  Set up an deep learning environment with PyTorch installed (because  --env pytorch )  Start a Jupyter server on the cloud, and open the url in your browser   You can also open the link to the your Jupyter dashboard using the displayed URL. For example:   Open the  mnist.ipynb  Notebook and start training your model interactively!", 
            "title": "Running Jupyter Notebook on FloydHub"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#next-steps", 
            "text": "", 
            "title": "Next steps"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#check-the-status-of-your-job", 
            "text": "You can view the job's status by going to the job's page in the web dashboard:   Alternatively, you can view the status from your terminal using the floyd status  command:  $ floyd status mckay/mnist-pytorch/projects/2\nJOB NAME                          CREATED         STATUS      DURATION ( s )   INSTANCE    DESCRIPTION\n----------------------            --------------  --------  -------------  ----------  -------------\nmckay/mnist-pytorch/projects/2     16  minutes ago  running                0   gpu", 
            "title": "Check the status of your job"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#stopping-your-notebook", 
            "text": "On the project page, click the  Cancel  button below the icon that shows the status of your job, as shown in the picture below:   Then click the  Confirm  button in the modal that pops up:   Warning  Jupyter Notebooks are designed for interactive development. Your job starts running on FloydHub's server when you execute the  floyd run --mode jupyter  command and it continues to be active till you explicitly stop your job.  Hence, even if you are not actively executing code inside your Notebook, the Jupyter server is still active on FloydHub and you are billed for the time.", 
            "title": "Stopping your Notebook"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#learn-more", 
            "text": "For a more in-depth tutorial about FloydHub that uses FloydHub's \"command\nmode\", check out  this tutorial .", 
            "title": "Learn More"
        }, 
        {
            "location": "/getstarted/get_started_jupyter/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/basics/create_new/", 
            "text": "A \nProject\n is a collection of the jobs you run along with their logs and\nresults. If you have used GitHub, projects in FloydHub are a lot like code\nrepositories.\n\n\nTo create a new Project, visit \nwww.floydhub.com/projects\n and click on the \"New Project\" button on the top right hand corner.\n\n\n\n\nGive the project a name and an apt description.\n\n\nThe \nVisibility\n field indicates who can see your project. If you set it to \nPublic\n, anyone can see your project, your code and data. If you are working on an open source project, this is a great way to share and contribute to the FloydHub community. If your code or data is proprietary, please select \nPrivate\n. This will ensure that only you and your team will have access to this project.\n\n\nOnce you have created a Project, you can start running jobs using the \nfloyd run\n command. For example, to start a Jupyter Notebook job:\n\n\n$ floyd init quick-start\nProject \nquick-start\n initialized in the current directory\n\n$ floyd run --gpu --env tensorflow --mode jupyter\nSyncing code ...\n\n\n\n\nAdding a Project README\n\n\nFloydHub will display a README file for your project, if you include a README file in your local code directory for a Project when you run a job. A good README file will help people on FloydHub understand your project, why it's useful, and how they can run the project.\n\n\nTo add a README to your Project, simply add a Markdown-styled text file to your project called \nREADME.md\n and FloydHub will automatically display that README file when you run your next job.", 
            "title": "Create a New Project"
        }, 
        {
            "location": "/guides/basics/create_new/#adding-a-project-readme", 
            "text": "FloydHub will display a README file for your project, if you include a README file in your local code directory for a Project when you run a job. A good README file will help people on FloydHub understand your project, why it's useful, and how they can run the project.  To add a README to your Project, simply add a Markdown-styled text file to your project called  README.md  and FloydHub will automatically display that README file when you run your next job.", 
            "title": "Adding a Project README"
        }, 
        {
            "location": "/guides/basics/delete/", 
            "text": "You can delete a project by clicking \nDelete project\n button on the Settings tab of the project on the web dashboard.\n\n\nExample: \nhttps://www.floydhub.com/mckay/projects/quick-start/settings\n\n\n\n\n\n\n\n\nImportant\n\n\nDeleting a project will delete all its jobs and their corresponding code,\noutput data and logs. This \ncannot\n be restored. Please be absolutely\nsure you want to delete a project before proceeding.\n\n\n\n\nWe recommend deleting individual jobs rather than projects.", 
            "title": "Delete a Project"
        }, 
        {
            "location": "/guides/run_a_job/", 
            "text": "Quick Look\n\n\nfloyd run [OPTIONS] [COMMAND]\n\n\n[OPTIONS]\n:\n\n\n\n\nInstance Type\n: \n--cpu\n \nor\n \n--gpu\n \nor\n \n--cpu+\n \nor\n \n--gpu+\n\n\nDataset(s)\n: \n--data \nname_of_datasource\n:\nmount_point_on_server\n\n\nMode\n: \n--mode \nmode_name\n\n\nEnvironment\n: \n--env \nenvironment_name\n\n\nMessage\n \n--message\n \nor\n \n-m\n\n\nTensorboard\n: \n--tensorboard\n\n\n\n\n[COMMAND]\n\n\n\n\nRunning jobs is the core action in the FloydHub workflow. A job pulls together\nyour code and dataset(s), sends them to a deep-learning server configured with\nthe right environment, and actually kicks off the necessary code to get the\ndata science done. This article serves as a more in-depth look at the\nins-and-outs of running jobs on FloydHub using the \nfloyd run\n command.\n\n\nThe \nfloyd run\n command can be broken down into the two main parts: the\n\n[OPTIONS]\n, and the \n[COMMAND]\n. We'll detail all the \n[OPTIONS]\n available to\nyou, as well as how to use the \n[COMMAND]\n properly. Use the links below as a\nquick reference.\n\n\nParts of the \nfloyd run\n Command\n\n\n[OPTIONS]\n\n\n\n\nInstance Type\n: \n--cpu\n \nor\n \n--gpu\n \nor\n \n--cpu+\n \nor\n \n--gpu+\n\n\nDataset(s)\n: \n--data\n\n\nMode\n: \n--mode\n\n\nEnvironment\n: \n--env\n\n\nMessage\n \n--message\n \nor\n \n-m\n\n\nTensorboard\n: \n--tensorboard\n\n\n\n\n[COMMAND]\n\n\n[OPTIONS]\n\n\nInstance Type\n\n\nTo specify the instance type means to choose what kind of FloydHub instance\nyour job will run on. Think of this as a hardware choice rather than a software\none. (The software environment is declared with the\n\nEnvironment (\n--env\n)\n \nOPTION\n of \nfloyd run\n command.)\n\n\nYou have four instance type options to choose from when running a job as\ndetailed below:\n\n\n\n\n\n\n\n\nfloyd run\n Flag\n\n\nInstance Type\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--gpu\n\n\nGPU\n\n\nPreemptible GPU server\n\n\n\n\n\n\n--gpu+\n\n\nGPU\n\n\nDedicated GPU server\n\n\n\n\n\n\n--cpu\n\n\nCPU\n\n\nPreemptible CPU server\n\n\n\n\n\n\n--cpu+\n\n\nCPU\n\n\nDedicated CPU server\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nThe default instance flag is \n--cpu\n. This means that if you don't pass\n  any of the above flags to \nfloyd run\n, your job will be run on a\n  preemptible CPU server.\n\n\nIf you pass more than one instance flag, this is the order of precedence:\n  \n--gpu+\n, \n--cpu+\n, \n--gpu\n, \n--cpu\n\n\n\n\n\n\nWhat is the difference between a preemptible and a dedicated instance?\n\n\nRefer to \nthis section\n of\nthe documentation for more information about dedicated and preemptible\ninstances.\n\n\nDataset(s)\n\n\nYou can specify up to five datasources (datasets or outputs from previous jobs)\nto mount to the server that will be running your job. For each datasource,\nspecify the \n--data\n flag as detailed below:\n\n\n--data \nname_of_datasource\n:\nmount_point_on_server\n\n\n\n\n\nFor more detailed information on mounting data to jobs, see\n\nthis article\n\n\nMode\n\n\nFloydHub jobs can currently be run in one of three modes:\n\n\n\n\n--mode job\n (DEFAULT)\n\n\n--mode jupyter\n\n\n--mode serve\n\n\n\n\nHere is a description of each mode:\n\n\n--mode job\n\n\nThis is the default mode so there is no need to specify \n--mode job\n when\nrunning \nfloyd run\n. You can think of this mode as \"regular mode\" or \"command\nmode\". When you run your job in this mode, your code is sent up to a FloydHub\ndeep-learning server and the \n[COMMAND]\n portion of \nfloyd run\n\nis executed.\n\n\n--mode jupyter\n\n\nThis is Jupyter Notebook mode. When you specify \n--mode jupyter\n, your code is\nuploaded to a FloydHub deep-learning server, and a Jupyter Notebook session is\nstarted in the directory containing your code.\n\n\nFloyHub serves this Jupyter Notebook session for you. Your Jupyter Notebook\nsession will be automatically opened in a web browser when it becomes\navailable. If you don't want your browser to automatcially open, pass the \n--no-open\n flag to \nfloyd run\n.\n\n\n\n\nUse This Mode When:\n\n\n\n\nYou want to work with a \nJupyter Notebook\n.\n\n\nYou want to open an \ninteractive shell\n on the FloydHub server where your code\n   is running. For details on how to do that, see \nthis article\n\n\n\n\n\n\n--mode serve\n\n\nThis mode is for serving your deep learning models as an API endpoint. To use\nserve mode, you'll need to make sure your project's code meets a few\nrequirements:\n\n\n\n\nContains a file called \napp.py\n that contains a Flask application that will\n  listen on port 5000.\n\n\nContains a \nrequirements.txt\n file that contains the line \nflask\n, which\n  declares it as a dependency.\n\n\n\n\n\n\nWarning\n\n\nThis mode is currently in preview, and is not appropriate for production\nuse.\n\n\n\n\nEnvironment\n\n\nSpecifying the environment means choosing what major deep-learning software\npackages you want available on the server that runs your code. This is not a\nspecification between a CPU server and a GPU server (that's the\n\nInstance Type\n \nOPTION\n of \nfloyd run\n).\n\n\nFloydHub offers servers with many different deep-learning software packages\npre-installed.  You can find a list of all the available environments\n\nhere\n.\n\n\nUse the \n--env\n flag to specify which environment you would like your job to\nrun in.\n\n\n\n\nImportant\n\n\nIt is best practice to pass the entire name of the environment,\nincluding the version number, to the \n--env\n flag. For example, instead of\n\n--env tensorflow\n, use \n--env tensorflow-1.3\n.\n\n\n\n\n\n\nExamples\n\n\n$ floyd run --env tensorflow-1.3 \npython train.py\n\n\n\n\n\n$ floyd run --env theano-0.8 \npython train.py\n\n\n\n\n\n$ floyd run --env pytorch-0.2 \npython train.py\n\n\n\n\n\n\n\nMessage\n\n\nUsing \n--message\n or \n-m\n, you can specify a message that describes your job,\nsimiliar to the way a commit message describes a git commit. The job message\nwill be displayed at various places on floydhub.com and is useful when\nreviewing past jobs that you'd like to iterate on.\n\n\nExample:\n\n\n$ floyd run -m \nlorem ipsum\n \necho \nhello world\n\nCreating project run. Total upload size: \n195\n.0B\nSyncing code ...\n\n[================================]\n \n1254\n/1254 - \n00\n:00:00\n\nJOB NAME\n--------------------------------\nmckay/projects/message-project/1\n\nTo view logs enter:\n   floyd logs mckay/projects/message-project/1\n\n\n\n\nHere are some examples of where the job message will be displayed on floydhub.com:\n\n\n\n\n\n\nTensorboard\n\n\nThe \n--tensorboard\n flag allows you to enable Tensorboard on your job. For more details, see \nthis article\n.\n\n\n[COMMAND]\n\n\nThe \n[COMMAND]\n portion of \nfloyd run\n is the command that will be executed on\nthe server when your job begins. It will be run in the directory on the server\nthat holds your code. To decide what to put in the \n[COMMAND]\n, answer this\nquestion:\n\n\n\n\nWhat command would I execute to kick off my code locally?\n\n\n\n\nThe answer to that question is what you should put into the \n[COMMAND]\n portion\nof \nfloyd run\n.\n\n\nAny valid \nbash\n command will work in the \n[COMMAND]\n portion of \nfloyd run\n.\nFor example, try these simple examples and look at their logs:\n\n\n$ floyd run \npwd\n\n\n\n\n\n$ floyd run \nls\n\n\n\n\n\n$ floyd run \npython -v\n\n\n\n\n\n$ floyd run \necho \nHello, world!\n\n\n\n\n\nMost commonly you'll be kicking off a Python script with your \n[COMMAND]\n, with\nsomething like this:\n\n\n$ floyd run \npython train.py\n\n\n\n\n\nBut you can feel free to get creative!\n\n\n\n\nPro Tip\n\n\nTry chaining together multiple commands with \n like this:\n\n\n$ floyd run \nbash my_setup_script.sh \n python train.py\n\n\n\n\n\nFor more examples, check out our\n\nsymlinking tutorial\n.\n\n\n\n\n\n\nNote\n\n\nJupyter Note book mode (\n--mode jupyter\n) and serve mode (\n--mode serve\n)\ndo not take a \n[COMMAND]\n. You'll kick off your job without passing a\n\n[COMMAND],\n with something like the following:\n\n\n$ floyd run --env pytorch-0.2 --mode jupyter\n\n\n\n\n$ floyd run --env pytorch-0.2 --mode serve --data mckay/datasets/mnist/1:mount\n\n\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Run a Job"
        }, 
        {
            "location": "/guides/run_a_job/#parts-of-the-floyd-run-command", 
            "text": "", 
            "title": "Parts of the floyd run Command"
        }, 
        {
            "location": "/guides/run_a_job/#options", 
            "text": "Instance Type :  --cpu   or   --gpu   or   --cpu+   or   --gpu+  Dataset(s) :  --data  Mode :  --mode  Environment :  --env  Message   --message   or   -m  Tensorboard :  --tensorboard", 
            "title": "[OPTIONS]"
        }, 
        {
            "location": "/guides/run_a_job/#command", 
            "text": "", 
            "title": "[COMMAND]"
        }, 
        {
            "location": "/guides/run_a_job/#options_1", 
            "text": "", 
            "title": "[OPTIONS]"
        }, 
        {
            "location": "/guides/run_a_job/#instance-type", 
            "text": "To specify the instance type means to choose what kind of FloydHub instance\nyour job will run on. Think of this as a hardware choice rather than a software\none. (The software environment is declared with the Environment ( --env )   OPTION  of  floyd run  command.)  You have four instance type options to choose from when running a job as\ndetailed below:     floyd run  Flag  Instance Type  Description      --gpu  GPU  Preemptible GPU server    --gpu+  GPU  Dedicated GPU server    --cpu  CPU  Preemptible CPU server    --cpu+  CPU  Dedicated CPU server      Important   The default instance flag is  --cpu . This means that if you don't pass\n  any of the above flags to  floyd run , your job will be run on a\n  preemptible CPU server.  If you pass more than one instance flag, this is the order of precedence:\n   --gpu+ ,  --cpu+ ,  --gpu ,  --cpu", 
            "title": "Instance Type"
        }, 
        {
            "location": "/guides/run_a_job/#what-is-the-difference-between-a-preemptible-and-a-dedicated-instance", 
            "text": "Refer to  this section  of\nthe documentation for more information about dedicated and preemptible\ninstances.", 
            "title": "What is the difference between a preemptible and a dedicated instance?"
        }, 
        {
            "location": "/guides/run_a_job/#datasets", 
            "text": "You can specify up to five datasources (datasets or outputs from previous jobs)\nto mount to the server that will be running your job. For each datasource,\nspecify the  --data  flag as detailed below:  --data  name_of_datasource : mount_point_on_server   For more detailed information on mounting data to jobs, see this article", 
            "title": "Dataset(s)"
        }, 
        {
            "location": "/guides/run_a_job/#mode", 
            "text": "FloydHub jobs can currently be run in one of three modes:   --mode job  (DEFAULT)  --mode jupyter  --mode serve   Here is a description of each mode:", 
            "title": "Mode"
        }, 
        {
            "location": "/guides/run_a_job/#-mode-job", 
            "text": "This is the default mode so there is no need to specify  --mode job  when\nrunning  floyd run . You can think of this mode as \"regular mode\" or \"command\nmode\". When you run your job in this mode, your code is sent up to a FloydHub\ndeep-learning server and the  [COMMAND]  portion of  floyd run \nis executed.", 
            "title": "--mode job"
        }, 
        {
            "location": "/guides/run_a_job/#-mode-jupyter", 
            "text": "This is Jupyter Notebook mode. When you specify  --mode jupyter , your code is\nuploaded to a FloydHub deep-learning server, and a Jupyter Notebook session is\nstarted in the directory containing your code.  FloyHub serves this Jupyter Notebook session for you. Your Jupyter Notebook\nsession will be automatically opened in a web browser when it becomes\navailable. If you don't want your browser to automatcially open, pass the  --no-open  flag to  floyd run .   Use This Mode When:   You want to work with a  Jupyter Notebook .  You want to open an  interactive shell  on the FloydHub server where your code\n   is running. For details on how to do that, see  this article", 
            "title": "--mode jupyter"
        }, 
        {
            "location": "/guides/run_a_job/#-mode-serve", 
            "text": "This mode is for serving your deep learning models as an API endpoint. To use\nserve mode, you'll need to make sure your project's code meets a few\nrequirements:   Contains a file called  app.py  that contains a Flask application that will\n  listen on port 5000.  Contains a  requirements.txt  file that contains the line  flask , which\n  declares it as a dependency.    Warning  This mode is currently in preview, and is not appropriate for production\nuse.", 
            "title": "--mode serve"
        }, 
        {
            "location": "/guides/run_a_job/#environment", 
            "text": "Specifying the environment means choosing what major deep-learning software\npackages you want available on the server that runs your code. This is not a\nspecification between a CPU server and a GPU server (that's the Instance Type   OPTION  of  floyd run ).  FloydHub offers servers with many different deep-learning software packages\npre-installed.  You can find a list of all the available environments here .  Use the  --env  flag to specify which environment you would like your job to\nrun in.   Important  It is best practice to pass the entire name of the environment,\nincluding the version number, to the  --env  flag. For example, instead of --env tensorflow , use  --env tensorflow-1.3 .    Examples  $ floyd run --env tensorflow-1.3  python train.py   $ floyd run --env theano-0.8  python train.py   $ floyd run --env pytorch-0.2  python train.py", 
            "title": "Environment"
        }, 
        {
            "location": "/guides/run_a_job/#message", 
            "text": "Using  --message  or  -m , you can specify a message that describes your job,\nsimiliar to the way a commit message describes a git commit. The job message\nwill be displayed at various places on floydhub.com and is useful when\nreviewing past jobs that you'd like to iterate on.  Example:  $ floyd run -m  lorem ipsum   echo  hello world \nCreating project run. Total upload size:  195 .0B\nSyncing code ... [================================]   1254 /1254 -  00 :00:00\n\nJOB NAME\n--------------------------------\nmckay/projects/message-project/1\n\nTo view logs enter:\n   floyd logs mckay/projects/message-project/1  Here are some examples of where the job message will be displayed on floydhub.com:", 
            "title": "Message"
        }, 
        {
            "location": "/guides/run_a_job/#tensorboard", 
            "text": "The  --tensorboard  flag allows you to enable Tensorboard on your job. For more details, see  this article .", 
            "title": "Tensorboard"
        }, 
        {
            "location": "/guides/run_a_job/#command_1", 
            "text": "The  [COMMAND]  portion of  floyd run  is the command that will be executed on\nthe server when your job begins. It will be run in the directory on the server\nthat holds your code. To decide what to put in the  [COMMAND] , answer this\nquestion:   What command would I execute to kick off my code locally?   The answer to that question is what you should put into the  [COMMAND]  portion\nof  floyd run .  Any valid  bash  command will work in the  [COMMAND]  portion of  floyd run .\nFor example, try these simple examples and look at their logs:  $ floyd run  pwd   $ floyd run  ls   $ floyd run  python -v   $ floyd run  echo  Hello, world!   Most commonly you'll be kicking off a Python script with your  [COMMAND] , with\nsomething like this:  $ floyd run  python train.py   But you can feel free to get creative!   Pro Tip  Try chaining together multiple commands with   like this:  $ floyd run  bash my_setup_script.sh   python train.py   For more examples, check out our symlinking tutorial .    Note  Jupyter Note book mode ( --mode jupyter ) and serve mode ( --mode serve )\ndo not take a  [COMMAND] . You'll kick off your job without passing a [COMMAND],  with something like the following:  $ floyd run --env pytorch-0.2 --mode jupyter  $ floyd run --env pytorch-0.2 --mode serve --data mckay/datasets/mnist/1:mount", 
            "title": "[COMMAND]"
        }, 
        {
            "location": "/guides/run_a_job/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/stop_job/", 
            "text": "Quick Look\n\n\nFrom the CLI:\n\n$ floyd stop \nusername\n/projects/\nproject_name\n/\njob_number\n\n\n\n\nFrom floydhub.com:\n\n\n\n\n\nYou can stop a queued or running job using Floyd CLI or using the web interface\non \nfloydhub.com\n.\n\n\nUsing the CLI\n\n\nA job can be stopped using the \nfloyd stop\n command and passing it the name of\nyour job, as shown below:\n\n\n$ floyd stop mckay/projects/ssh/2\nExperiment shutdown request submitted. Check status to confirm shutdown\n\n\n\n\nFloyd CLI is fairly smart at resolving job names. It defaults to using your\nusername, the project initialized in the directory you're working in, and the\nlatest job number. So if you don't pass a job name to \nfloyd stop\n, it will\nstop your most recent job. Here are a couple examples of shortened job names\nthat are valid:\n\n\n# Stops the most recent job for your user\ns project called \nfoo\n\n$ floyd stop foo\n\n\n\n\n# Stops job 2 for your user\ns project called \nfoo\n\n$ floyd stop foo/2\n\n\n\n\nTo check the status of your job to make sure it has shut down properly, use the\n\nfloyd status\n command, as shown below:\n\n\n$ floyd status mckay/projects/ssh/2\nJOB NAME                     CREATED         STATUS      DURATION\n(\ns\n)\n  INSTANCE    DESCRIPTION\n---------------------------  --------------  --------  -------------  ----------  -------------\nmckay/projects/ssh/2         \n47\n seconds ago  shutdown             \n24\n  c1\n\n\n\n\nThe same job-name shortcuts available for \nfloyd stop\n are available whenever\nyou need to specify a job name, including with \nfloyd status \njob_name\n.\n\n\nUsing floydhub.com\n\n\nOn the project page, click the \nCancel\n button below the icon that shows the status of your job, as shown in the picture below:\n\n\n\n\nThen click the \nConfirm\n button in the modal that pops up:\n\n\n\nThe status of your job will update to \nShutdown\n when your job has successfully\nbeen canceled.", 
            "title": "Stop a Job"
        }, 
        {
            "location": "/guides/stop_job/#using-the-cli", 
            "text": "A job can be stopped using the  floyd stop  command and passing it the name of\nyour job, as shown below:  $ floyd stop mckay/projects/ssh/2\nExperiment shutdown request submitted. Check status to confirm shutdown  Floyd CLI is fairly smart at resolving job names. It defaults to using your\nusername, the project initialized in the directory you're working in, and the\nlatest job number. So if you don't pass a job name to  floyd stop , it will\nstop your most recent job. Here are a couple examples of shortened job names\nthat are valid:  # Stops the most recent job for your user s project called  foo \n$ floyd stop foo  # Stops job 2 for your user s project called  foo \n$ floyd stop foo/2  To check the status of your job to make sure it has shut down properly, use the floyd status  command, as shown below:  $ floyd status mckay/projects/ssh/2\nJOB NAME                     CREATED         STATUS      DURATION ( s )   INSTANCE    DESCRIPTION\n---------------------------  --------------  --------  -------------  ----------  -------------\nmckay/projects/ssh/2          47  seconds ago  shutdown              24   c1  The same job-name shortcuts available for  floyd stop  are available whenever\nyou need to specify a job name, including with  floyd status  job_name .", 
            "title": "Using the CLI"
        }, 
        {
            "location": "/guides/stop_job/#using-floydhubcom", 
            "text": "On the project page, click the  Cancel  button below the icon that shows the status of your job, as shown in the picture below:   Then click the  Confirm  button in the modal that pops up:  The status of your job will update to  Shutdown  when your job has successfully\nbeen canceled.", 
            "title": "Using floydhub.com"
        }, 
        {
            "location": "/guides/restart_job/", 
            "text": "Quick Look\n\n\nJupyter Notebook Jobs in Web Dashboard:\n\n\n\n\nUsing Floyd CLI:\n\n\n$ floyd restart mckay/projects/mnist/1 --gpu \npython train.py\n\n\n\n\n\n\n\nJupyter Notebook Job in Web Dashboard:\n\n\nJupyter Notebook jobs can be restarted through the web dashboard. Navigate to\nthe job's detail page, and click the \nRestart\n button in the top right corner\nas shown in the screenshot below:\n\n\n\n\nThis will start your Jupyter Notebook where you last left it. If you'd like to\nrestart your job back where it \nstarted\n, just restart the previous job. See the\nrestart workflow in action:\n\n\n\n\nJupyter Notebook jobs can also be restarted using Floyd CLI, as detailed below.\n\n\nUsing Floyd CLI:\n\n\nAll jobs can be restarted/re-run using Floyd CLI's \nfloyd restart\n command. This\ncommand takes a \njob_name\n, and allows the same \n[OPTIONS]\n and \n[COMMAND]\n\nparameters as the \nfloyd run\n command. You can specify a \nshortened job\nname\n to this command.\n\n\nThis is most useful when you want to restart/re-run a job, but override certain\nparameters of the job (like upgrading its instance type) or overriding the\n\n[COMMAND]\n that was used in the job.\n\n\nBelow are a few examples. Each examples restarts/re-runs the\n\nmckay/projects/quick-start/1\n job, but overrides different parts of the job:\n\n\n# Override the command\n$ floyd restart mckay/projects/quick-start/1 \npython train.py\n\n\n\n\n\n# Run the job on a GPU server\n$ floyd restart mckay/projects/quick-start/1 --gpu\n\n\n\n\n# Run the job with a new version of a dataset\n$ floyd restart mckay/projects/quick-start/1 --data mckay/datasets/mnist/1:mnist\n\n\n\n\nParameter sweeping:\n\n\nA great use case for the restart command is when you need to run a series of jobs with\ndifferent training parameters. You just need to make your training script take in all the\nparameters from the command line. After you run the first job using the \nfloyd run\n command:\n\n\n#\n \nRun\n \nthe\n \nfirst\n \njob\n \nwith\n \ninitial\n \nparameters\n\n\n$\n \nfloyd\n \nrun\n \n--\ngpu\n \npython\n \ntrain\n.\npy\n \n--\nlearning-rate\n \n0.01\n \n--\nbatch-size\n \n8\n \n--\nepochs\n \n100\n\n\n...\n\n\nJOB\n \nNAME\n\n\n--------------------------------\n\n\nmckay/projects/tf-grid-search/1\n\n\n\n\n\nNow you can just restart this job with different set of parameters:\n\n\n# Change the learning rate\n$ floyd restart mckay/projects/tf-grid-search/1 --gpu \npython train.py --learning-rate 0.05 --batch-size 8 --epochs 100\n\n\n\n\n\n# Change the number of epochs\n$ floyd restart mckay/projects/tf-grid-search/1 --gpu \npython train.py --learning-rate 0.05 --batch-size 8 --epochs 500\n\n\n\n\n\nThis gives you the ability to try a range of parameters without uploading your code each time.", 
            "title": "Restart a Job"
        }, 
        {
            "location": "/guides/restart_job/#jupyter-notebook-jobs-in-web-dashboard", 
            "text": "", 
            "title": "Jupyter Notebook Jobs in Web Dashboard:"
        }, 
        {
            "location": "/guides/restart_job/#using-floyd-cli", 
            "text": "$ floyd restart mckay/projects/mnist/1 --gpu  python train.py", 
            "title": "Using Floyd CLI:"
        }, 
        {
            "location": "/guides/restart_job/#jupyter-notebook-job-in-web-dashboard", 
            "text": "Jupyter Notebook jobs can be restarted through the web dashboard. Navigate to\nthe job's detail page, and click the  Restart  button in the top right corner\nas shown in the screenshot below:   This will start your Jupyter Notebook where you last left it. If you'd like to\nrestart your job back where it  started , just restart the previous job. See the\nrestart workflow in action:   Jupyter Notebook jobs can also be restarted using Floyd CLI, as detailed below.", 
            "title": "Jupyter Notebook Job in Web Dashboard:"
        }, 
        {
            "location": "/guides/restart_job/#using-floyd-cli_1", 
            "text": "All jobs can be restarted/re-run using Floyd CLI's  floyd restart  command. This\ncommand takes a  job_name , and allows the same  [OPTIONS]  and  [COMMAND] \nparameters as the  floyd run  command. You can specify a  shortened job\nname  to this command.  This is most useful when you want to restart/re-run a job, but override certain\nparameters of the job (like upgrading its instance type) or overriding the [COMMAND]  that was used in the job.  Below are a few examples. Each examples restarts/re-runs the mckay/projects/quick-start/1  job, but overrides different parts of the job:  # Override the command\n$ floyd restart mckay/projects/quick-start/1  python train.py   # Run the job on a GPU server\n$ floyd restart mckay/projects/quick-start/1 --gpu  # Run the job with a new version of a dataset\n$ floyd restart mckay/projects/quick-start/1 --data mckay/datasets/mnist/1:mnist", 
            "title": "Using Floyd CLI:"
        }, 
        {
            "location": "/guides/restart_job/#parameter-sweeping", 
            "text": "A great use case for the restart command is when you need to run a series of jobs with\ndifferent training parameters. You just need to make your training script take in all the\nparameters from the command line. After you run the first job using the  floyd run  command:  #   Run   the   first   job   with   initial   parameters  $   floyd   run   -- gpu   python   train . py   -- learning-rate   0.01   -- batch-size   8   -- epochs   100  ...  JOB   NAME  --------------------------------  mckay/projects/tf-grid-search/1   Now you can just restart this job with different set of parameters:  # Change the learning rate\n$ floyd restart mckay/projects/tf-grid-search/1 --gpu  python train.py --learning-rate 0.05 --batch-size 8 --epochs 100   # Change the number of epochs\n$ floyd restart mckay/projects/tf-grid-search/1 --gpu  python train.py --learning-rate 0.05 --batch-size 8 --epochs 500   This gives you the ability to try a range of parameters without uploading your code each time.", 
            "title": "Parameter sweeping:"
        }, 
        {
            "location": "/guides/jobs/tensorboard/", 
            "text": "Tensorboard\n \nis a visualization tool for Tensorflow projects. Tensorboard can help \nvisualize the Tensorflow computation graph and plot quantitative metrics about your run. This \nguide will help you understand how to enable Tensorboard in your jobs.\n\n\nKey concepts of Tensorboard\n\n\nIf you would like to know more about the concepts of Tensorboard please check out\nthe \nTensorboard README\n\nfile. This page also goes into the details of Tensorboard and explains the various \ndashboards that are present in the Tensorboard UI.\n\n\nEnabling Tensorboard in your job\n\n\nTo enable Tensorboard in your job, you need to specify a \n--tensorboard\n flag \nwhen you run the job. Tensorboard can be enabled for both CLI jobs and when running \nJupyter notebooks.\n\n\nExample\n\n\nThis code snipped will train an MNIST model and also store the training summary \nto a log directory.\n\n\ngit clone https://github.com/floydhub/tensorflow-examples\n\ncd\n tensorflow-examples/tensorboard\n\n\n# Initialize the current directory to an existing or new project\n\nfloyd init mnist-tensorboard\nfloyd run --tensorboard \npython mnist_tensorboard.py --log_dir /output/mnist --max_steps 5000\n\n\n\n\n\n\n\nNotice that the the \nlog_dir\n parameter is set to a path in the \n/output\n directory.\nOn Floydhub, \n/output\n is a special directory that Tensorboard watches. Be sure to send \nall data meant for Tensorboard to any directory under \n/output\n path.\n\n\n\n\nNow you can view the job on your Project dashboard.\n\n\n\n\nClick on the job that was just started. You will notice that the job page now has a link \nto Tensorboard. Click on it to open the Tensorboard dashboard in a new tab.\n\n\n\n\nTensorboard Dashboard\n\n\n\n\nYou can see that the \"SCALARS\" tab of Tensorboard is logging the accuracy of the \ntraining and test data along with some other values. You may need to click on the title \nbars (like \naccuracy_1\n) for the graph to open.\n\n\nThe reason why these values are appearing on the dashboard is because the \n\nmnist_tensorboard.py\n code has the following lines:\n\n\ntf\n.\nsummary\n.\nscalar\n(\ncross_entropy\n,\n \ncross_entropy\n)\n\n\n...\n\n\ntf\n.\nsummary\n.\nscalar\n(\naccuracy\n,\n \naccuracy\n)\n\n\n\n\n\nYou can read more about how to use Tensorboard to log additional information in \nthe \nTensorboard README\n.\n\n\nExplore the other tabs in the Tensorboad dashboard like \"IMAGES\" and \"GRAPHS\".\n\n\nTensorboard Tabs\n\n\n\n\nThe IMAGES dashboard shows the transformations happening to the mnist images\nin real time while the training is happening.\n\n\n\n\nThe GRAPHS dashboard shows a representation of Tensorflow's computation graph.\nYou can click into each part of the model to get more details.\n\n\n\n\nTensorboard feature is only available for Tensorflow environments. \nSee \nthis\n page for full list of Tensorflow environments you \ncan use.\n\n\n\n\nStopping Tensorboard\n\n\nTensorboard runs in the same machine where your code is running. So you do not have \nto stop it explicitly. It will be up until your job finishes and then stop automatically. \nTensorboard will become inaccessible when the job finishes in any of the \nSuccess\n, \nFailed\n, \n\nTimeout\n or \nShutdown\n states.\n\n\nTensorboard in Jupyter mode\n\n\nTensorboard can be run in Jupyter mode as well. You will notice that the links for both \nthe Jupyter notebook and the Tensorboard appear in the Job page.\n\n\n\n\nOffline Training\n\n\nUntil now, we saw how to use Tensorboard directly on Floydhub \nwhile\n your job is actively running. \nAlternatively you can also view the metrics offline after your \ntraining is done.\n\n\nFor that, you need to first download the output of your project to your local \nmachine.\n\n\nmkdir tensorboard_output \n \ncd\n tensorboard_output\nfloyd data clone floydhub/mnist-tensorboard/6/output\n\n\n\n\nThen you need to install tensorflow in your local machine. The instructions depend \non your OS. See the Tensorflow install instructions \nhere\n.\n\n\nAfter that you can just run the \ntensorboard\n command and point it to the output \ndirectory downloaded from Floydhub.\n\n\ntensorboard --logdir\n=\ntensorboard_output\n\n\n\n\nThen you can view the Tensorboard dashboard on your machine running at \n\nhttp://127.0.0.1:6006/", 
            "title": "Enable Tensorboard"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#key-concepts-of-tensorboard", 
            "text": "If you would like to know more about the concepts of Tensorboard please check out\nthe  Tensorboard README \nfile. This page also goes into the details of Tensorboard and explains the various \ndashboards that are present in the Tensorboard UI.", 
            "title": "Key concepts of Tensorboard"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#enabling-tensorboard-in-your-job", 
            "text": "To enable Tensorboard in your job, you need to specify a  --tensorboard  flag \nwhen you run the job. Tensorboard can be enabled for both CLI jobs and when running \nJupyter notebooks.", 
            "title": "Enabling Tensorboard in your job"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#example", 
            "text": "This code snipped will train an MNIST model and also store the training summary \nto a log directory.  git clone https://github.com/floydhub/tensorflow-examples cd  tensorflow-examples/tensorboard # Initialize the current directory to an existing or new project \nfloyd init mnist-tensorboard\nfloyd run --tensorboard  python mnist_tensorboard.py --log_dir /output/mnist --max_steps 5000    Notice that the the  log_dir  parameter is set to a path in the  /output  directory.\nOn Floydhub,  /output  is a special directory that Tensorboard watches. Be sure to send \nall data meant for Tensorboard to any directory under  /output  path.   Now you can view the job on your Project dashboard.   Click on the job that was just started. You will notice that the job page now has a link \nto Tensorboard. Click on it to open the Tensorboard dashboard in a new tab.", 
            "title": "Example"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#tensorboard-dashboard", 
            "text": "You can see that the \"SCALARS\" tab of Tensorboard is logging the accuracy of the \ntraining and test data along with some other values. You may need to click on the title \nbars (like  accuracy_1 ) for the graph to open.  The reason why these values are appearing on the dashboard is because the  mnist_tensorboard.py  code has the following lines:  tf . summary . scalar ( cross_entropy ,   cross_entropy )  ...  tf . summary . scalar ( accuracy ,   accuracy )   You can read more about how to use Tensorboard to log additional information in \nthe  Tensorboard README .  Explore the other tabs in the Tensorboad dashboard like \"IMAGES\" and \"GRAPHS\".", 
            "title": "Tensorboard Dashboard"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#tensorboard-tabs", 
            "text": "The IMAGES dashboard shows the transformations happening to the mnist images\nin real time while the training is happening.   The GRAPHS dashboard shows a representation of Tensorflow's computation graph.\nYou can click into each part of the model to get more details.   Tensorboard feature is only available for Tensorflow environments. \nSee  this  page for full list of Tensorflow environments you \ncan use.", 
            "title": "Tensorboard Tabs"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#stopping-tensorboard", 
            "text": "Tensorboard runs in the same machine where your code is running. So you do not have \nto stop it explicitly. It will be up until your job finishes and then stop automatically. \nTensorboard will become inaccessible when the job finishes in any of the  Success ,  Failed ,  Timeout  or  Shutdown  states.", 
            "title": "Stopping Tensorboard"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#tensorboard-in-jupyter-mode", 
            "text": "Tensorboard can be run in Jupyter mode as well. You will notice that the links for both \nthe Jupyter notebook and the Tensorboard appear in the Job page.", 
            "title": "Tensorboard in Jupyter mode"
        }, 
        {
            "location": "/guides/jobs/tensorboard/#offline-training", 
            "text": "Until now, we saw how to use Tensorboard directly on Floydhub  while  your job is actively running. \nAlternatively you can also view the metrics offline after your \ntraining is done.  For that, you need to first download the output of your project to your local \nmachine.  mkdir tensorboard_output    cd  tensorboard_output\nfloyd data clone floydhub/mnist-tensorboard/6/output  Then you need to install tensorflow in your local machine. The instructions depend \non your OS. See the Tensorflow install instructions  here .  After that you can just run the  tensorboard  command and point it to the output \ndirectory downloaded from Floydhub.  tensorboard --logdir = tensorboard_output  Then you can view the Tensorboard dashboard on your machine running at  http://127.0.0.1:6006/", 
            "title": "Offline Training"
        }, 
        {
            "location": "/guides/delete_job/", 
            "text": "You can delete an individual job by clicking on \nDelete job\n button on the Settings tab of the job's page on the web dashboard.\n\n\nExample: \nhttps://www.floydhub.com/mckay/projects/quick-start/1/settings\n\n\n\n\nDeleting a job using the CLI\n\n\nYou can also delete a job from the CLI using the \nfloyd delete\n command.\n\n\n$ floyd delete mckay/projects/quick-start/1\n\nDelete Run: mckay/quick-start/1? \n[\ny/N\n]\n: y\nJob mckay/quick-start/1 Deleted\n\n\n\n\nFloyd CLI is fairly smart at resolving job names. It defaults to using your\nusername, the project initialized in the directory you're working in, and the\nlatest job number. So if you don't pass a job name to \nfloyd delete\n, it will\ndelete your most recent job. Here are a couple examples of shortened job names\nthat are valid:\n\n\n# Deletes the most recent job for your user\ns project called \nfoo\n\n$ floyd delete foo\n\n\n\n\n# Deletes job 2 for your user\ns project called \nfoo\n\n$ floyd delete foo/2\n\n\n\n\nThe same job-name shortcuts available for \nfloyd delete\n are available whenever\nyou need to specify a job name, including with \nfloyd status \njob_name\n.\n\n\nFor more information on using shortened names, see\n\nthis article\n\n\nDeleting output of a Job\n\n\nIt is not possible to delete just the output of a job. You will have to delete the job itself.", 
            "title": "Delete a Job"
        }, 
        {
            "location": "/guides/delete_job/#deleting-a-job-using-the-cli", 
            "text": "You can also delete a job from the CLI using the  floyd delete  command.  $ floyd delete mckay/projects/quick-start/1\n\nDelete Run: mckay/quick-start/1?  [ y/N ] : y\nJob mckay/quick-start/1 Deleted  Floyd CLI is fairly smart at resolving job names. It defaults to using your\nusername, the project initialized in the directory you're working in, and the\nlatest job number. So if you don't pass a job name to  floyd delete , it will\ndelete your most recent job. Here are a couple examples of shortened job names\nthat are valid:  # Deletes the most recent job for your user s project called  foo \n$ floyd delete foo  # Deletes job 2 for your user s project called  foo \n$ floyd delete foo/2  The same job-name shortcuts available for  floyd delete  are available whenever\nyou need to specify a job name, including with  floyd status  job_name .  For more information on using shortened names, see this article", 
            "title": "Deleting a job using the CLI"
        }, 
        {
            "location": "/guides/delete_job/#deleting-output-of-a-job", 
            "text": "It is not possible to delete just the output of a job. You will have to delete the job itself.", 
            "title": "Deleting output of a Job"
        }, 
        {
            "location": "/guides/floyd_ignore/", 
            "text": "Floydignore is a special floyd cli construct that allows you to specify\nwhich files need to be uploaded to the server. This is very similar to\nhow gitignore works.\n\n\nMinimizing the number of files to upload saves upload time and disk space used\nby your experiments.\n\n\nInitialization\n\n\nEverytime a new dataset or project is \ninitialized\n using \nfloyd-cli\n a new\n\n.floydignore\n file is created in the current path.\n\n\n$ \ncd\n /code/project\n$ floyd init style-transfer\nProject \nstyle-transfer\n initialized in current directory\n$ cat .floydignore\n\n\n# Directories and files to ignore when uploading code to floyd\n\n\n.git\n.eggs\neggs\nlib\nlib64\nparts\nsdist\nvar\n\n\n\n\nNote: If a \n.floydignore\n file already exists in the initialization path, it will not be overridden.\n\n\nOptions\n\n\nThere are different ways to specify files, directories, or\n\nglob patterns\n that you want\nto be ignored. Below is a list with examples:\n\n\n# Ignore all .dat files in the whole project:\n*.dat\n\n# Ignore all .dat files in some_folder:\nsome_folder/*.dat\n\n# Ignore all .dat files in some_folder and its subfolders:\nsome_folder/**/*.dat\n\n# Ignore all files (and folders) named .DS_Store\n.DS_Store\n\n# Ignore a specific file named .DS_Store located in some_folder\nsome_folder/.DS_Store\n\n# Ignore all files named .DS_Store in some_folder and its subfolders\nsome_folder/**/.DS_Store\n\n# Ignore all files in some_folder\n/some_folder/\n\n# Ignore all files in some_folder (works the same as the rule above)\nsome_folder/\n\n# Ignore all files in some_folder (works the same as the rule above)\nsome_folder\n\n\n\n\nYou can also whitelist files, directories, and glob patterns by preceding them\nwith a \n!\n.  Items matching the pattern following the \n!\n that were excluded by\na previous pattern will become included again. It is not possible to re-include\na file if a parent directory of that file is excluded. Put a backslash (\n\\\n) in\nfront of the first \n!\n for patterns that begin with a literal \n!\n, for example,\n\n\\!important!.txt\n.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Ignore Files"
        }, 
        {
            "location": "/guides/floyd_ignore/#initialization", 
            "text": "Everytime a new dataset or project is  initialized  using  floyd-cli  a new .floydignore  file is created in the current path.  $  cd  /code/project\n$ floyd init style-transfer\nProject  style-transfer  initialized in current directory\n$ cat .floydignore # Directories and files to ignore when uploading code to floyd \n\n.git\n.eggs\neggs\nlib\nlib64\nparts\nsdist\nvar  Note: If a  .floydignore  file already exists in the initialization path, it will not be overridden.", 
            "title": "Initialization"
        }, 
        {
            "location": "/guides/floyd_ignore/#options", 
            "text": "There are different ways to specify files, directories, or glob patterns  that you want\nto be ignored. Below is a list with examples:  # Ignore all .dat files in the whole project:\n*.dat\n\n# Ignore all .dat files in some_folder:\nsome_folder/*.dat\n\n# Ignore all .dat files in some_folder and its subfolders:\nsome_folder/**/*.dat\n\n# Ignore all files (and folders) named .DS_Store\n.DS_Store\n\n# Ignore a specific file named .DS_Store located in some_folder\nsome_folder/.DS_Store\n\n# Ignore all files named .DS_Store in some_folder and its subfolders\nsome_folder/**/.DS_Store\n\n# Ignore all files in some_folder\n/some_folder/\n\n# Ignore all files in some_folder (works the same as the rule above)\nsome_folder/\n\n# Ignore all files in some_folder (works the same as the rule above)\nsome_folder  You can also whitelist files, directories, and glob patterns by preceding them\nwith a  ! .  Items matching the pattern following the  !  that were excluded by\na previous pattern will become included again. It is not possible to re-include\na file if a parent directory of that file is excluded. Put a backslash ( \\ ) in\nfront of the first  !  for patterns that begin with a literal  ! , for example, \\!important!.txt .", 
            "title": "Options"
        }, 
        {
            "location": "/guides/floyd_ignore/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/ssh/", 
            "text": "Quick Look\n\n\n$ floyd run --mode jupyter\n\n\n\n\n\n\n\n\nYou can start a bash session into your job's environment by running your job in\nJupyter Notebook mode:\n\n\n$ floyd run --mode jupyter\nCreating project run. Total upload size: \n183\n.0B\nSyncing code ...\n\n[================================]\n \n916\n/916 - \n00\n:00:00\n\nJOB NAME\n--------------------\nmckay/projects/ssh/1\n\nSetting up your instance and waiting \nfor\n Jupyter notebook to become available\n\nPath to jupyter notebook: https://floydhub.com/notebooks/YXau92xMFUshUbMdKqKVVX\n\n\n\n\nWhen you visit your running Jupyter Notebook in the browser, click the \nNew\n\nbutton in the top right corner of the screen and select the \nTerminal\n option\nas shown below:\n\n\n\n\nThat button will launch a terminal session in your running instance with root\naccess:\n\n\n\nCurrently, FloydHub does not offer true SSH access into instances, but the\nmethod described above is sufficient for what most users request SSH for.", 
            "title": "SSH into a Job"
        }, 
        {
            "location": "/faqs/job/", 
            "text": "Why does \nfloyd status\n return an empty list even though I have several\n\n\nruns in my account?\n\n\nFloyd CLI uses the project directory to store the run information (similar to git). So you need\nto be in the directory where you initialized the project and you should be able to see all your\nruns. You can also use the \nweb dashboard\n to view all your\nprojects in one place.\n\n\nWhat do I do when I get \"What do you do when you get \u201cYou are over the allowed limits for this operation. Consider upgrading your account\u201d?\n\n\nFloydhub currently allows only 1 active job per user for trial plan and 3 active jobs for individual plan. If you require more concurrency, contact\nus from the \npricing\n page.\n\n\nI get \"Too many open files\" error when I run my project.\n\n\nFloyd CLI throws this error when you have too many files in your current directory that needs to be uploaded.\nThe actual limit depends on your OS / machine specs.\n\n\nYou can either:\n\n\n\n\nRemove unnecessary files from the directory (like build directory, docs etc.)\n\n\nAdd them to \n.floydignore\n file. Floyd CLI will just ignore these directories.\nSee the \nfloydignore\n documentation to understand how this can be configured.\n\n\nTar them into a single file and untar them at runtime.\n\n\n\n\nAlternatively, instead of uploading files from your local machine, you can also\n\ndownload files\n from a remote URL\ndirectly into Floyd servers.\n\n\nWhy do I get an \"Experiments limit reached\" error when I run a job?\n\n\nFloydHub currently allows only 1 active job in the free Trial plan and 3 active jobs in the Individual plan. \nIf you see an \nError\n:\n \nExperiments\n \nlimit\n \nreached\n message when you run a job, it means you have maxed out your \nconcurrency limits. Please stop you running job(s) or wait for them to finish, and try again.\n\n\nWe have to enforce this concurrency constraint because we have a finite number of GPU machines and have to ensure that no single user is starving the group. In the near future, we will support queueing of jobs so that you can queue multiple jobs to be run as slots become available.\n\n\nI ran my project in Jupyter mode but the url does not seem to work.\n\n\nJupyter notebook server takes a couple of minutes to start. Until then you will get a \"Bad Gateway\"\nor similar error when you access the URL. You can check the status of the Jupyter notebook\nby running the \nlogs\n command.\n\n\nAm I using the GPU instance by default?\n\n\nJobs are run on CPU instances by default. You can specify \n--gpu\n to run them on GPU instances.\n\n\nMy job is taking a while to \"sync changes\". How do I make it go faster?\n\n\nFloyd CLI uploads \nall\n the files in your current directory before starting your experiment.\nThere are a few ways to make this go faster:\n\n\n\n\nRemove unnecessary files from the directory (like build directory, docs etc.)\n\n\nAdd sub-directories to \n.floydignore\n file. Floyd CLI will ignore and not upload these sub-directories.\nSee the \ninit\n command and \nignore files guide\n to understand how this can be configured.\n\n\nIf you have large data files consider uploading them separately as a \ndata source\n.\nYou can then \nrefer\n to them in your project.\n\n\n\n\nMy job finished but how I do I see my output?\n\n\nYou can use the floyd \noutput\n command to view the output of your\nproject. If you want to use this output in your next run view \nthis guide\n.\n\n\nDo I have to pay for the entire time my Jupyter Notebook is running?\n\n\nUnfortunately, yes. As much as we would like to, we are unable to charge you only for the \ncomputation time\n.\n\n\nThis is an engineering challenge. When you start a Jupyter Notebook instance and start executing commands,\nyour state is maintained in memory (both CPU and GPU memory). So the instance has to be alive for the\nentire duration of your notebook, not just when you are executing commands.\n\n\nFor example, when you execute \nimport\n \ntensorflow\n\ncommand, Tensorflow will allocate the entire GPU memory to the current session and waits for the next command. This makes the instance unusable by anyone else, so we have to charge you for the duration your Notebook is alive.\n\n\nCan I view my Jupyter Notebook after my job has stopped?\n\n\nYes. When you use a Jupyter Notebook on FloydHub, your Notebook is saved periodically in the \n/output\n dir. So, your work is not lost after your job has ended, shutdown or timed out.\n\n\nYou can view your saved Notebook using the \nfloyd output\n command. Example:\n\n\n$ floyd output redeipirati/projects/pytorch-fast-neural-style/3/output\n\n\n\n\nOr in the \nOutput\n tab of your job on the web dashboard, example: \nwww.floydhub.com/redeipirati/projects/pytorch-fast-neural-style/3/output\n\n\nCan I restart a stopped or timed out job?\n\n\nUnfortunately, not directly. We will be implementing a single command to do this soon!\n\n\nIn the meanwhile, you can follow these steps to do this manually:\n\n\n\n\nJupyter Notebook\n: Your Notebook is \nsaved periodically\n. To restart your Notebook after your job has stopped, please download the output of your stopped job to your machine and start another job. Example:\n\n\n\n\n# Download the saved Notebook from previous job\n\n\n# NOTE: This will overwrite the contents of your current dir\n\n$ floyd data clone redeipirati/projects/pytorch-fast-neural-style/3/output\n\n\n# Start a new job\n\n$ floyd run --mode jupyter\n\n\n\n\n\n\nScript\n: If you are running a script/command, you will have to start a new job using the \nfloyd run \ncommand\n command.\n\n\n\n\nWhy is my job in the \"Queued\" state for several minutes?\n\n\nThis means that a machine is being prepared to run your job. \n\n\nMost times, we have several CPU and GPU machines that are ready and your job can start execution in a few seconds. During high traffic periods, we may not have a machine ready for you and have to spin up a new instance for your job on-demand (details below). This might take up to 10 minutes in some cases. We are actively working on reducing this wait time.\n\n\nDetails\n: When you execute a \nfloyd run\n command, Floyd does several things in the background:\n\n\n\n\nProvision a CPU or GPU instance on the cloud\n\n\nSet up a deep learning environment with GPU drivers and the correct environment (as specified by \n--env\n) installed using Docker\n\n\nMount any data you specify using the \n--data\n flag\n\n\nSpin up a Jupyter server, if \n--mode jupyter\n flag\n\n\n\n\nEach of these steps can take up to a couple of minutes. Usually Steps 1 and 2 are already done, but during peak usage hours, we might have to do this on-demand.\n\n\nWhy do I see \"Setting up your instance...\" for several minutes when running a Jupyter Notebook?\n\n\nThe \nSetting up your instance...\n message is displayed when a machine is being prepared to run your Jupyter Notebook.\n\n\n\n\nWhen you execute a \nfloyd run --mode jupyter\n command, the CLI waits for a CPU or GPU machine to be ready before it opens up an interactive Jupyter Notebook for your work on. Usually, this takes a few seconds, but during high traffic periods it can take up to 10 minutes in some cases.\n\n\nFor more details on why it takes time, please see \nWhy is my job in the \"Queued\" state for several minutes?\n\n\nWhy are my logs not displayed in real-time?\n\n\nYou can stream your logs from the CLI using the \nfloyd logs -t \nJOB_NAME\n command. However, sometimes you may notice that the logs are not displayed in real-time. This is because of output buffering. Please make sure that your logs are flushed out if you prefer to view real-time logs.\n\n\nFor example, in Python:\n\n\nimport\n \nsys\n\n\n...\n\n\nprint\n(\nHello world\n)\n\n\nsys\n.\nstdout\n.\nflush\n()\n\n\n\n\n\nWhy did my job timeout after 1 hour?\n\n\nYou are likely in the Free Trial Plan. Jobs run in the trial plan have a maximum runtime of 1 hour. It will automatically timeout after that. \n\n\n\n\nYou can upgrade to the \nPaid Plan\n to overcome these limits.\n\n\nWhy was my CPU job Killed without warning?\n\n\nOccasionally, you may notice that your CPU job died without warning. The output logs just display \nKilled\n. For example,\n\n\n################################################################################\n\n\n\n2017\n-07-24 \n03\n:33:42,530 INFO - Run Output:\n...\n\n2017\n-07-24 \n03\n:33:52,920 INFO - Using TensorFlow backend.\n\n2017\n-07-24 \n03\n:34:04,381 INFO - \n loading UNet of size 1152x256...\n\n2017\n-07-24 \n03\n:34:10,942 INFO - Epoch \n1\n/100\n\n2017\n-07-24 \n03\n:35:17,221 INFO - Killed\n\n2017\n-07-24 \n03\n:35:18,680 INFO - \n\n################################################################################\n\n\n\n\n\nThis happens when your machine runs out of memory (OOM). i.e. your job consumes more memory than is available on our CPU machines.\n\n\nAll jobs run on FloydHub are executed inside a Docker container. Our current CPU machines have \n7GB memory\n. When memory used by your job exceeds 7GB, Docker automatically kills the job to protect the system and avoid abuse.\n\n\nThe resolution is to optimize your code to consume less memory. For example, read less data into your in-memory datastructures or reduce your batch size. We will be introducing more powerful CPUs, which higher memory in the near future.\n\n\nWhy did I get a \"Long running FloydHub Jupyter job detected\" email?\n\n\nFloydHub monitors Jupyter notebook instances that are no longer actively used and notifies the owner. This is a reminder in case the user forgot to turn off the instance after use and should help save resources.\n\n\nNote: If you create \nTerminals within Jupyter notebooks\n, this feature will not work. This is because FloydHub cannot reliably detect if the instance is being used or not.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Troubleshooting & FAQs"
        }, 
        {
            "location": "/faqs/job/#why-does-floyd-status-return-an-empty-list-even-though-i-have-several", 
            "text": "runs in my account?  Floyd CLI uses the project directory to store the run information (similar to git). So you need\nto be in the directory where you initialized the project and you should be able to see all your\nruns. You can also use the  web dashboard  to view all your\nprojects in one place.", 
            "title": "Why does floyd status return an empty list even though I have several"
        }, 
        {
            "location": "/faqs/job/#what-do-i-do-when-i-get-what-do-you-do-when-you-get-you-are-over-the-allowed-limits-for-this-operation-consider-upgrading-your-account", 
            "text": "Floydhub currently allows only 1 active job per user for trial plan and 3 active jobs for individual plan. If you require more concurrency, contact\nus from the  pricing  page.", 
            "title": "What do I do when I get \"What do you do when you get \u201cYou are over the allowed limits for this operation. Consider upgrading your account\u201d?"
        }, 
        {
            "location": "/faqs/job/#i-get-too-many-open-files-error-when-i-run-my-project", 
            "text": "Floyd CLI throws this error when you have too many files in your current directory that needs to be uploaded.\nThe actual limit depends on your OS / machine specs.  You can either:   Remove unnecessary files from the directory (like build directory, docs etc.)  Add them to  .floydignore  file. Floyd CLI will just ignore these directories.\nSee the  floydignore  documentation to understand how this can be configured.  Tar them into a single file and untar them at runtime.   Alternatively, instead of uploading files from your local machine, you can also download files  from a remote URL\ndirectly into Floyd servers.", 
            "title": "I get \"Too many open files\" error when I run my project."
        }, 
        {
            "location": "/faqs/job/#why-do-i-get-an-experiments-limit-reached-error-when-i-run-a-job", 
            "text": "FloydHub currently allows only 1 active job in the free Trial plan and 3 active jobs in the Individual plan. \nIf you see an  Error :   Experiments   limit   reached  message when you run a job, it means you have maxed out your \nconcurrency limits. Please stop you running job(s) or wait for them to finish, and try again.  We have to enforce this concurrency constraint because we have a finite number of GPU machines and have to ensure that no single user is starving the group. In the near future, we will support queueing of jobs so that you can queue multiple jobs to be run as slots become available.", 
            "title": "Why do I get an \"Experiments limit reached\" error when I run a job?"
        }, 
        {
            "location": "/faqs/job/#i-ran-my-project-in-jupyter-mode-but-the-url-does-not-seem-to-work", 
            "text": "Jupyter notebook server takes a couple of minutes to start. Until then you will get a \"Bad Gateway\"\nor similar error when you access the URL. You can check the status of the Jupyter notebook\nby running the  logs  command.", 
            "title": "I ran my project in Jupyter mode but the url does not seem to work."
        }, 
        {
            "location": "/faqs/job/#am-i-using-the-gpu-instance-by-default", 
            "text": "Jobs are run on CPU instances by default. You can specify  --gpu  to run them on GPU instances.", 
            "title": "Am I using the GPU instance by default?"
        }, 
        {
            "location": "/faqs/job/#my-job-is-taking-a-while-to-sync-changes-how-do-i-make-it-go-faster", 
            "text": "Floyd CLI uploads  all  the files in your current directory before starting your experiment.\nThere are a few ways to make this go faster:   Remove unnecessary files from the directory (like build directory, docs etc.)  Add sub-directories to  .floydignore  file. Floyd CLI will ignore and not upload these sub-directories.\nSee the  init  command and  ignore files guide  to understand how this can be configured.  If you have large data files consider uploading them separately as a  data source .\nYou can then  refer  to them in your project.", 
            "title": "My job is taking a while to \"sync changes\". How do I make it go faster?"
        }, 
        {
            "location": "/faqs/job/#my-job-finished-but-how-i-do-i-see-my-output", 
            "text": "You can use the floyd  output  command to view the output of your\nproject. If you want to use this output in your next run view  this guide .", 
            "title": "My job finished but how I do I see my output?"
        }, 
        {
            "location": "/faqs/job/#do-i-have-to-pay-for-the-entire-time-my-jupyter-notebook-is-running", 
            "text": "Unfortunately, yes. As much as we would like to, we are unable to charge you only for the  computation time .  This is an engineering challenge. When you start a Jupyter Notebook instance and start executing commands,\nyour state is maintained in memory (both CPU and GPU memory). So the instance has to be alive for the\nentire duration of your notebook, not just when you are executing commands.  For example, when you execute  import   tensorflow \ncommand, Tensorflow will allocate the entire GPU memory to the current session and waits for the next command. This makes the instance unusable by anyone else, so we have to charge you for the duration your Notebook is alive.", 
            "title": "Do I have to pay for the entire time my Jupyter Notebook is running?"
        }, 
        {
            "location": "/faqs/job/#can-i-view-my-jupyter-notebook-after-my-job-has-stopped", 
            "text": "Yes. When you use a Jupyter Notebook on FloydHub, your Notebook is saved periodically in the  /output  dir. So, your work is not lost after your job has ended, shutdown or timed out.  You can view your saved Notebook using the  floyd output  command. Example:  $ floyd output redeipirati/projects/pytorch-fast-neural-style/3/output  Or in the  Output  tab of your job on the web dashboard, example:  www.floydhub.com/redeipirati/projects/pytorch-fast-neural-style/3/output", 
            "title": "Can I view my Jupyter Notebook after my job has stopped?"
        }, 
        {
            "location": "/faqs/job/#can-i-restart-a-stopped-or-timed-out-job", 
            "text": "Unfortunately, not directly. We will be implementing a single command to do this soon!  In the meanwhile, you can follow these steps to do this manually:   Jupyter Notebook : Your Notebook is  saved periodically . To restart your Notebook after your job has stopped, please download the output of your stopped job to your machine and start another job. Example:   # Download the saved Notebook from previous job  # NOTE: This will overwrite the contents of your current dir \n$ floyd data clone redeipirati/projects/pytorch-fast-neural-style/3/output # Start a new job \n$ floyd run --mode jupyter   Script : If you are running a script/command, you will have to start a new job using the  floyd run  command  command.", 
            "title": "Can I restart a stopped or timed out job?"
        }, 
        {
            "location": "/faqs/job/#why-is-my-job-in-the-queued-state-for-several-minutes", 
            "text": "This means that a machine is being prepared to run your job.   Most times, we have several CPU and GPU machines that are ready and your job can start execution in a few seconds. During high traffic periods, we may not have a machine ready for you and have to spin up a new instance for your job on-demand (details below). This might take up to 10 minutes in some cases. We are actively working on reducing this wait time.  Details : When you execute a  floyd run  command, Floyd does several things in the background:   Provision a CPU or GPU instance on the cloud  Set up a deep learning environment with GPU drivers and the correct environment (as specified by  --env ) installed using Docker  Mount any data you specify using the  --data  flag  Spin up a Jupyter server, if  --mode jupyter  flag   Each of these steps can take up to a couple of minutes. Usually Steps 1 and 2 are already done, but during peak usage hours, we might have to do this on-demand.", 
            "title": "Why is my job in the \"Queued\" state for several minutes?"
        }, 
        {
            "location": "/faqs/job/#why-do-i-see-setting-up-your-instance-for-several-minutes-when-running-a-jupyter-notebook", 
            "text": "The  Setting up your instance...  message is displayed when a machine is being prepared to run your Jupyter Notebook.   When you execute a  floyd run --mode jupyter  command, the CLI waits for a CPU or GPU machine to be ready before it opens up an interactive Jupyter Notebook for your work on. Usually, this takes a few seconds, but during high traffic periods it can take up to 10 minutes in some cases.  For more details on why it takes time, please see  Why is my job in the \"Queued\" state for several minutes?", 
            "title": "Why do I see \"Setting up your instance...\" for several minutes when running a Jupyter Notebook?"
        }, 
        {
            "location": "/faqs/job/#why-are-my-logs-not-displayed-in-real-time", 
            "text": "You can stream your logs from the CLI using the  floyd logs -t  JOB_NAME  command. However, sometimes you may notice that the logs are not displayed in real-time. This is because of output buffering. Please make sure that your logs are flushed out if you prefer to view real-time logs.  For example, in Python:  import   sys  ...  print ( Hello world )  sys . stdout . flush ()", 
            "title": "Why are my logs not displayed in real-time?"
        }, 
        {
            "location": "/faqs/job/#why-did-my-job-timeout-after-1-hour", 
            "text": "You are likely in the Free Trial Plan. Jobs run in the trial plan have a maximum runtime of 1 hour. It will automatically timeout after that.    You can upgrade to the  Paid Plan  to overcome these limits.", 
            "title": "Why did my job timeout after 1 hour?"
        }, 
        {
            "location": "/faqs/job/#why-was-my-cpu-job-killed-without-warning", 
            "text": "Occasionally, you may notice that your CPU job died without warning. The output logs just display  Killed . For example,  ################################################################################  2017 -07-24  03 :33:42,530 INFO - Run Output:\n... 2017 -07-24  03 :33:52,920 INFO - Using TensorFlow backend. 2017 -07-24  03 :34:04,381 INFO -   loading UNet of size 1152x256... 2017 -07-24  03 :34:10,942 INFO - Epoch  1 /100 2017 -07-24  03 :35:17,221 INFO - Killed 2017 -07-24  03 :35:18,680 INFO -  ################################################################################   This happens when your machine runs out of memory (OOM). i.e. your job consumes more memory than is available on our CPU machines.  All jobs run on FloydHub are executed inside a Docker container. Our current CPU machines have  7GB memory . When memory used by your job exceeds 7GB, Docker automatically kills the job to protect the system and avoid abuse.  The resolution is to optimize your code to consume less memory. For example, read less data into your in-memory datastructures or reduce your batch size. We will be introducing more powerful CPUs, which higher memory in the near future.", 
            "title": "Why was my CPU job Killed without warning?"
        }, 
        {
            "location": "/faqs/job/#why-did-i-get-a-long-running-floydhub-jupyter-job-detected-email", 
            "text": "FloydHub monitors Jupyter notebook instances that are no longer actively used and notifies the owner. This is a reminder in case the user forgot to turn off the instance after use and should help save resources.  Note: If you create  Terminals within Jupyter notebooks , this feature will not work. This is because FloydHub cannot reliably detect if the instance is being used or not.", 
            "title": "Why did I get a \"Long running FloydHub Jupyter job detected\" email?"
        }, 
        {
            "location": "/faqs/job/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/", 
            "text": "Create a new Dataset\n\n\nA \nDataset\n is a collection of data. If you have used Github, datasets in\nFloydHub are a lot like code repositories, except they are for storing and\nversioning data.\n\n\nTo create a new Dataset, visit\n\nwww.floydhub.com/datasets\n and click on the\n\"New Dataset\" button on the top right hand corner.\n\n\n\n\nGive the dataset a name and an apt description.\n\n\nThe \nVisibility\n field indicates who can see your dataset. If you set it to\n\nPublic\n, anyone can see your dataset and data versions. If you are working on\nan open source project, this is a great way to share and contribute to the\nFloydHub community. If your data is proprietary, please select \nPrivate\n. This\nwill ensure that only you and your team will have access to this dataset.\n\n\nThe section below shows how to upload a dataset from your local machine. If your\ndata is available on the internet, you can can \ncreate a dataset out of it directly\n.\n\n\nUpload a Dataset\n\n\nOnce you have created a dataset, you can upload data from your terminal using\nthe \nfloyd data\n command:\n\n\n\n\nfloyd data init \ndataset_name\n\n\nfloyd data upload\n\n\n\n\nFor example:\n\n\n$ floyd data init imagenet-2017\nDataset \nimagenet-2017\n initialized in current directory\n...\n$ floyd data upload\nCompressing data...\n\n\n\n\n\n\nNote\n\n\nDepending on the size of your dataset and the speed of your internet\nconnection, uploading a dataset can take a while.\n\n\n\n\nResuming an Upload\n\n\nDataset uploads are resumable. If your Internet connection cuts out during an\nupload, you'll be able to resume it later if you choose to.\n\n\nIf your upload has stopped before it completing, resume it using the \n--resume\n\nor \n-r\n flag:\n\n\n$ floyd data upload --resume\nUploading compressed data. Total upload size: \n74\n.0MiB\n\n[=\n                               \n]\n \n4194304\n/77626756 - \n00\n:00:00\n\n\n\n\nIf you don't pass the \n--resume\n flag, but you have an unfinished upload, you\nwill be prompted to specify whether or not you'd like to resume the previous\nupload:\n\n\n$ floyd data upload\nAn unfinished upload exists. Would you like to resume it? \n[\ny/N\n]\n: N\nCompressing data...\n\n\n\n\nUpdating/Versioning Your Dataset\n\n\nIf you've made changes to your dataset and would like to upload it again, use\nthe following steps. You'll notice they are the same as uploading your dataset\nthe first time:\n\n\n\n\ncd\n into your dataset's directory\n\n\nRun \nfloyd data init \ndataset_name\n to prepare to upload\n\n\nRun \nfloyd data upload\n\n\n\n\nYour dataset will be versioned for you, so you can still reference the old one\nif you'd like. Datasets will be named with sequential numbers, like this:\n\n\n\n\nmckay/datasets/foo/1\n\n\nmckay/datasets/foo/2\n\n\nmckay/datasets/foo/3\n\n\n...\n\n\n\n\nWhen using a dataset in a job, be sure to reference to the dataset version that\nyour job needs.\n\n\nUnderstanding the Upload Process\n\n\nWhen you upload a dataset to FloydHub, Floyd CLI compresses and zips your data\nbefore securely transferring it to FloydHub's servers over the Internet. Once\nyour dataset has been uploaded, FloyHub decompresses and unzips your dataset\nfor you. If you have a large dataset, unpacking your data on FloydHub's servers\ncan take a while.\n\n\nYou can check the status of your upload using \nfloyd data status\n with the name\nof your dataset, as shown below:\n\n\n$ floyd data status mckay/datasets/mnist/1\nDATA NAME                    CREATED        STATUS    DISK USAGE\n---------------------------  -------------  --------  ------------\nmckay/datasets/mnist/1       \n3\n minutes ago  valid     \n82\n.96 MB\n\n\n\n\nvalid\n is the state you're looking for. That means that your dataset has finished being unpacked and is ready to use.\n\n\n\n\nGood to Know\n\n\nIt will not save you time to compress your dataset before uploading it,\nsince Floyd CLI already compresses your dataset to minimize upload time.\n\n\n\n\nDownload large datasets directly to FloydHub from the internet\n\n\nOften times, it might not be practical to upload datasets to FloydHub from your local machine. For example, your upload speeds might be too slow, or you just don't want to download a large dataset from the internet just to upload it again.\n\n\nIf your data is already available on the internet, then you can create a dataset directly on FloydHub.\n\n\nStep 1: Run a terminal on FloydHub servers using Jupyter mode\n\n\nYou can create a terminal session on FloydHub. Here are the quick steps:\n\n\n\n\nRun a Jupyter Notebook job using a CPU instance\n\n\n\n\n$ floyd run --mode jupyter\n\n\n\n\n\n\nOnce your Jupyter server starts, create a Terminal\n\n\n\n\n\n\n\n\nOnce you're in the terminal, you'll automatically be in the \n/output\n directory, but you can always confirm with the \npwd\n command. From here, you can download your data to your FloydHub instance.\n\n\n\n\nHere is an example that downloads a CSV with details about members of the United States Congress\n\n\n$ mkdir congress\n$ \ncd\n congress/\n$ wget https://theunitedstates.io/congress-legislators/legislators-current.csv\n\n\n\n\n\n\nPost process your data (if necessary)\n\n\n\n\nFor example, if the file that you downloaded is a tar file, you can untar it here. Or you can download multiple files and organize them here. Or you could open up a Jupyter notebook within this session and transform your data even further. Just make sure to clean up the \n/output\n directory so that only the files that you want in your dataset are present there.\n\n\nUntar the files to the current dir\n\n$ tar xvzf train-images-idx3-ubyte.gz\n\nRemove the tar file\n\n$ rm -rf train-images-idx3-ubyte.gz\n\nEnsure that only the files you want are present in `/output`\n\n$ ls /output\n\n\n\n\nStep 2: Stop the Jupyter Notebook session and create a dataset from the job's output\n\n\nNavigate to your current job's page on FloydHub and click the Cancel button to stop this active Jupyter session. Once the job has been shut down, you can click the \nCreate Dataset\n button on the \nOutput\n tab to open a modal that will help you turn this output into a FloydHub dataset.\n\n\n\n\nThe modal will ask if you'd like to copy this output to one of your existing datasets or create a new dataset entirely.\n\n\n\n\nClick the \nCreate Dataset from Output\n button once you're ready, and you'll be navigated to your newly created Dataset on FloydHub.", 
            "title": "Create and Upload a Dataset"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/#create-a-new-dataset", 
            "text": "A  Dataset  is a collection of data. If you have used Github, datasets in\nFloydHub are a lot like code repositories, except they are for storing and\nversioning data.  To create a new Dataset, visit www.floydhub.com/datasets  and click on the\n\"New Dataset\" button on the top right hand corner.   Give the dataset a name and an apt description.  The  Visibility  field indicates who can see your dataset. If you set it to Public , anyone can see your dataset and data versions. If you are working on\nan open source project, this is a great way to share and contribute to the\nFloydHub community. If your data is proprietary, please select  Private . This\nwill ensure that only you and your team will have access to this dataset.  The section below shows how to upload a dataset from your local machine. If your\ndata is available on the internet, you can can  create a dataset out of it directly .", 
            "title": "Create a new Dataset"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/#upload-a-dataset", 
            "text": "Once you have created a dataset, you can upload data from your terminal using\nthe  floyd data  command:   floyd data init  dataset_name  floyd data upload   For example:  $ floyd data init imagenet-2017\nDataset  imagenet-2017  initialized in current directory\n...\n$ floyd data upload\nCompressing data...   Note  Depending on the size of your dataset and the speed of your internet\nconnection, uploading a dataset can take a while.", 
            "title": "Upload a Dataset"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/#resuming-an-upload", 
            "text": "Dataset uploads are resumable. If your Internet connection cuts out during an\nupload, you'll be able to resume it later if you choose to.  If your upload has stopped before it completing, resume it using the  --resume \nor  -r  flag:  $ floyd data upload --resume\nUploading compressed data. Total upload size:  74 .0MiB [=                                 ]   4194304 /77626756 -  00 :00:00  If you don't pass the  --resume  flag, but you have an unfinished upload, you\nwill be prompted to specify whether or not you'd like to resume the previous\nupload:  $ floyd data upload\nAn unfinished upload exists. Would you like to resume it?  [ y/N ] : N\nCompressing data...", 
            "title": "Resuming an Upload"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/#updatingversioning-your-dataset", 
            "text": "If you've made changes to your dataset and would like to upload it again, use\nthe following steps. You'll notice they are the same as uploading your dataset\nthe first time:   cd  into your dataset's directory  Run  floyd data init  dataset_name  to prepare to upload  Run  floyd data upload   Your dataset will be versioned for you, so you can still reference the old one\nif you'd like. Datasets will be named with sequential numbers, like this:   mckay/datasets/foo/1  mckay/datasets/foo/2  mckay/datasets/foo/3  ...   When using a dataset in a job, be sure to reference to the dataset version that\nyour job needs.", 
            "title": "Updating/Versioning Your Dataset"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/#understanding-the-upload-process", 
            "text": "When you upload a dataset to FloydHub, Floyd CLI compresses and zips your data\nbefore securely transferring it to FloydHub's servers over the Internet. Once\nyour dataset has been uploaded, FloyHub decompresses and unzips your dataset\nfor you. If you have a large dataset, unpacking your data on FloydHub's servers\ncan take a while.  You can check the status of your upload using  floyd data status  with the name\nof your dataset, as shown below:  $ floyd data status mckay/datasets/mnist/1\nDATA NAME                    CREATED        STATUS    DISK USAGE\n---------------------------  -------------  --------  ------------\nmckay/datasets/mnist/1        3  minutes ago  valid      82 .96 MB  valid  is the state you're looking for. That means that your dataset has finished being unpacked and is ready to use.   Good to Know  It will not save you time to compress your dataset before uploading it,\nsince Floyd CLI already compresses your dataset to minimize upload time.", 
            "title": "Understanding the Upload Process"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/#download-large-datasets-directly-to-floydhub-from-the-internet", 
            "text": "Often times, it might not be practical to upload datasets to FloydHub from your local machine. For example, your upload speeds might be too slow, or you just don't want to download a large dataset from the internet just to upload it again.  If your data is already available on the internet, then you can create a dataset directly on FloydHub.", 
            "title": "Download large datasets directly to FloydHub from the internet"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/#step-1-run-a-terminal-on-floydhub-servers-using-jupyter-mode", 
            "text": "You can create a terminal session on FloydHub. Here are the quick steps:   Run a Jupyter Notebook job using a CPU instance   $ floyd run --mode jupyter   Once your Jupyter server starts, create a Terminal     Once you're in the terminal, you'll automatically be in the  /output  directory, but you can always confirm with the  pwd  command. From here, you can download your data to your FloydHub instance.   Here is an example that downloads a CSV with details about members of the United States Congress  $ mkdir congress\n$  cd  congress/\n$ wget https://theunitedstates.io/congress-legislators/legislators-current.csv   Post process your data (if necessary)   For example, if the file that you downloaded is a tar file, you can untar it here. Or you can download multiple files and organize them here. Or you could open up a Jupyter notebook within this session and transform your data even further. Just make sure to clean up the  /output  directory so that only the files that you want in your dataset are present there.  Untar the files to the current dir\n\n$ tar xvzf train-images-idx3-ubyte.gz\n\nRemove the tar file\n\n$ rm -rf train-images-idx3-ubyte.gz\n\nEnsure that only the files you want are present in `/output`\n\n$ ls /output", 
            "title": "Step 1: Run a terminal on FloydHub servers using Jupyter mode"
        }, 
        {
            "location": "/guides/create_and_upload_dataset/#step-2-stop-the-jupyter-notebook-session-and-create-a-dataset-from-the-jobs-output", 
            "text": "Navigate to your current job's page on FloydHub and click the Cancel button to stop this active Jupyter session. Once the job has been shut down, you can click the  Create Dataset  button on the  Output  tab to open a modal that will help you turn this output into a FloydHub dataset.   The modal will ask if you'd like to copy this output to one of your existing datasets or create a new dataset entirely.   Click the  Create Dataset from Output  button once you're ready, and you'll be navigated to your newly created Dataset on FloydHub.", 
            "title": "Step 2: Stop the Jupyter Notebook session and create a dataset from the job's output"
        }, 
        {
            "location": "/guides/delete_dataset/", 
            "text": "To delete a dataset, click the \nDelete dataset\n button on the \nSettings\n tab\nof the dataset on the web dashboard.\n\n\nExample: \nhttps://www.floydhub.com/alice/datasets/quick-start/settings\n\n\n\n\n\n\n\n\nImportant\n\n\nDeleting a dataset will delete all its individual versions of data. This\n\ncannot\n be restored. Please be absolutely sure you want to delete a\ndataset before proceeding.\n\n\n\n\nWe recommend deleting individual data versions rather than the entire dataset.\n\n\nDeleting an uploaded datasource\n\n\nYou can delete a particular version of a Dataset by clicking on \nDelete data\n\nbutton on the Settings tab of the data's page on the web dashboard.\n\n\nExample: \nhttps://www.floydhub.com/alice/datasets/quick-start/1/settings\n\n\n\n\nDeleting an uploaded datasource from CLI\n\n\nYou can also delete an uploaded datasource from the CLI using the \nfloyd data\ndelete\n command.\n\n\n$ floyd data delete alice/datasets/quick-start/1\n\nDelete Data: alice/quick-start/1? \n[\ny/N\n]\n: y\nData alice/datasets/quick-start/1: Deleted", 
            "title": "Delete a Dataset"
        }, 
        {
            "location": "/guides/delete_dataset/#deleting-an-uploaded-datasource", 
            "text": "You can delete a particular version of a Dataset by clicking on  Delete data \nbutton on the Settings tab of the data's page on the web dashboard.  Example:  https://www.floydhub.com/alice/datasets/quick-start/1/settings", 
            "title": "Deleting an uploaded datasource"
        }, 
        {
            "location": "/guides/delete_dataset/#deleting-an-uploaded-datasource-from-cli", 
            "text": "You can also delete an uploaded datasource from the CLI using the  floyd data\ndelete  command.  $ floyd data delete alice/datasets/quick-start/1\n\nDelete Data: alice/quick-start/1?  [ y/N ] : y\nData alice/datasets/quick-start/1: Deleted", 
            "title": "Deleting an uploaded datasource from CLI"
        }, 
        {
            "location": "/guides/data/mounting_data/", 
            "text": "In this guide, we will explain how to attach one or more datasets to a job.\nFirst, let's review some basics about FloydHub datasets.\n\n\nDatasets\n\n\nA Floyd dataset is a directory (folder) of data that can be used during a\njob. To create a new dataset, please follow\n\nthis guide\n. You can view the\ndatasets you have created in the datasets page in the dashboard. You can also\nview public datasets by searching for them on FloydHub.\n\n\nWhy keep data separate from code?\n\n\nA data scientist tweaks his/her code often during the process of creating a\ndeep-learning model. However, he/she doesn't change the underlying data nearly\nas often, if at all.\n\n\nEach time you run a job on FloydHub, a copy of your code is uploaded to\nFloydHub and run on one of FloyHub's powerful deep-learning servers. Because\nyour data isn't changing from job to job, it wouldn't make sense to keep your\ndata with your code and upload it with each job. Instead, we upload a\ndataset once, and attach, or \"mount\", it to each job. This saves a\nsignificant amount of time on each job.\n\n\nBeyond that, keeping data separate from code allows you to collaborate more\neasily with others. A dataset can be used by any user who has access to it, so\nteams and communities can work on solving problems together using the same\nunderlying data.\n\n\nMounting a Dataset\n\n\nWhat does it mean to \"mount\" a dataset to a job?\n\n\nIn the world of file systems, the term \"mount\" means to attach one file system\nor folder to another file system. For example, when you insert a flash drive\n(which is a mini file system) into your computer, its file system gets mounted\nto your computer's main file system so that you can retrieve, remove, or save\ndata to the flash drive. Once the flash drive is mounted to your computer's\nfile system, other programs can access it as if it were a native part of your\ncomputer's file system. This same mounting pattern is how datasets are handled\non FloydHub.\n\n\nWhen you use the \nfloyd run\n command to run a job, your code will be sent up to\nFloydHub and run on a powerful deep-learning server that can run your job. If\nyou want your code to be able to access a dataset during the job, you need to\nmount the dataset to the server where the job is running (just like you need to\nplug a flash drive into your computer in order to access its files). Mounting\ndatasets to a job is easy: just pass the \n--data\n flag to the \nfloyd run\n cli\ncommand as detailed below.\n\n\nThe \n--data\n flag\n\n\nTo properly use the \n--data\n flag with \nfloyd run\n, you need to specify two\nthings:\n\n\n\n\nThe name of the dataset you want to mount. (Note: this command can take \nshortened dataset and output names\n)\n\n\nA name for the folder where the data will be\n  accessible to your code during the job, we call this the \"mount point\". You\n  can give this folder (mount point) any name you want.\n\n\n\n\nThese two things are separated by a \n:\n with no spaces. Here's the syntax:\n\n\nfloyd run --data \ndata_set_name\n:\nmount_point\n \u2026(rest of run command)\n\n\n\n\nLet's go through a couple of examples to show how to mount one or more datasets to your job.\n\n\nExample 1\n\n\nThe command below will mount FloydHub's public\n\nUdacity GAN\n\n dataset at \n/my_data\n\n\nfloyd run --data floydhub/datasets/udacity-gan/1:/my_data \npython my_script.py\n\n\n\n\nA couple of things to note:\n\n\n\n\nThere is no space between the name of the dataset\n    (\nfloydhub/datasets/udacity-gan/1\n) and the mount point name (\n/my_data\n).\n\n\nA colon (\n:\n) is used to separate the name of the dataset and the\n    mount point.\n\n\nDatasets are always mounted at the root directory (\n/\n). This means that if\n    you specify \n/foo\n as the mountpoint, your data will be mounted at \n/foo\n.\n    If you specify \nfoo\n as the mount point, your data will still be mounted\n    at \n/foo\n. Preceding the mount point with a \n/\n is optional\u2012your data will\n    be mounted at the same location either way. You'll see us use both\n    variations in this guide.\n\n\nNested mount points are not supported. This means mount points like\n    \nmy_data/foo\n or \n/home/me/data\n will not work. If you need your data\n    available at a nested directory, check out the \nSymlinking mounted\n    data\n guide.\n\n\n\n\n\n\nImportant\n\n\nA common mistake is to for code to reference a mounted dataset without a\npreceding \n/\n. If you specify \nmy_data\n as the mount point for your\ndataset, your code needs to look in \n/my_data\n to find the dataset. Without\nthe preceding \n/\n, your code will look for the dataset in the wrong place\n(the directory the code is running in). In your code, always precede your\nreferences to the mount point with a \n/\n.\n\n\n\n\nExample 2\n\n\nThis example spins up a Jupyter Notebook and mounts the\n\nVGG 19-layers\n\ndataset under \n/vgg\n:\n\nfloyd run --data floydhub/datasets/vgg-ilsvrc-19-layers/1:vgg --mode jupyter\n\n\nThe Jupyter Notebook will have access to the VGGNet pre-trained models under\n\n/vgg\n.\n\n\n\n\nMounting the output of another job\n\n\nYou can link jobs by mounting the output of one job as the input of a new job.\nThis allows you to iterate on the ouput of a past job.\n\n\nYou can refer to the output of a job by its name with \n/output\n appended to it.\nFor example: \nmckay/projects/quick-start/1/output\n refers to\nthe output of the job \nmckay/projects/quick-start/1\n\n\nUse the \n--data\n flag in the \nfloyd run\n command, mount past output to a job,\njust as you would to mount a dataset. For example:\n\n\n$ floyd run \n\\\n\n  --data mckay/projects/quick-start/1/output:/model\n  \npython eval.py\n\n\n\n\n\nThis will make the output of \nmckay/projects/quick-start/1\n\navailable at \n/model\n for the new job to use.\n\n\nNote: You need to have access to a job to be able to mount its output.\n\n\nMounting multiple datasources\n\n\nYou can attach up to five datasources (datasets and/or job outputs) to a job\nusing the \n--data\n flag in the \nfloyd run\n command. Ensure that each mount\npoint is unique.\n\n\n$ floyd run \n\\\n\n  --data udacity/datasets/mnist/1:digits \n\\\n\n  --data udacity/datasets/celeba/1:celeb \n\\\n\n  \npython script.py\n\n\n\n\n\nIn this case, the above datasets will be mounted at \n/digits\n and \n/celeb\n,\nrespectively.\n\n\nViewing mounted datasets in the web dashboard\n\n\nYou can view the mounted datasets and their respective mount points for a\nspecific job by going to the Data tab:\n\n\n\n\nSymlinking your mounted data\n\n\nFloydhub's basic data-mounting functionality is sufficient for most users'\nneeds. However, if you find yourself with more complex requirements, symlinking\ncan almost certainly provide a solution.\n\n\nHere are some common FloydHub data-mounting needs that symlinking can solve:\n\n\n\n\nCode requires the data to be available at a location that is not valid with\n    the mounting syntax of \nfloyd run --data\n.\n\n\nMultiple mounted datasources need to be available under a single directory.\n\n\nDirectories in a single datasource need to be split into their own\n    locations.\n\n\n\n\nFor documentation on symlinking, please see this guide: \nSymlinking mounted\ndata\n\n\nUnderstanding dataset names\n\n\nThe full name of a datasource (\nusername\n/datasets/\ndataset_name\n/\nversion\n)\nconsists of three parts:\n\n\n\n\nUsername\n\n\nDataset Name\n\n\nVersion\n\n\n\n\nFor example: \nudacity/datasets/mnist/1\n\n\nDefault mount points\n\n\nWe highly recommend that you explicitly specify the mount points for your data\nusing the \n--data \ndata_name\n:\nmount_point\n convention.\n\n\nIf, however, you do not specify a mount point, the default values are:\n\n\n\n\n\n\nSingle data mount: If you only mount one datasource without specifying a\n  mount point, it is mounted at \n/input\n\n\n\n\n\n\nMultiple data mounts: If you mount multiple datasource without specifying\n  mount points, they will each be mounted under their respective GUIDs (e.g.\n  \n/xKduBzTr4LAsc6eVPZVPVd\n). GUIDs are 32-character random strings that\n  difficult to track down, so we highly discourage this pattern.\n\n\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Mount Data to a Job"
        }, 
        {
            "location": "/guides/data/mounting_data/#datasets", 
            "text": "A Floyd dataset is a directory (folder) of data that can be used during a\njob. To create a new dataset, please follow this guide . You can view the\ndatasets you have created in the datasets page in the dashboard. You can also\nview public datasets by searching for them on FloydHub.", 
            "title": "Datasets"
        }, 
        {
            "location": "/guides/data/mounting_data/#why-keep-data-separate-from-code", 
            "text": "A data scientist tweaks his/her code often during the process of creating a\ndeep-learning model. However, he/she doesn't change the underlying data nearly\nas often, if at all.  Each time you run a job on FloydHub, a copy of your code is uploaded to\nFloydHub and run on one of FloyHub's powerful deep-learning servers. Because\nyour data isn't changing from job to job, it wouldn't make sense to keep your\ndata with your code and upload it with each job. Instead, we upload a\ndataset once, and attach, or \"mount\", it to each job. This saves a\nsignificant amount of time on each job.  Beyond that, keeping data separate from code allows you to collaborate more\neasily with others. A dataset can be used by any user who has access to it, so\nteams and communities can work on solving problems together using the same\nunderlying data.", 
            "title": "Why keep data separate from code?"
        }, 
        {
            "location": "/guides/data/mounting_data/#mounting-a-dataset", 
            "text": "", 
            "title": "Mounting a Dataset"
        }, 
        {
            "location": "/guides/data/mounting_data/#what-does-it-mean-to-mount-a-dataset-to-a-job", 
            "text": "In the world of file systems, the term \"mount\" means to attach one file system\nor folder to another file system. For example, when you insert a flash drive\n(which is a mini file system) into your computer, its file system gets mounted\nto your computer's main file system so that you can retrieve, remove, or save\ndata to the flash drive. Once the flash drive is mounted to your computer's\nfile system, other programs can access it as if it were a native part of your\ncomputer's file system. This same mounting pattern is how datasets are handled\non FloydHub.  When you use the  floyd run  command to run a job, your code will be sent up to\nFloydHub and run on a powerful deep-learning server that can run your job. If\nyou want your code to be able to access a dataset during the job, you need to\nmount the dataset to the server where the job is running (just like you need to\nplug a flash drive into your computer in order to access its files). Mounting\ndatasets to a job is easy: just pass the  --data  flag to the  floyd run  cli\ncommand as detailed below.", 
            "title": "What does it mean to \"mount\" a dataset to a job?"
        }, 
        {
            "location": "/guides/data/mounting_data/#the-data-flag", 
            "text": "To properly use the  --data  flag with  floyd run , you need to specify two\nthings:   The name of the dataset you want to mount. (Note: this command can take  shortened dataset and output names )  A name for the folder where the data will be\n  accessible to your code during the job, we call this the \"mount point\". You\n  can give this folder (mount point) any name you want.   These two things are separated by a  :  with no spaces. Here's the syntax:  floyd run --data  data_set_name : mount_point  \u2026(rest of run command)  Let's go through a couple of examples to show how to mount one or more datasets to your job.", 
            "title": "The --data flag"
        }, 
        {
            "location": "/guides/data/mounting_data/#example-1", 
            "text": "The command below will mount FloydHub's public Udacity GAN \n dataset at  /my_data  floyd run --data floydhub/datasets/udacity-gan/1:/my_data  python my_script.py   A couple of things to note:   There is no space between the name of the dataset\n    ( floydhub/datasets/udacity-gan/1 ) and the mount point name ( /my_data ).  A colon ( : ) is used to separate the name of the dataset and the\n    mount point.  Datasets are always mounted at the root directory ( / ). This means that if\n    you specify  /foo  as the mountpoint, your data will be mounted at  /foo .\n    If you specify  foo  as the mount point, your data will still be mounted\n    at  /foo . Preceding the mount point with a  /  is optional\u2012your data will\n    be mounted at the same location either way. You'll see us use both\n    variations in this guide.  Nested mount points are not supported. This means mount points like\n     my_data/foo  or  /home/me/data  will not work. If you need your data\n    available at a nested directory, check out the  Symlinking mounted\n    data  guide.    Important  A common mistake is to for code to reference a mounted dataset without a\npreceding  / . If you specify  my_data  as the mount point for your\ndataset, your code needs to look in  /my_data  to find the dataset. Without\nthe preceding  / , your code will look for the dataset in the wrong place\n(the directory the code is running in). In your code, always precede your\nreferences to the mount point with a  / .", 
            "title": "Example 1"
        }, 
        {
            "location": "/guides/data/mounting_data/#example-2", 
            "text": "This example spins up a Jupyter Notebook and mounts the VGG 19-layers \ndataset under  /vgg : floyd run --data floydhub/datasets/vgg-ilsvrc-19-layers/1:vgg --mode jupyter \nThe Jupyter Notebook will have access to the VGGNet pre-trained models under /vgg .", 
            "title": "Example 2"
        }, 
        {
            "location": "/guides/data/mounting_data/#mounting-the-output-of-another-job", 
            "text": "You can link jobs by mounting the output of one job as the input of a new job.\nThis allows you to iterate on the ouput of a past job.  You can refer to the output of a job by its name with  /output  appended to it.\nFor example:  mckay/projects/quick-start/1/output  refers to\nthe output of the job  mckay/projects/quick-start/1  Use the  --data  flag in the  floyd run  command, mount past output to a job,\njust as you would to mount a dataset. For example:  $ floyd run  \\ \n  --data mckay/projects/quick-start/1/output:/model\n   python eval.py   This will make the output of  mckay/projects/quick-start/1 \navailable at  /model  for the new job to use.  Note: You need to have access to a job to be able to mount its output.", 
            "title": "Mounting the output of another job"
        }, 
        {
            "location": "/guides/data/mounting_data/#mounting-multiple-datasources", 
            "text": "You can attach up to five datasources (datasets and/or job outputs) to a job\nusing the  --data  flag in the  floyd run  command. Ensure that each mount\npoint is unique.  $ floyd run  \\ \n  --data udacity/datasets/mnist/1:digits  \\ \n  --data udacity/datasets/celeba/1:celeb  \\ \n   python script.py   In this case, the above datasets will be mounted at  /digits  and  /celeb ,\nrespectively.", 
            "title": "Mounting multiple datasources"
        }, 
        {
            "location": "/guides/data/mounting_data/#viewing-mounted-datasets-in-the-web-dashboard", 
            "text": "You can view the mounted datasets and their respective mount points for a\nspecific job by going to the Data tab:", 
            "title": "Viewing mounted datasets in the web dashboard"
        }, 
        {
            "location": "/guides/data/mounting_data/#symlinking-your-mounted-data", 
            "text": "Floydhub's basic data-mounting functionality is sufficient for most users'\nneeds. However, if you find yourself with more complex requirements, symlinking\ncan almost certainly provide a solution.  Here are some common FloydHub data-mounting needs that symlinking can solve:   Code requires the data to be available at a location that is not valid with\n    the mounting syntax of  floyd run --data .  Multiple mounted datasources need to be available under a single directory.  Directories in a single datasource need to be split into their own\n    locations.   For documentation on symlinking, please see this guide:  Symlinking mounted\ndata", 
            "title": "Symlinking your mounted data"
        }, 
        {
            "location": "/guides/data/mounting_data/#understanding-dataset-names", 
            "text": "The full name of a datasource ( username /datasets/ dataset_name / version )\nconsists of three parts:   Username  Dataset Name  Version   For example:  udacity/datasets/mnist/1", 
            "title": "Understanding dataset names"
        }, 
        {
            "location": "/guides/data/mounting_data/#default-mount-points", 
            "text": "We highly recommend that you explicitly specify the mount points for your data\nusing the  --data  data_name : mount_point  convention.  If, however, you do not specify a mount point, the default values are:    Single data mount: If you only mount one datasource without specifying a\n  mount point, it is mounted at  /input    Multiple data mounts: If you mount multiple datasource without specifying\n  mount points, they will each be mounted under their respective GUIDs (e.g.\n   /xKduBzTr4LAsc6eVPZVPVd ). GUIDs are 32-character random strings that\n  difficult to track down, so we highly discourage this pattern.", 
            "title": "Default mount points"
        }, 
        {
            "location": "/guides/data/mounting_data/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/", 
            "text": "Floydhub's basic data-mounting functionality is sufficient for most users'\nneeds. However, if you find yourself with more complex requirements, symlinking\ncan almost certainly provide a solution.\n\n\nHere are some common FloydHub data-mounting needs that symlinking can solve:\n\n\n\n\nCode requires the data to be available at a location that is not valid with\n    the mounting syntax of \nfloyd run --data\n.\n\n\nMultiple mounted datasources need to be available under a single directory.\n\n\nDirectories in a single datasource need to be split into their own\n    locations.\n\n\n\n\nSimply copying the data from one location to another during your job would also\nsolve these problems, but copying data is slow and inefficient, especially for\nlarge datasets. Symlinks can be created very quickly and are a much more ideal\nsolution.\n\n\nCreating symlinks\n\n\nYou can create symlinks on FloydHub's deep-learning servers during your job\nusing the same \nln\n command available on *nix operating systems (like Linux\nand MacOS). To create a symlink during a job you'll need to send the \nln\n\ncommand to the server's operating system. If you already know how to create\nsymlinks on a *nix OS, you can skip forward to the \nWays to use the \nln\n\ncommand with FloydHub section\n. If\nyou don't know much about symlinks, read on for a quick primer.\n\n\nIntro to symlinks\n\n\nYou can think of a symlink as an alias for a file or directory that allows\nprograms to find the file or directory at more than one location on the file\nsystem. Calling a symlink an \"alias\" isn't a technically correct way to refer\nto it, but we'll sometimes use that term in this guide because it can be more\nfitting and easier to internalize and remember.\n\n\nSymlink (\nln\n) syntax\n\n\nCreating a symlink on a *nix OS uses the \nln\n command. Here is the syntax:\n\n$ ln -s \nTARGET\n \nLINK_PATH_WITH_OPTIONAL_NAME\n\n\n\n\nWhere \nTARGET\n is the path of the existing file or directory you want to\ncreate an alias/symlink for, and \nLINK_PATH_WITH_OPTIONAL_NAME\n is the path\nand (optional) name of the new symlink/alias of the \nTARGET\n.\n\n\nIf \nLINK_PATH_WITH_OPTIONAL_NAME\n doesn't include a name, the name of the\n\nTARGET\n will be used. Let's go through a couple of examples to clarify that.\nAssuming you want to create an alias/symlink for \n/my_data\n and have already\ncreated a new directory called \n/existing_dir\n:\n\n\n\n\nSpecifying a name in \nLINK_PATH_WITH_OPTIONAL_NAME\n:\n    \n$ ln -s /my_data /existing_dir/new_name_for_my_data\n`\n\n\n\n    The new name here is \nnew_name_for_my_data\n. Given the command above, your\n    data will be accessible at \n/my_data\n and\n    \n/existing_dir/new_name_for_my_data\n.\n\n\nWithout specifying a name in \nLINK_PATH_WITH_OPTIONAL_NAME\n:\n    \n$ ln -s /my_data /existing_dir\n`\n\n\n\n    Because no new name was given, the name of the \nTARGET\n (which is\n    \nmy_data\n) will be used. The data will accessible at \n/my_data\n and\n    \n/existing_dir/my_data\n.\n\n\n\n\nHere are some notes/gotchas about creating symlinks using \nln\n:\n\n\n\n\nAlways use absolute paths for both the \nTARGET\n and\n    \nLINK_PATH_WITH_OPTIONAL_NAME\n parameters. This will ensure you don't run\n    into some odd behaviors that can manifest when using relative paths.\n\n\nThe directory in which the \nLINK_PATH_WITH_OPTIONAL_NAME\n terminates must\n    already exist. For example, if you want to make the data located at\n    \n/my_data\n to be aliased/symlinked at \n/home/me/foo_data\n, you first need\n    to create the \n/home/me\n directory.The following commands would\n    successfully implement this goal:\n    \n# First we make sure the /home/me directory exists:\n$ mkdir -p /home/me\n\n# Now we are ready to create the symlink. We\nll supply the name foo_data\n# to the \nLINK_PATH_WITH_OPTIONAL_NAME\n parameter:\n$ ln -s /my_data /home/me/foo_data\n\n\n\n\n\nWays to use the \nln\n command with FloydHub\n\n\nTo send the \nln\n command to the server's OS to create a symlink during your\njob, you can follow one of at least a few approaches:\n\n\n1. Using the \n[COMMAND]\n portion of \nfloyd run\n\n\nWith this approach, we add the \nln\n calls to the \n[COMMAND]\n portion of the\ncall to \nfloyd run [OPTIONS] [COMMAND]\n (see the \nfloyd run\n syntax\n\nhere\n). This is the most straight-forward approach, but\nit can it can get a bit unwieldy if you have more complex needs.\n\n\nExample 1\n\n\nLet's say you have two datasources mounted under \n/train\n and \n/test\n,\nrespectively. Your Python script \ntrain_and_eval.py\n expects both the\ndatasources to be available under the same parent directory, say \n/data/train\n\nand \n/data/test\n. You can symlink the datasources to those locations.\n\n\nIn the example below, we create a directory called \n/data\n, and then create\nlinks inside of it to our datasets, which are at \n/train\n and \n/test\n (note the\n\n--data\n flags). This means that our Python script can reference \n/data/train\n\nand \n/data/test\n and it will find our datasets.\n\n\nThe \n[COMMAND]\n portion of the \nfloyd run [OPTIONS] [COMMAND]\n in this example\nchains a series of commands, which are executed in sequence:\n\n\n\n\nmkdir -p /data\n\n\nln -s /train /data\n\n\nln -s /test /data\n\n\npython train_and_eval.py\n\n\n\n\nHere's the command in full:\n\n\n$ floyd run \n\\\n\n  --data udacity/datasets/bike-sharing-dataset/1:train \n\\\n\n  --data floydhub/datasets/mnist-test/1:test \n\\\n\n  \nmkdir -p /data \n ln -s /train /data \n ln -s /test /data \n python train_and_eval.py\n\n\n\n\n\n2. Using a bash script\n\n\nAs you can see in the previous section, using the \n[COMMAND]\n portion of \nfloydrun [OPTIONS] [COMMAND]\n can get unwieldy when there are many commands. A\nbetter alternative is to create a bash script that creates our symlinks and\nalso kicks off our main Python script.\n\n\nIf you are not familiar with writing bash scripts, a quick search of the\nInternet can get you up to speed on the basics, but bash scripting is out of\nthe scope of this documentation.\n\n\nExample 1\n\n\nThis bash script should live in the root/top-level directory of your project.\nWe'll call ours \nrun.sh\n. Here's an example of what it might look like:\n\n\n#!/bin/bash\n\n\n\n# Create a /data directory\n\nmkdir /data\n\n\n# Symlink mounted data to their destinations\n\nln -s /train /data\nln -s /test /data\n\n\n# Execute Python script\n\npython train_and_eval.py\n\n\n\n\nLet's execute the bash script using \nfloyd run\n:\n\n\n$ floyd run \n\\\n\n  --data floydhub/datasets/imagenet-vgg-verydeep-19/1:train \n\\\n\n  --data floydhub/datasets/mnist/1:test \n\\\n\n   \nbash run.sh\n\n\n\n\n\nBecause the last line of our bash script runs our Python script, we can kick\noff our entire job by running only the bash script.\n\n\nThis is a very effective pattern if your jobs require a more complex\nsetup\u2014create a bash script that sets up your environment, and then have the\nbash script call your python script. This keeps your setup separate from your\ncode, and keeps things clean.\n\n\nExample 2\n\n\nYour data is mounted under \n/vgg\n using \n--datafloydhub/datasets/vgg-ilsvrc-19-layers/1:vgg\n. However, your code expects the\ndata to be present at \n/home/data/vgg/2017\n. Let's create a symlink to make\nyour data available at \n/home/data/vgg/2017\n. Here's an example of a bash\nscript (we'll call it \nrun.sh\n) that takes care of the symlinking and\ncalls our training script.\n\n\n#!/bin/bash\n\n\n\n# Create directory\n\nmkdir -p /home/data/vgg\n\n\n# Symlink our data at /home/data/vgg/2017\n\nln -s /vgg /home/data/vgg/2017\n\n\n# Call our training script\n\npython train.py\n\n\n\n\nNow let's tie it all together with our \nfloyd run command\n:\n\n\n$ floyd run --data floydhub/datasets/vgg-ilsvrc-19-layers/1:vgg \nbash run.sh\n\n\n\n\n\n3. For Jupyter Notebooks\n\n\nSymlinking with Jupyter Notebooks follows the same principles that a regular\njob does. Here's an example:\n\n\nExample 1\n\n\nYour data is mounted under \n/vgg\n using \n--datafloydhub/datasets/vgg-ilsvrc-19-layers/1:vgg\n, but you want your data to be\npresent at \n/home/data/vgg/2017\n. The notebook below accomplishes that with a\nsymlink:\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Symlink Mounted Data"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#creating-symlinks", 
            "text": "You can create symlinks on FloydHub's deep-learning servers during your job\nusing the same  ln  command available on *nix operating systems (like Linux\nand MacOS). To create a symlink during a job you'll need to send the  ln \ncommand to the server's operating system. If you already know how to create\nsymlinks on a *nix OS, you can skip forward to the  Ways to use the  ln \ncommand with FloydHub section . If\nyou don't know much about symlinks, read on for a quick primer.", 
            "title": "Creating symlinks"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#intro-to-symlinks", 
            "text": "You can think of a symlink as an alias for a file or directory that allows\nprograms to find the file or directory at more than one location on the file\nsystem. Calling a symlink an \"alias\" isn't a technically correct way to refer\nto it, but we'll sometimes use that term in this guide because it can be more\nfitting and easier to internalize and remember.", 
            "title": "Intro to symlinks"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#symlink-ln-syntax", 
            "text": "Creating a symlink on a *nix OS uses the  ln  command. Here is the syntax: $ ln -s  TARGET   LINK_PATH_WITH_OPTIONAL_NAME   Where  TARGET  is the path of the existing file or directory you want to\ncreate an alias/symlink for, and  LINK_PATH_WITH_OPTIONAL_NAME  is the path\nand (optional) name of the new symlink/alias of the  TARGET .  If  LINK_PATH_WITH_OPTIONAL_NAME  doesn't include a name, the name of the TARGET  will be used. Let's go through a couple of examples to clarify that.\nAssuming you want to create an alias/symlink for  /my_data  and have already\ncreated a new directory called  /existing_dir :   Specifying a name in  LINK_PATH_WITH_OPTIONAL_NAME :\n     $ ln -s /my_data /existing_dir/new_name_for_my_data `  \n    The new name here is  new_name_for_my_data . Given the command above, your\n    data will be accessible at  /my_data  and\n     /existing_dir/new_name_for_my_data .  Without specifying a name in  LINK_PATH_WITH_OPTIONAL_NAME :\n     $ ln -s /my_data /existing_dir `  \n    Because no new name was given, the name of the  TARGET  (which is\n     my_data ) will be used. The data will accessible at  /my_data  and\n     /existing_dir/my_data .   Here are some notes/gotchas about creating symlinks using  ln :   Always use absolute paths for both the  TARGET  and\n     LINK_PATH_WITH_OPTIONAL_NAME  parameters. This will ensure you don't run\n    into some odd behaviors that can manifest when using relative paths.  The directory in which the  LINK_PATH_WITH_OPTIONAL_NAME  terminates must\n    already exist. For example, if you want to make the data located at\n     /my_data  to be aliased/symlinked at  /home/me/foo_data , you first need\n    to create the  /home/me  directory.The following commands would\n    successfully implement this goal:\n     # First we make sure the /home/me directory exists:\n$ mkdir -p /home/me\n\n# Now we are ready to create the symlink. We ll supply the name foo_data\n# to the  LINK_PATH_WITH_OPTIONAL_NAME  parameter:\n$ ln -s /my_data /home/me/foo_data", 
            "title": "Symlink (ln) syntax"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#ways-to-use-the-ln-command-with-floydhub", 
            "text": "To send the  ln  command to the server's OS to create a symlink during your\njob, you can follow one of at least a few approaches:", 
            "title": "Ways to use the ln command with FloydHub"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#1-using-the-command-portion-of-floyd-run", 
            "text": "With this approach, we add the  ln  calls to the  [COMMAND]  portion of the\ncall to  floyd run [OPTIONS] [COMMAND]  (see the  floyd run  syntax here ). This is the most straight-forward approach, but\nit can it can get a bit unwieldy if you have more complex needs.", 
            "title": "1. Using the [COMMAND] portion of floyd run"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#example-1", 
            "text": "Let's say you have two datasources mounted under  /train  and  /test ,\nrespectively. Your Python script  train_and_eval.py  expects both the\ndatasources to be available under the same parent directory, say  /data/train \nand  /data/test . You can symlink the datasources to those locations.  In the example below, we create a directory called  /data , and then create\nlinks inside of it to our datasets, which are at  /train  and  /test  (note the --data  flags). This means that our Python script can reference  /data/train \nand  /data/test  and it will find our datasets.  The  [COMMAND]  portion of the  floyd run [OPTIONS] [COMMAND]  in this example\nchains a series of commands, which are executed in sequence:   mkdir -p /data  ln -s /train /data  ln -s /test /data  python train_and_eval.py   Here's the command in full:  $ floyd run  \\ \n  --data udacity/datasets/bike-sharing-dataset/1:train  \\ \n  --data floydhub/datasets/mnist-test/1:test  \\ \n   mkdir -p /data   ln -s /train /data   ln -s /test /data   python train_and_eval.py", 
            "title": "Example 1"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#2-using-a-bash-script", 
            "text": "As you can see in the previous section, using the  [COMMAND]  portion of  floydrun [OPTIONS] [COMMAND]  can get unwieldy when there are many commands. A\nbetter alternative is to create a bash script that creates our symlinks and\nalso kicks off our main Python script.  If you are not familiar with writing bash scripts, a quick search of the\nInternet can get you up to speed on the basics, but bash scripting is out of\nthe scope of this documentation.", 
            "title": "2. Using a bash script"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#example-1_1", 
            "text": "This bash script should live in the root/top-level directory of your project.\nWe'll call ours  run.sh . Here's an example of what it might look like:  #!/bin/bash  # Create a /data directory \nmkdir /data # Symlink mounted data to their destinations \nln -s /train /data\nln -s /test /data # Execute Python script \npython train_and_eval.py  Let's execute the bash script using  floyd run :  $ floyd run  \\ \n  --data floydhub/datasets/imagenet-vgg-verydeep-19/1:train  \\ \n  --data floydhub/datasets/mnist/1:test  \\ \n    bash run.sh   Because the last line of our bash script runs our Python script, we can kick\noff our entire job by running only the bash script.  This is a very effective pattern if your jobs require a more complex\nsetup\u2014create a bash script that sets up your environment, and then have the\nbash script call your python script. This keeps your setup separate from your\ncode, and keeps things clean.", 
            "title": "Example 1"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#example-2", 
            "text": "Your data is mounted under  /vgg  using  --datafloydhub/datasets/vgg-ilsvrc-19-layers/1:vgg . However, your code expects the\ndata to be present at  /home/data/vgg/2017 . Let's create a symlink to make\nyour data available at  /home/data/vgg/2017 . Here's an example of a bash\nscript (we'll call it  run.sh ) that takes care of the symlinking and\ncalls our training script.  #!/bin/bash  # Create directory \nmkdir -p /home/data/vgg # Symlink our data at /home/data/vgg/2017 \nln -s /vgg /home/data/vgg/2017 # Call our training script \npython train.py  Now let's tie it all together with our  floyd run command :  $ floyd run --data floydhub/datasets/vgg-ilsvrc-19-layers/1:vgg  bash run.sh", 
            "title": "Example 2"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#3-for-jupyter-notebooks", 
            "text": "Symlinking with Jupyter Notebooks follows the same principles that a regular\njob does. Here's an example:", 
            "title": "3. For Jupyter Notebooks"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#example-1_2", 
            "text": "Your data is mounted under  /vgg  using  --datafloydhub/datasets/vgg-ilsvrc-19-layers/1:vgg , but you want your data to be\npresent at  /home/data/vgg/2017 . The notebook below accomplishes that with a\nsymlink:", 
            "title": "Example 1"
        }, 
        {
            "location": "/guides/data/symlink_mounted_data/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/environments/", 
            "text": "Environments\n\n\nBelow is the list of Deep Learning environments supported by FloydHub. Any of\nthese can be specified in the floyd \nrun\n command using the\n\n--env\n option.\n\n\nIf no \n--env\n is provided, it uses the \nkeras\n image by default, which comes with Python\n3, Keras 2.0.4 and Tensorflow 1.1.0 pre-installed.\n\n\n\n\n\n\n\n\nFramework\n\n\nEnv name (--env parameter)\n\n\nDescription\n\n\nDocker Image\n\n\n\n\n\n\n\n\n\n\nKeras\n\n\nkeras\n\n\nTensorflow 1.1.0 + keras 2.0.6 on Python3.5.\n\n\n\n\n\n\n\n\n\n\nkeras:py2\n\n\nTensorflow 1.1.0 + keras 2.0.6 on Python2.\n\n\n\n\n\n\n\n\nTensorflow 1.4\n\n\ntensorflow-1.4\n\n\nTensorflow 1.4.0 + Keras 2.0.8 on Python3.6.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\n\n\ntensorflow-1.4:py2\n\n\nTensorflow 1.4.0 + Keras 2.0.8 on Python2.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\nTensorflow 1.3\n\n\ntensorflow-1.3\n\n\nTensorflow 1.3.0 + Keras 2.0.6 on Python3.6.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\n\n\ntensorflow-1.3:py2\n\n\nTensorflow 1.3.0 + Keras 2.0.6 on Python2.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\nTensorflow 1.2\n\n\ntensorflow-1.2\n\n\nTensorflow 1.2.0 + Keras 2.0.6 on Python3.5.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\n\n\ntensorflow-1.2:py2\n\n\nTensorflow 1.2.0 + Keras 2.0.6 on Python2.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\nTensorflow 1.1\n\n\ntensorflow\n\n\nTensorflow 1.1.0 + Keras 2.0.6 on Python3.5.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\n\n\ntensorflow:py2\n\n\nTensorflow 1.1.0 + Keras 2.0.6 on Python2.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\nTensorflow 1.0\n\n\ntensorflow-1.0\n\n\nTensorflow 1.0.0 + Keras 2.0.6 on Python3.5.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\n\n\ntensorflow-1.0:py2\n\n\nTensorflow 1.0.0 + Keras 2.0.6 on Python2.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\nTensorflow 0.12\n\n\ntensorflow-0.12\n\n\nTensorflow 0.12.1 + Keras 1.2.2 on Python3.5.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\n\n\ntensorflow-0.12:py2\n\n\nTensorflow 0.12.1 + Keras 1.2.2 on Python2.\n\n\nfloydhub/tensorflow\n\n\n\n\n\n\nPyTorch 0.2\n\n\npytorch-0.2\n\n\nPyTorch 0.2.0 on Python 3.\n\n\nfloydhub/pytorch\n\n\n\n\n\n\n\n\npytorch-0.2:py2\n\n\nPyTorch 0.2.0 on Python 2.\n\n\nfloydhub/pytorch\n\n\n\n\n\n\nPyTorch 0.1\n\n\npytorch-0.1\n\n\nPyTorch 0.1.12 on Python 3.\n\n\nfloydhub/pytorch\n\n\n\n\n\n\n\n\npytorch-0.1:py2\n\n\nPyTorch 0.1.12 on Python 2.\n\n\nfloydhub/pytorch\n\n\n\n\n\n\nTheano 0.8\n\n\ntheano-0.8\n\n\nTheano rel-0.8.2 + Keras 1.2.2 on Python3.5.\n\n\nfloydhub/theano\n\n\n\n\n\n\n\n\ntheano-0.8:py2\n\n\nTheano rel-0.8.2 + Keras 1.2.2 on Python2.\n\n\nfloydhub/theano\n\n\n\n\n\n\nTheano 0.9\n\n\ntheano-0.9\n\n\nTheano rel-0.8.2 + Keras 2.0.3 on Python3.5.\n\n\nfloydhub/theano\n\n\n\n\n\n\n\n\ntheano-0.9:py2\n\n\nTheano rel-0.8.2 + Keras 2.0.3 on Python2.\n\n\nfloydhub/theano\n\n\n\n\n\n\nCaffe\n\n\ncaffe\n\n\nCaffe rc4 on Python3.5.\n\n\nfloydhub/caffe\n\n\n\n\n\n\n\n\ncaffe:py2\n\n\nCaffe rc4 on Python2.\n\n\nfloydhub/caffe\n\n\n\n\n\n\nTorch\n\n\ntorch\n\n\nTorch 7 with Python 3 env.\n\n\nfloydhub/torch\n\n\n\n\n\n\n\n\ntorch:py2\n\n\nTorch 7 with Python 2 env.\n\n\nfloydhub/torch\n\n\n\n\n\n\nChainer 1.23\n\n\nchainer-1.23\n\n\nChainer 1.23.0 on Python 3.\n\n\nfloydhub/chainer\n\n\n\n\n\n\n\n\nchainer-1.23:py2\n\n\nChainer 1.23.0 on Python 2.\n\n\nfloydhub/chainer\n\n\n\n\n\n\nChainer 2.0\n\n\nchainer-2.0\n\n\nChainer 1.23.0 on Python 3.\n\n\nfloydhub/chainer\n\n\n\n\n\n\n\n\nchainer-2.0:py2\n\n\nChainer 1.23.0 on Python 2.\n\n\nfloydhub/chainer\n\n\n\n\n\n\nMxNet (beta)\n\n\nmxnet:py2\n\n\nMxNet 0.9.3a on Python 2.\n\n\nfloydhub/mxnet\n\n\n\n\n\n\nKur\n\n\nkur\n\n\nKur 0.3.0 on Python 3.\n\n\nfloydhub/kur\n\n\n\n\n\n\n\n\nAll environments are available for both CPU and GPU execution. For example,\n\n\nTo run a Python2 Tensorflow job on CPU\n\n$ floyd run --env tensorflow:py2 \npython mnist_cnn.py\n\n\n\n\nTo run a Python2 Tensorflow job on GPU (CUDA, cuDNN, etc. installed)\n\n$ floyd run --env tensorflow:py2 --gpu \npython mnist_cnn.py\n\n\n\n\nThe following software packages (in addition to many other common libraries) are available in all the environments:\n\nh5py, iPython, Jupyter, matplotlib, numpy, OpenCV, Pandas, Pillow, scikit-learn, scipy, sklearn\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "List of Available Environments"
        }, 
        {
            "location": "/guides/environments/#environments", 
            "text": "Below is the list of Deep Learning environments supported by FloydHub. Any of\nthese can be specified in the floyd  run  command using the --env  option.  If no  --env  is provided, it uses the  keras  image by default, which comes with Python\n3, Keras 2.0.4 and Tensorflow 1.1.0 pre-installed.     Framework  Env name (--env parameter)  Description  Docker Image      Keras  keras  Tensorflow 1.1.0 + keras 2.0.6 on Python3.5.      keras:py2  Tensorflow 1.1.0 + keras 2.0.6 on Python2.     Tensorflow 1.4  tensorflow-1.4  Tensorflow 1.4.0 + Keras 2.0.8 on Python3.6.  floydhub/tensorflow     tensorflow-1.4:py2  Tensorflow 1.4.0 + Keras 2.0.8 on Python2.  floydhub/tensorflow    Tensorflow 1.3  tensorflow-1.3  Tensorflow 1.3.0 + Keras 2.0.6 on Python3.6.  floydhub/tensorflow     tensorflow-1.3:py2  Tensorflow 1.3.0 + Keras 2.0.6 on Python2.  floydhub/tensorflow    Tensorflow 1.2  tensorflow-1.2  Tensorflow 1.2.0 + Keras 2.0.6 on Python3.5.  floydhub/tensorflow     tensorflow-1.2:py2  Tensorflow 1.2.0 + Keras 2.0.6 on Python2.  floydhub/tensorflow    Tensorflow 1.1  tensorflow  Tensorflow 1.1.0 + Keras 2.0.6 on Python3.5.  floydhub/tensorflow     tensorflow:py2  Tensorflow 1.1.0 + Keras 2.0.6 on Python2.  floydhub/tensorflow    Tensorflow 1.0  tensorflow-1.0  Tensorflow 1.0.0 + Keras 2.0.6 on Python3.5.  floydhub/tensorflow     tensorflow-1.0:py2  Tensorflow 1.0.0 + Keras 2.0.6 on Python2.  floydhub/tensorflow    Tensorflow 0.12  tensorflow-0.12  Tensorflow 0.12.1 + Keras 1.2.2 on Python3.5.  floydhub/tensorflow     tensorflow-0.12:py2  Tensorflow 0.12.1 + Keras 1.2.2 on Python2.  floydhub/tensorflow    PyTorch 0.2  pytorch-0.2  PyTorch 0.2.0 on Python 3.  floydhub/pytorch     pytorch-0.2:py2  PyTorch 0.2.0 on Python 2.  floydhub/pytorch    PyTorch 0.1  pytorch-0.1  PyTorch 0.1.12 on Python 3.  floydhub/pytorch     pytorch-0.1:py2  PyTorch 0.1.12 on Python 2.  floydhub/pytorch    Theano 0.8  theano-0.8  Theano rel-0.8.2 + Keras 1.2.2 on Python3.5.  floydhub/theano     theano-0.8:py2  Theano rel-0.8.2 + Keras 1.2.2 on Python2.  floydhub/theano    Theano 0.9  theano-0.9  Theano rel-0.8.2 + Keras 2.0.3 on Python3.5.  floydhub/theano     theano-0.9:py2  Theano rel-0.8.2 + Keras 2.0.3 on Python2.  floydhub/theano    Caffe  caffe  Caffe rc4 on Python3.5.  floydhub/caffe     caffe:py2  Caffe rc4 on Python2.  floydhub/caffe    Torch  torch  Torch 7 with Python 3 env.  floydhub/torch     torch:py2  Torch 7 with Python 2 env.  floydhub/torch    Chainer 1.23  chainer-1.23  Chainer 1.23.0 on Python 3.  floydhub/chainer     chainer-1.23:py2  Chainer 1.23.0 on Python 2.  floydhub/chainer    Chainer 2.0  chainer-2.0  Chainer 1.23.0 on Python 3.  floydhub/chainer     chainer-2.0:py2  Chainer 1.23.0 on Python 2.  floydhub/chainer    MxNet (beta)  mxnet:py2  MxNet 0.9.3a on Python 2.  floydhub/mxnet    Kur  kur  Kur 0.3.0 on Python 3.  floydhub/kur     All environments are available for both CPU and GPU execution. For example,  To run a Python2 Tensorflow job on CPU $ floyd run --env tensorflow:py2  python mnist_cnn.py   To run a Python2 Tensorflow job on GPU (CUDA, cuDNN, etc. installed) $ floyd run --env tensorflow:py2 --gpu  python mnist_cnn.py   The following software packages (in addition to many other common libraries) are available in all the environments: h5py, iPython, Jupyter, matplotlib, numpy, OpenCV, Pandas, Pillow, scikit-learn, scipy, sklearn", 
            "title": "Environments"
        }, 
        {
            "location": "/guides/environments/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/", 
            "text": "Floydhub's \nenvironments\n come with many common deep learning and machine learning packages and dependencies preinstalled. Examples of pre-installed packages include \nnumpy\n, \nscipy\n, \nOpenCV\n, \nOpenAI Gym\n, \nSpaCy\n, etc.\n\n\nIf you need additional or custom packages, you can install them before running your job.\n\n\nInstalling Python dependencies\n\n\nIf your code needs additional Python packages at run time, you can add them to\na special file named \nfloyd_requirements.txt\n. If you want to install\nnon-Python packages, please see \nhere\n.\n\n\nIt is similar to Python's \nrequirements.txt\n file and should be present in the same directory from where you issue the \nfloyd run\n command. This is a special file that will be read before your job is started and the packages listed here will be installed before running your job.\n\n\n\n\nHere is an \nfloyd_requirements.txt\n example file:\n\n\nredis\n\ntqdm\n==\n4\n.11.2\n\n\n\n\nWhen this file is present in the project's root directory, any job that is run inside this project will have the \nredis\n and \ntqdm\n (version \n4.11.2\n) packages installed and available at runtime.\n\n\nNotes\n\n\n\n\nOnly Python packages\n: This will only install Python packages available in \nPyPi\n. Please ensure that the package you are trying to install is available.\n\n\nOne package per line\n: Ensure that you have only one package per line in \nfloyd_requirements.txt\n\n\nInstalling specific versions\n: You can install specific versions of packages using the \npackage\n==\nversion\n notation. For example, an entry \ntqdm\n will install the latest version of the package, but \ntqdm==4.11.2\n will force install that specific version.\n\n\n\n\nInstalling Non-Python Dependencies\n\n\nYou might want to install non-Python packages or other packages that have custom installation steps. If you are using a Jupyter Notebook, you can follow \nthese steps\n to install arbitrary packages interactively.\n\n\nIf you are running a script using the \nfloyd run \ncommand\n command, you can do one of the following:\n\n\n\n\n\n\nInclude the installation steps in the \ncommand\n\n\nFor example, to install \nOpenAI Universe\n\nbefore running your actual script \ntrain.py\n:\n\n\n$ floyd run \ngit clone https://github.com/openai/universe.git \n cd universe \n pip install -e . \n python train.py\n\n\n\n\n\nThis will clone the Universe git repo, install it and then execute \npythontrain.py\n.\n\n\n\n\n\n\nCreate an installation script\n:\n\n\nIncluding the setup instructions in the \nrun\n command can get unwieldy very\nsoon. An alternative would be to create a bash script with the sequence of\nsetup commands (say, \nsetup.sh\n) and then execute this bash script as part\nof your \nfloyd run\n.\n\n\nHere's an example of what \nsetup.sh\n might look like:\n\n\n#!/bin/bash\n\n\ngit clone https://github.com/openai/universe.git\n\ncd\n universe\npip install -e .\n\n\n\n\nExecute the setup bash script in your \nfloyd run\n command before your\nactual job:\n\n\n$ floyd run \nbash setup.sh \n python train.py\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nYour job will run in a Linux environment. You can use the APT package\nmanager to install dependencies using \napt-get\n.\n\n\n\n\nInstalling Dependencies Inside Jupyter Notebook\n\n\nYou can install packages (Python or otherwise) interactively inside Jupyter Notebooks. To execute a non-Python command inside a Notebook, prepend it with \n!\n.\n\n\nFor example, to install \ntextblob\n, you can execute \n!pip install textblob\n inside your Notebook:\n\n\n\n\nYou can also use this method to install non-Python packages. For example, to install \nOpenAI Universe\n inside your Notebook, you can execute \n!git clone https://github.com/openai/universe.git \n cd universe \n pip install -e .\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Install Extra Dependencies"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/#installing-python-dependencies", 
            "text": "If your code needs additional Python packages at run time, you can add them to\na special file named  floyd_requirements.txt . If you want to install\nnon-Python packages, please see  here .  It is similar to Python's  requirements.txt  file and should be present in the same directory from where you issue the  floyd run  command. This is a special file that will be read before your job is started and the packages listed here will be installed before running your job.   Here is an  floyd_requirements.txt  example file:  redis tqdm == 4 .11.2  When this file is present in the project's root directory, any job that is run inside this project will have the  redis  and  tqdm  (version  4.11.2 ) packages installed and available at runtime.", 
            "title": "Installing Python dependencies"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/#notes", 
            "text": "Only Python packages : This will only install Python packages available in  PyPi . Please ensure that the package you are trying to install is available.  One package per line : Ensure that you have only one package per line in  floyd_requirements.txt  Installing specific versions : You can install specific versions of packages using the  package == version  notation. For example, an entry  tqdm  will install the latest version of the package, but  tqdm==4.11.2  will force install that specific version.", 
            "title": "Notes"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/#installing-non-python-dependencies", 
            "text": "You might want to install non-Python packages or other packages that have custom installation steps. If you are using a Jupyter Notebook, you can follow  these steps  to install arbitrary packages interactively.  If you are running a script using the  floyd run  command  command, you can do one of the following:    Include the installation steps in the  command  For example, to install  OpenAI Universe \nbefore running your actual script  train.py :  $ floyd run  git clone https://github.com/openai/universe.git   cd universe   pip install -e .   python train.py   This will clone the Universe git repo, install it and then execute  pythontrain.py .    Create an installation script :  Including the setup instructions in the  run  command can get unwieldy very\nsoon. An alternative would be to create a bash script with the sequence of\nsetup commands (say,  setup.sh ) and then execute this bash script as part\nof your  floyd run .  Here's an example of what  setup.sh  might look like:  #!/bin/bash \n\ngit clone https://github.com/openai/universe.git cd  universe\npip install -e .  Execute the setup bash script in your  floyd run  command before your\nactual job:  $ floyd run  bash setup.sh   python train.py      Note  Your job will run in a Linux environment. You can use the APT package\nmanager to install dependencies using  apt-get .", 
            "title": "Installing Non-Python Dependencies"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/#installing-dependencies-inside-jupyter-notebook", 
            "text": "You can install packages (Python or otherwise) interactively inside Jupyter Notebooks. To execute a non-Python command inside a Notebook, prepend it with  ! .  For example, to install  textblob , you can execute  !pip install textblob  inside your Notebook:   You can also use this method to install non-Python packages. For example, to install  OpenAI Universe  inside your Notebook, you can execute  !git clone https://github.com/openai/universe.git   cd universe   pip install -e .", 
            "title": "Installing Dependencies Inside Jupyter Notebook"
        }, 
        {
            "location": "/guides/jobs/installing_dependencies/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/basics/using_gpu/", 
            "text": "Running your job on CPU vs. GPU\n\n\nWhen you run a job using the \nfloyd run\n command, it is executed on a CPU instance on FloydHub's servers, by default. \n\n\n$ floyd run \npython mnist_cnn.py\n\n\n\n\n\nYou can also force your job to execute on on a CPU using the \n--cpu\n flag\n\n\n$ floyd run --cpu \npython mnist_cnn.py\n\n\n\n\n\nIf you want to run your job on a GPU, simply add the \n--gpu\n flag. Just make sure your code is optimized to use the available GPU.\n\n\n$ floyd run --gpu \npython mnist_cnn.py\n\n\n\n\n\nChecking GPU stats\n\n\nYou can check the GPU stats by running a dummy job that executes the \nnvidia-smi\n command.\n\n\n$ floyd run --gpu \nnvidia-smi\n\nSyncing code...\n...\n\n$ floyd logs -t \nJOB_NAME\n\n\nMon Jul \n31\n \n22\n:45:14 \n2017\n       \n+-----------------------------------------------------------------------------+\n\n|\n NVIDIA-SMI \n375\n.66                 Driver Version: \n375\n.66                    \n|\n\n\n|\n-------------------------------+----------------------+----------------------+\n\n|\n GPU  Name        Persistence-M\n|\n Bus-Id        Disp.A \n|\n Volatile Uncorr. ECC \n|\n\n\n|\n Fan  Temp  Perf  Pwr:Usage/Cap\n|\n         Memory-Usage \n|\n GPU-Util  Compute M. \n|\n\n\n|\n===============================\n+\n======================\n+\n======================\n|\n\n\n|\n   \n0\n  Tesla K80           Off  \n|\n \n0000\n:00:1E.0     Off \n|\n                    \n0\n \n|\n\n\n|\n N/A   43C    P8    25W / 149W \n|\n      0MiB / 11439MiB \n|\n      \n0\n%      Default \n|\n\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n\n|\n Processes:                                                       GPU Memory \n|\n\n\n|\n  GPU       PID  Type  Process name                               Usage      \n|\n\n\n|\n=============================================================================\n|\n\n\n|\n  No running processes found                                                 \n|\n\n+-----------------------------------------------------------------------------+\n\n\n\n\nIf you are using a Jupyter Notebook, you can also just execute the \n!nvidia-smi\n command inside it. (\nMake note\n of the \n!\n character at the beginning of the command)", 
            "title": "Using CPU vs GPU"
        }, 
        {
            "location": "/guides/basics/using_gpu/#running-your-job-on-cpu-vs-gpu", 
            "text": "When you run a job using the  floyd run  command, it is executed on a CPU instance on FloydHub's servers, by default.   $ floyd run  python mnist_cnn.py   You can also force your job to execute on on a CPU using the  --cpu  flag  $ floyd run --cpu  python mnist_cnn.py   If you want to run your job on a GPU, simply add the  --gpu  flag. Just make sure your code is optimized to use the available GPU.  $ floyd run --gpu  python mnist_cnn.py", 
            "title": "Running your job on CPU vs. GPU"
        }, 
        {
            "location": "/guides/basics/using_gpu/#checking-gpu-stats", 
            "text": "You can check the GPU stats by running a dummy job that executes the  nvidia-smi  command.  $ floyd run --gpu  nvidia-smi \nSyncing code...\n...\n\n$ floyd logs -t  JOB_NAME \n\nMon Jul  31   22 :45:14  2017        \n+-----------------------------------------------------------------------------+ |  NVIDIA-SMI  375 .66                 Driver Version:  375 .66                     |  | -------------------------------+----------------------+----------------------+ |  GPU  Name        Persistence-M |  Bus-Id        Disp.A  |  Volatile Uncorr. ECC  |  |  Fan  Temp  Perf  Pwr:Usage/Cap |          Memory-Usage  |  GPU-Util  Compute M.  |  | =============================== + ====================== + ====================== |  |     0   Tesla K80           Off   |   0000 :00:1E.0     Off  |                      0   |  |  N/A   43C    P8    25W / 149W  |       0MiB / 11439MiB  |        0 %      Default  | \n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+ |  Processes:                                                       GPU Memory  |  |   GPU       PID  Type  Process name                               Usage       |  | ============================================================================= |  |   No running processes found                                                  | \n+-----------------------------------------------------------------------------+  If you are using a Jupyter Notebook, you can also just execute the  !nvidia-smi  command inside it. ( Make note  of the  !  character at the beginning of the command)", 
            "title": "Checking GPU stats"
        }, 
        {
            "location": "/faqs/environments/", 
            "text": "Can I switch between Python 2 and Python 3 environments?\n\n\nMost of our Python based \nenvironments\n support both Python 2 and Python 3.\n\n\nThe default environments are Python 3. But you can use Python 2 by using the \n:py2\n tag with the environment name, when available. For example,\n\n\n\n\n\n\nTensorflow 1.2 with Python 3\n\n$ floyd run --env tensorflow-1.2\n\n\n\n\n\n\n\nTensorflow 1.2 with Python 2\n\n$ floyd run --env tensorflow-1.2:py2\n\n\n\n\n\n\n\nWe currently have Python 2 and 3 environments for Tensorflow, Theano, Caffe, PyTorch and Chainer. Please see our \ncomplete list of environments\n for more details on the \n--env\n name.", 
            "title": "Troubleshooting & FAQs"
        }, 
        {
            "location": "/faqs/environments/#can-i-switch-between-python-2-and-python-3-environments", 
            "text": "Most of our Python based  environments  support both Python 2 and Python 3.  The default environments are Python 3. But you can use Python 2 by using the  :py2  tag with the environment name, when available. For example,    Tensorflow 1.2 with Python 3 $ floyd run --env tensorflow-1.2    Tensorflow 1.2 with Python 2 $ floyd run --env tensorflow-1.2:py2    We currently have Python 2 and 3 environments for Tensorflow, Theano, Caffe, PyTorch and Chainer. Please see our  complete list of environments  for more details on the  --env  name.", 
            "title": "Can I switch between Python 2 and Python 3 environments?"
        }, 
        {
            "location": "/guides/ssh/", 
            "text": "Quick Look\n\n\n$ floyd run --mode jupyter\n\n\n\n\n\n\n\n\nYou can start a bash session into your job's environment by running your job in\nJupyter Notebook mode:\n\n\n$ floyd run --mode jupyter\nCreating project run. Total upload size: \n183\n.0B\nSyncing code ...\n\n[================================]\n \n916\n/916 - \n00\n:00:00\n\nJOB NAME\n--------------------\nmckay/projects/ssh/1\n\nSetting up your instance and waiting \nfor\n Jupyter notebook to become available\n\nPath to jupyter notebook: https://floydhub.com/notebooks/YXau92xMFUshUbMdKqKVVX\n\n\n\n\nWhen you visit your running Jupyter Notebook in the browser, click the \nNew\n\nbutton in the top right corner of the screen and select the \nTerminal\n option\nas shown below:\n\n\n\n\nThat button will launch a terminal session in your running instance with root\naccess:\n\n\n\nCurrently, FloydHub does not offer true SSH access into instances, but the\nmethod described above is sufficient for what most users request SSH for.", 
            "title": "SSH into a Job"
        }, 
        {
            "location": "/guides/data/storing_output/", 
            "text": "Overview\n\n\nSaving information generated during a job is easy.\n\n\nOn a FloydHub deep learning server your code has access to a directory called\n\n/output\n. The \n/output\n directory is a special directory that is used to store\ninformation you want to save for future use after a job finishes. Anything\nsaved in the \n/output\n directory at the time a job finishes will be preserved\nand can be accessed and reused later.\n\n\nThe most common thing users save is model checkpoints, but anything that ends\nup in the \n/output\n directory at the end of a job will be saved (use your\nimagination!).\n\n\nLet's work through a couple of examples to see how to save data during a job.\n\n\nExample 1\n\n\nThis job prints the string \"Hello, world!\", and saves it to a file called\n\nhello.txt\n. Because \nhello.txt\n is located in the \n/output\n directory, it will\nbe saved and available after the job finishes:\n\n\n$ floyd init my_awesome_hello_world_project\n$ floyd run \necho \nHello, world!\n \n /output/hello.txt\n\nSyncing code ...\n\n\n\n\n\n\nNote\n\n\nIf you are not familiar with what\n\necho \nHello, world!\n \n /output/hello.txt\n does, here's a quick\nexplanation:\n\n\n\n\nThe \necho \nHello, world!\n part outputs the string \nHello, world!\n.\n\n\nThe \n part of the command redirects the printed output of \necho \nHello, world!\n\n  (which is, of course, \nHello world!\n) to the file specified after the \n.\n\n\nThe \n/output/hello.txt\n part of the command specifies where the \nHello,world!\n should be written to: \n/output/hello.txt\n. Because \nhello.txt\n is\n  in the \n/output\n directory, it will be preserved for future reference and\n  use.\n\n\n\n\n\n\nExample 2\n\n\nIn this example, we'll use Python to save some data to a file in the \n/output\n\ndirectory. Put this code in a file named \nsave_example.py\n:\n\n\nwith\n \nopen\n(\n/output/myfile.txt\n,\n \na\n)\n \nas\n \nf\n:\n\n    \nf\n.\nwrite\n(\nPlease save me!\n\\n\n)\n\n\n\n\n\nIf you run this code locally on your computer, you'll probably get something\nlike this:\n\n\nTraceback \n(\nmost recent call last\n)\n:\n  File \nsave_example.py\n, line \n1\n, in \nmodule\n\n    with open\n(\n/output/myfile.txt\n, \na\n)\n as f:\nIOError: \n[\nErrno \n2\n]\n No such file or directory: \n/output/myfile.txt\n\n\n\n\n\nThat's because there is no \n/output\n directory on your computer. In contrast,\nevery job on FloydHub runs on a server that has a \n/output\n directory, so the\ncommand won't fail on FloydHub. Let's run it with the following commands:\n\n\n$ floyd init save_example_2\n$ floyd run \npython save_example.py\n\nCreating project run. Total upload size: \n267\n.0B\nSyncing code ...\n\n\nSuccess! We can now view the output, download it, or even use it again in\nfuture jobs.\n\n\nNow that we've completed a couple trivial examples, let's do something more\nuseful and realistic.\n\n\nExample 3\n\n\nHere is a sample Tensorflow example that saves a model checkpoint. Because we\nwrite (save) the data to the \n/output\n directory, we'll be able to use it\nlater. A future job can use this model checkpoint as a starting point.\nConsider this partial code, and note the call to \nsaver.save(sess,\n/output/model.ckpt\n)\n:\n\n\nimport\n \ntensorflow\n \nas\n \ntf\n\n\n\n...\n\n\n\nsaver\n \n=\n \ntf\n.\ntrain\n.\nSaver\n()\n\n\nwith\n \ntf\n.\nSession\n()\n \nas\n \nsess\n:\n\n    \nsess\n.\nrun\n(\ninit\n)\n\n    \n...\n\n    \nsave_path\n \n=\n \nsaver\n.\nsave\n(\nsess\n,\n \n/output/model.ckpt\n)\n\n    \nprint\n(\nModel saved in file: \n%s\n \n%\n \nsave_path\n)\n\n    \n...\n\n\n\n\n\nBecause model is stored under the special \n/output\n directory, it will be saved\neven after your job ends, and can be used again in future jobs.\n\n\nViewing Saved Output Data\n\n\nYou can view the saved output of a job using the \nfloyd output\n command:\n\n\n$ floyd output mckay/projects/quick-start/1\nOpening output directory in your browser...\n\n\n\n\nAlternatively, you can browse or download the saved output by visiting the\n\nOutput\n tab of the job on your dashboard as shown in the image below:\n\n\n\n\nUsing output as a data source\n\n\nYou can use the output of one job as the input to your next job. To see how to\nmount output data, please see \nthis guide\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Save Output"
        }, 
        {
            "location": "/guides/data/storing_output/#overview", 
            "text": "Saving information generated during a job is easy.  On a FloydHub deep learning server your code has access to a directory called /output . The  /output  directory is a special directory that is used to store\ninformation you want to save for future use after a job finishes. Anything\nsaved in the  /output  directory at the time a job finishes will be preserved\nand can be accessed and reused later.  The most common thing users save is model checkpoints, but anything that ends\nup in the  /output  directory at the end of a job will be saved (use your\nimagination!).  Let's work through a couple of examples to see how to save data during a job.", 
            "title": "Overview"
        }, 
        {
            "location": "/guides/data/storing_output/#example-1", 
            "text": "This job prints the string \"Hello, world!\", and saves it to a file called hello.txt . Because  hello.txt  is located in the  /output  directory, it will\nbe saved and available after the job finishes:  $ floyd init my_awesome_hello_world_project\n$ floyd run  echo  Hello, world!    /output/hello.txt \nSyncing code ...   Note  If you are not familiar with what echo  Hello, world!    /output/hello.txt  does, here's a quick\nexplanation:   The  echo  Hello, world!  part outputs the string  Hello, world! .  The   part of the command redirects the printed output of  echo  Hello, world! \n  (which is, of course,  Hello world! ) to the file specified after the  .  The  /output/hello.txt  part of the command specifies where the  Hello,world!  should be written to:  /output/hello.txt . Because  hello.txt  is\n  in the  /output  directory, it will be preserved for future reference and\n  use.", 
            "title": "Example 1"
        }, 
        {
            "location": "/guides/data/storing_output/#example-2", 
            "text": "In this example, we'll use Python to save some data to a file in the  /output \ndirectory. Put this code in a file named  save_example.py :  with   open ( /output/myfile.txt ,   a )   as   f : \n     f . write ( Please save me! \\n )   If you run this code locally on your computer, you'll probably get something\nlike this:  Traceback  ( most recent call last ) :\n  File  save_example.py , line  1 , in  module \n    with open ( /output/myfile.txt ,  a )  as f:\nIOError:  [ Errno  2 ]  No such file or directory:  /output/myfile.txt   That's because there is no  /output  directory on your computer. In contrast,\nevery job on FloydHub runs on a server that has a  /output  directory, so the\ncommand won't fail on FloydHub. Let's run it with the following commands:  $ floyd init save_example_2\n$ floyd run  python save_example.py \nCreating project run. Total upload size:  267 .0B\nSyncing code ... \nSuccess! We can now view the output, download it, or even use it again in\nfuture jobs.  Now that we've completed a couple trivial examples, let's do something more\nuseful and realistic.", 
            "title": "Example 2"
        }, 
        {
            "location": "/guides/data/storing_output/#example-3", 
            "text": "Here is a sample Tensorflow example that saves a model checkpoint. Because we\nwrite (save) the data to the  /output  directory, we'll be able to use it\nlater. A future job can use this model checkpoint as a starting point.\nConsider this partial code, and note the call to  saver.save(sess, /output/model.ckpt ) :  import   tensorflow   as   tf  ...  saver   =   tf . train . Saver ()  with   tf . Session ()   as   sess : \n     sess . run ( init ) \n     ... \n     save_path   =   saver . save ( sess ,   /output/model.ckpt ) \n     print ( Model saved in file:  %s   %   save_path ) \n     ...   Because model is stored under the special  /output  directory, it will be saved\neven after your job ends, and can be used again in future jobs.", 
            "title": "Example 3"
        }, 
        {
            "location": "/guides/data/storing_output/#viewing-saved-output-data", 
            "text": "You can view the saved output of a job using the  floyd output  command:  $ floyd output mckay/projects/quick-start/1\nOpening output directory in your browser...  Alternatively, you can browse or download the saved output by visiting the Output  tab of the job on your dashboard as shown in the image below:", 
            "title": "Viewing Saved Output Data"
        }, 
        {
            "location": "/guides/data/storing_output/#using-output-as-a-data-source", 
            "text": "You can use the output of one job as the input to your next job. To see how to\nmount output data, please see  this guide", 
            "title": "Using output as a data source"
        }, 
        {
            "location": "/guides/data/storing_output/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/delete_output/", 
            "text": "It is not possible to delete just the output of a job. You will have to \ndelete the job itself\n.", 
            "title": "Delete Output"
        }, 
        {
            "location": "/guides/download_output/", 
            "text": "Quick Look\n\n\nOn the job screen on floydhub.com:\n\n\n\n\nOr from the CLI:\n\n$ floyd data clone \nusername\n/\nproject_name\n/\nrun_number\n/output\n\n\n\n\n\nAfter you've \nsaved output\n from a job, you can download\nthe output of the job on floydhub.com or from the CLI.\n\n\nTo learn how to re-use output in a new job, see\n\nthis documentation\n.\n\n\nFrom floydhub.com\n\n\nFrom the job's page on floydhub.com, you can browse and download the output of\nthe job by using the \"Browse\" and \"Download\" icons found on the \"Output\" tab:\n\n\n\n\nFrom the CLI\n\n\nTo open the browsing window from the CLI, use the\n\nfloyd data clone\n command and pass it the path of the\njob's output:\n\n\n$ floyd data clone mckay/projects/quick-start/1/output\nOpening output directory in your browser ...\n\n\n\n\nThe path of the output is \nyour_username\n/projects/\nproject_name\n/\nrun_number\n/output\n.", 
            "title": "Download Saved Output"
        }, 
        {
            "location": "/guides/download_output/#from-floydhubcom", 
            "text": "From the job's page on floydhub.com, you can browse and download the output of\nthe job by using the \"Browse\" and \"Download\" icons found on the \"Output\" tab:", 
            "title": "From floydhub.com"
        }, 
        {
            "location": "/guides/download_output/#from-the-cli", 
            "text": "To open the browsing window from the CLI, use the floyd data clone  command and pass it the path of the\njob's output:  $ floyd data clone mckay/projects/quick-start/1/output\nOpening output directory in your browser ...  The path of the output is  your_username /projects/ project_name / run_number /output .", 
            "title": "From the CLI"
        }, 
        {
            "location": "/guides/browse_output/", 
            "text": "Quick Look\n\n\nOn the job screen on floydhub.com:\n\n\n\n\nOr from the CLI:\n\n$ floyd output \nusername\n/projects/\nproject_name\n/\nrun_number\n\n\n\n\n\n\nAfter you've \nsaved output\n from a job, you can browse the\noutput of the job on floydhub.com. Using the CLI, you can open to the job's\nin-browser data-browsing page.\n\n\nFor downloading output, see \nthis documentation\n.\n\n\nTo learn how to re-use output in a new job, see \nthis\ndocumentation\n.\n\n\nFrom floydhub.com\n\n\nFrom the job's page on floydhub.com, you can browse and download the output of\nthe job by using the \"Browse\" and \"Download\" icons found on the \"Output\" tab:\n\n\n\n\nFrom the CLI\n\n\nTo open the browsing window from the CLI, use the\n\nfloyd data clone\n command and pass it the path of the\njob's output:\n\n\n$ floyd output mckay/projects/quick-start/1\nOpening output directory in your browser ...\n\n\n\n\nThe path of the output is \nyour_username\n/projects/\nproject_name\n/\nrun_number\n/output\n.", 
            "title": "Browse Saved Output"
        }, 
        {
            "location": "/guides/browse_output/#from-floydhubcom", 
            "text": "From the job's page on floydhub.com, you can browse and download the output of\nthe job by using the \"Browse\" and \"Download\" icons found on the \"Output\" tab:", 
            "title": "From floydhub.com"
        }, 
        {
            "location": "/guides/browse_output/#from-the-cli", 
            "text": "To open the browsing window from the CLI, use the floyd data clone  command and pass it the path of the\njob's output:  $ floyd output mckay/projects/quick-start/1\nOpening output directory in your browser ...  The path of the output is  your_username /projects/ project_name / run_number /output .", 
            "title": "From the CLI"
        }, 
        {
            "location": "/guides/reusing_output/", 
            "text": "You can link jobs by mounting the output of one job as the input of a new job.\nThis allows you to iterate on the ouput of a past job. Output is mounted to a\njob in the same way data is. To learn more about mounting data, see \nthis\nguide\n.\n\n\nYou can refer to the output of a job by its name with \n/output\n appended to it.\nFor example: \nmckay/projects/quick-start/1/output\n refers to\nthe output of the job \nmckay/projects/quick-start/1\n\n\nUse the \n--data\n flag in the \nfloyd run\n command, mount past output to a job,\njust as you would to mount a dataset. For example:\n\n\n$ floyd run \n\\\n\n  --data mckay/projects/quick-start/1/output:/model \npython train.py\n\n\n\n\n\nThis will make the output of \nmckay/projects/quick-start/12\n\navailable at \n/model\n for the new job to use.\n\n\nNote: You need to have access to a job to be able to mount its output.", 
            "title": "Using Previous Output in a New Job"
        }, 
        {
            "location": "/guides/basics/install/", 
            "text": "Floyd CLI (\nfloyd-cli\n) is a Python-based command line tool to interact with FloydHub from your terminal.\n\n\nfloyd-cli\n is available on \npypi\n and\nruns on both Python 2.7 and Python 3.5, and works on Windows, MacOS, and Linux.\n\n\nUse \npip\n to install the CLI.\n\n\n$ pip install -U floyd-cli\n\n\n\n\nUse \npip3\n if you only want to install the CLI for Python 3:\n\n\n$ pip3 install -U floyd-cli\n\n\n\n\nAfter installation you can view the commands supported by the CLI using the\n\n--help\n option.\n\n\n$ floyd --help\nUsage: floyd \n[\nOPTIONS\n]\n COMMAND \n[\nARGS\n]\n...\n\n  Floyd CLI interacts with Floyd server and executes your commands. More\n  \nhelp\n is available under each \ncommand\n listed below.\n\n...\n\n\n\n\nDetailed documentation for the floyd commands is available in the \ndocumentation\n.\n\n\nHaving trouble?\n\n\nIf you had troubles with the above installation, consider using \nvirtualenv\n to\ninstall \nfloyd-cli\n. This solves most users' installation issues. We've\ndetailed the process below.\n\n\nUsing \nvirtualenv\n to Install \nfloyd-cli\n\n\nIf you aren't familiar with \nvirtualenv\n, you can think of it as a way to\ncreate totally fresh environments in which you can have a clean copy of Python\nand install only the packages you need for working on a certain task or\nproject. You can turn these environments on and off as you need to use them.\nThis is useful because it helps you avoid issues that arise when you need more\nthan one version of the same package on your computer. If you'd like more\ninformation on \nvirtualenv\n, check out its \nuser\nguide\n or \nthis introductory\ntutorial\n\n\nWe highly recommend using \nvirtualenv\n for installing and using \nfloyd-cli\n.\nBecause it helps avoid any library version conflicts, it results in a smoother\ninstallation process. Using \nvirtualenv\n solves most users' installation\nissues.\n\n\nStep-by-step Process of Installing \nfloyd-cli\n with \nvirtualenv\n\n\nHere is a step-by-step walkthrough of how to install \nfloyd-cli\n using\n\nvirtualenv\n. First let's use \npip\n to install \nvirtualenv\n:\n\n\n$ sudo pip install virtualenv\n\n\n\n\nCreate a virtualenv using the \nvirtualenv\n command. You need to pass a path\nwhere your environment will be located. This is both where all the information\nabout your virtualenv will be stored, as well as all of your virtualenv's\npackages. Below, we use \n~/floyd\n as the path:\n\n\n$ virtualenv ~/floyd\n\n\nThis will create a clean environment (a virtualenv) called \nfloyd\n (named after\nthe folder we specified when we created the virtualenv).\n\n\nYou can now activate and start using the virtualenv by running:\n\n\n$ \nsource\n ~/floyd/bin/activate\n\n\nWhen you want to use this virtualenv, you'll need to activate it using the same\n\nsource ~/floyd/bin/activate\n command. Each new terminal session will require\nyou to activate the virtualenv. To turn off the virtualenv, you can either exit your terminal session, or enter the \ndeactivate\n command into your terminal.\n\n\nAfter activating the virtualenv stored at \n~/floyd\n, your terminal prompt\nshould change to have \n(floyd)\n prepended to it. This serves as a reminder that\nyou have the \nfloyd\n virtualenv turned on. Since our virtualenv is new, it\ndoesn't have any packages installed in it yet. Let's install \nfloyd-cli\n:\n\n\n(\nfloyd\n)\n $ pip install -U floyd-cli\n\n\n\n\nSuccess!\n\n\nYou are now ready to use the \nfloyd commands\n. Check out our \nQuick Start tutorial\n to get started using FloydHub and \nfloyd-cli\n.\n\n\n\n\nImportant\n\n\nYou need to activate your virtualenv using the \nsource ~/floyd/bin/activate\n\ncommand above each time you open a new terminal and want to use \nfloyd-cli\n\nin it.\n\n\n\n\nHaving trouble installing the CLI?\n\n\nSee the list of \nFAQs related to installation\n.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Install Floyd CLI"
        }, 
        {
            "location": "/guides/basics/install/#having-trouble", 
            "text": "If you had troubles with the above installation, consider using  virtualenv  to\ninstall  floyd-cli . This solves most users' installation issues. We've\ndetailed the process below.", 
            "title": "Having trouble?"
        }, 
        {
            "location": "/guides/basics/install/#using-virtualenv-to-install-floyd-cli", 
            "text": "If you aren't familiar with  virtualenv , you can think of it as a way to\ncreate totally fresh environments in which you can have a clean copy of Python\nand install only the packages you need for working on a certain task or\nproject. You can turn these environments on and off as you need to use them.\nThis is useful because it helps you avoid issues that arise when you need more\nthan one version of the same package on your computer. If you'd like more\ninformation on  virtualenv , check out its  user\nguide  or  this introductory\ntutorial  We highly recommend using  virtualenv  for installing and using  floyd-cli .\nBecause it helps avoid any library version conflicts, it results in a smoother\ninstallation process. Using  virtualenv  solves most users' installation\nissues.", 
            "title": "Using virtualenv to Install floyd-cli"
        }, 
        {
            "location": "/guides/basics/install/#step-by-step-process-of-installing-floyd-cli-with-virtualenv", 
            "text": "Here is a step-by-step walkthrough of how to install  floyd-cli  using virtualenv . First let's use  pip  to install  virtualenv :  $ sudo pip install virtualenv  Create a virtualenv using the  virtualenv  command. You need to pass a path\nwhere your environment will be located. This is both where all the information\nabout your virtualenv will be stored, as well as all of your virtualenv's\npackages. Below, we use  ~/floyd  as the path:  $ virtualenv ~/floyd \nThis will create a clean environment (a virtualenv) called  floyd  (named after\nthe folder we specified when we created the virtualenv).  You can now activate and start using the virtualenv by running:  $  source  ~/floyd/bin/activate \nWhen you want to use this virtualenv, you'll need to activate it using the same source ~/floyd/bin/activate  command. Each new terminal session will require\nyou to activate the virtualenv. To turn off the virtualenv, you can either exit your terminal session, or enter the  deactivate  command into your terminal.  After activating the virtualenv stored at  ~/floyd , your terminal prompt\nshould change to have  (floyd)  prepended to it. This serves as a reminder that\nyou have the  floyd  virtualenv turned on. Since our virtualenv is new, it\ndoesn't have any packages installed in it yet. Let's install  floyd-cli :  ( floyd )  $ pip install -U floyd-cli  Success!  You are now ready to use the  floyd commands . Check out our  Quick Start tutorial  to get started using FloydHub and  floyd-cli .   Important  You need to activate your virtualenv using the  source ~/floyd/bin/activate \ncommand above each time you open a new terminal and want to use  floyd-cli \nin it.", 
            "title": "Step-by-step Process of Installing floyd-cli with virtualenv"
        }, 
        {
            "location": "/guides/basics/install/#having-trouble-installing-the-cli", 
            "text": "See the list of  FAQs related to installation .", 
            "title": "Having trouble installing the CLI?"
        }, 
        {
            "location": "/guides/basics/install/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/guides/basics/login/", 
            "text": "Quick Preparation Checklist\n\n\n\n\nYou must have a \nFloydHub account\n\n\nYou must be \nlogged in to your web dashboard\n\n\nYou must have \nfloyd-cli\n \ninstalled on your computer\n\n\n\n\nYou can use the \nfloyd login\n command to log in to\nyour FloydHub account through your command line\n\n\n$ floyd login\nAuthentication token page will now open in your browser. Continue? \n[\nY/n\n]\n: y\nPlease copy and paste the authentication token.\nThis is an invisible field. Paste token and press ENTER:\n\n\n\n\nThis will open your browser and display your authentication token. You can also\naccess this directly at\n\nfloydhub.com/settings/security\n.\n(\nNote:\n only visible if you are \nlogged in\n to\nyour web dashboard).\n\n\n\n\nCopy the token and paste it in your terminal.\n\n\nLogging in without opening a browser\n\n\nSometimes you may need to log in to Floyd on a computer that can't open a\nbrowser. Using your\n\ntoken from floydhub.com\n, you can\nlog in without opening a browser by passing the \n--token\n flag to\n\nfloyd login\n:\n\n\n$ floyd login --token\nPlease copy and paste the authentication token.\nThis is an invisible field. Paste token and press ENTER:\nLogin Successful\n\n\n\n\nHaving problems with logging in using floyd-cli?\n\n\n\n\nAre you on a Windows machine? Please see our \nFAQs for Windows\n\n\nOther problems? Check out our \nLogin FAQs\n or \ncontact us", 
            "title": "Log In Using Floyd CLI"
        }, 
        {
            "location": "/guides/basics/login/#quick-preparation-checklist", 
            "text": "You must have a  FloydHub account  You must be  logged in to your web dashboard  You must have  floyd-cli   installed on your computer   You can use the  floyd login  command to log in to\nyour FloydHub account through your command line  $ floyd login\nAuthentication token page will now open in your browser. Continue?  [ Y/n ] : y\nPlease copy and paste the authentication token.\nThis is an invisible field. Paste token and press ENTER:  This will open your browser and display your authentication token. You can also\naccess this directly at floydhub.com/settings/security .\n( Note:  only visible if you are  logged in  to\nyour web dashboard).   Copy the token and paste it in your terminal.", 
            "title": "Quick Preparation Checklist"
        }, 
        {
            "location": "/guides/basics/login/#logging-in-without-opening-a-browser", 
            "text": "Sometimes you may need to log in to Floyd on a computer that can't open a\nbrowser. Using your token from floydhub.com , you can\nlog in without opening a browser by passing the  --token  flag to floyd login :  $ floyd login --token\nPlease copy and paste the authentication token.\nThis is an invisible field. Paste token and press ENTER:\nLogin Successful", 
            "title": "Logging in without opening a browser"
        }, 
        {
            "location": "/guides/basics/login/#having-problems-with-logging-in-using-floyd-cli", 
            "text": "Are you on a Windows machine? Please see our  FAQs for Windows  Other problems? Check out our  Login FAQs  or  contact us", 
            "title": "Having problems with logging in using floyd-cli?"
        }, 
        {
            "location": "/guides/shortnames/", 
            "text": "When a Floyd CLI command takes a job name, a dataset name, or a job output, you can always specify a full length name, like this:\n\n\nJob:\n\n$ floyd status mckay/projects/quick-start/1\n\n\n\nDataset:\n\n$ floyd data status mckay/datasets/mnist/1\n\n\n\nJob Output:\n\n$ floyd status mckay/projects/quick-start/1/output\n\n\n\nFloyd CLI allows you to use shortened names instead of full-length ones. When\nyou leave out parts of a name, the CLI does the following:\n\n\n\n\nWhen a username is missing, the username of the logged-in user is used.\n\n\nWhen a project or dataset name is missing, the name of the currently\n    initialized project or dataset is used.\n\n\nWhen a job number or a dataset version number is missing, the most recent version is used.\n\n\nIf you leave out the \n/projects/\n or \n/datasets/\n part of the name, it is\n    inferred based on the command and other parts of the name. In most cases,\n    you shouldn't have to specify these parts of the name. However, if you want\n    to specify an output, you'll need to make sure your name has \n/output\n at the\n    end of it, output names are otherwise the same as job names.\n\n\n\n\nThese assumptions allow you to avoid typing out a full name in almost any\nsituation.\n\n\nExamples\n\n\nBelow are some examples on how to use shortened names for jobs, datasets, and\noutout. While only a selection of Floyd CLI commands are used in these\nexamples, anywhere job, output, or dataset name is used, a shortened name can\nbe used.\n\n\nBelow is a list of commands that take shortened names. Click on any item in the\nlist to go to its documentation:\n\n\n\n\nfloyd status \njob_name\n\n\nfloyd clone \njob_name\n\n\nfloyd info \njob_name\n\n\nfloyd logs \njob_name\n\n\nfloyd output \njob_name\n\n\nfloyd stop \njob_name\n\n\nfloyd delete \njob_name\n\n\nfloyd restart \njob_name\n\n\nfloyd run --data \ndataset_or_output_name\n:\nmount_point\n\n\nfloyd data status \ndataset_name\n\n\nfloyd data clone \ndataset_or_output_name\n\n\nfloyd data output \ndataset_or_output_name\n\n\nfloyd data delete \ndataset_name\n\n\n\n\nJob Name Examples\n\n\nHere are some examples that demonstrate how to use shorter names when\nreferencing a job. For these examples, we'll assume your username is \nfooster\n,\nthe project you've initialized in the current directory is called \nmy_project\n,\nand the last job you ran in \nmy_project\n is number \n3\n. Each example will show\ntwo code snippets: the first showing a command using a shortened name, and a\nsecond showing the full-length equivalent of the first command.\n\n\nBelow, we leave the job name completely blank and fall back on all our defaults: current user's username, current project's name, and most recent job number:\n\n\n$ floyd logs\n\n\n\n$ floyd logs fooster/projects/my_project/3\n\n\n\nHere, we want to specify a job number of a past job, but still under our current project:\n\n\n$ floyd logs \n1\n\n\n\n\n$ floyd logs fooster/projects/my_project/1\n\n\n\nHere, we want to specify a job under a different project, but still a project\nowned by our user. Because we don't specify a job number, the latest job will\nbe used (let's say it's 5):\n\n\n$ floyd logs other_project\n\n\n\n$ floyd logs fooster/projects/other_project/5\n\n\n\nWhat if we want to specify job 3 of \nother_project\n? Like this:\n\n\n$ floyd logs other_project/3\n\n\n\n$ floyd logs fooster/projects/other_project/3\n\n\n\nHere, we specify a different users's project. Let's say it's a user named\n\nalice\n and the project is called \nquick-start\n. Because we don't want the most\nrecent job, we'll specify the job number:\n\n\n$ floyd logs alice/quick-start/1\n\n\n\n$ floyd logs alice/projects/quick-start/1\n\n\n\nDataset Name Examples\n\n\nHere are some examples that demonstrate how to use shorter names when\nreferencing a dataset. For these examples, we'll assume your username is \nfooster\n,\nthe dataset you've initialized in the current directory is called \nmy_dataset\n,\nand the most recent version of the dataset is \n2\n. Each example will show\ntwo code snippets: the first showing a command using a shortened name, and a\nsecond showing the same command with the full-length equivalent of the\nshortened name.\n\n\nHere, we check the status of an older dataset version (version 1):\n\n\n$ floyd data status \n1\n\n\n\n\n$ floyd data status fooster/datasets/my_dataset/1\n\n\n\nHere, we open the browser to view the most recent version of our dataset.\nSpecifying nothing uses all our defaults--the current user's username, the\ndataset initialized in the current directory, and the most recent version of\nthe dataset:\n\n\n$ floyd data output\n\n\n\n$ floyd data output fooster/datasets/my_dataset/1\n\n\n\nIn this example, let's assume we're in a directory with a project initialized\nin it (otherwise the \nfloyd run\n command would not work). We'll mount a dataset\ncalled \nhello\n that belongs to the current user. We want the most recent\nversion of the dataset (which we'll say is \n2\n), so we don't specify the\nversion number.\n\n\n$ floyd run --data my_dataset:/model \npython train.py\n\n\n\n\n$ floyd run --data fooster/datasets/hello/2:/model \npython train.py\n\n\n\n\nOutput Name Examples\n\n\nWhen referring to the output of a job, you'll need to be sure to tack \n/output\n\nonto the end of the shortened name so the CLI knows you're referring to the\njob's output, and not the job itself. In the examples below, We'll assume that\nour username is \nfooster\n and that we have a project named \nmy_project\n\ninitialized in the current directory. Here are a couple examples:\n\n\nBelow, we want to mount the output of \nfooster/projects/my_project/3/output\n at \n/model\n:\n\n\n$ floyd run --data \n3\n/output:/model \npython eval.py\n\n\n\n\n$ floyd run --data fooster/projects/my_project/3/output:/model \npython eval.py\n\n\n\n\nHere we clone the output of job number 1 of our current project:\n\n\n$ floyd data clone \n1\n/output\n\n\n\n$ floyd data clone fooster/projects/my_project/1/output", 
            "title": "Using Shortened Job and Dataset Names"
        }, 
        {
            "location": "/guides/shortnames/#examples", 
            "text": "Below are some examples on how to use shortened names for jobs, datasets, and\noutout. While only a selection of Floyd CLI commands are used in these\nexamples, anywhere job, output, or dataset name is used, a shortened name can\nbe used.  Below is a list of commands that take shortened names. Click on any item in the\nlist to go to its documentation:   floyd status  job_name  floyd clone  job_name  floyd info  job_name  floyd logs  job_name  floyd output  job_name  floyd stop  job_name  floyd delete  job_name  floyd restart  job_name  floyd run --data  dataset_or_output_name : mount_point  floyd data status  dataset_name  floyd data clone  dataset_or_output_name  floyd data output  dataset_or_output_name  floyd data delete  dataset_name", 
            "title": "Examples"
        }, 
        {
            "location": "/guides/shortnames/#job-name-examples", 
            "text": "Here are some examples that demonstrate how to use shorter names when\nreferencing a job. For these examples, we'll assume your username is  fooster ,\nthe project you've initialized in the current directory is called  my_project ,\nand the last job you ran in  my_project  is number  3 . Each example will show\ntwo code snippets: the first showing a command using a shortened name, and a\nsecond showing the full-length equivalent of the first command.  Below, we leave the job name completely blank and fall back on all our defaults: current user's username, current project's name, and most recent job number:  $ floyd logs  $ floyd logs fooster/projects/my_project/3  Here, we want to specify a job number of a past job, but still under our current project:  $ floyd logs  1   $ floyd logs fooster/projects/my_project/1  Here, we want to specify a job under a different project, but still a project\nowned by our user. Because we don't specify a job number, the latest job will\nbe used (let's say it's 5):  $ floyd logs other_project  $ floyd logs fooster/projects/other_project/5  What if we want to specify job 3 of  other_project ? Like this:  $ floyd logs other_project/3  $ floyd logs fooster/projects/other_project/3  Here, we specify a different users's project. Let's say it's a user named alice  and the project is called  quick-start . Because we don't want the most\nrecent job, we'll specify the job number:  $ floyd logs alice/quick-start/1  $ floyd logs alice/projects/quick-start/1", 
            "title": "Job Name Examples"
        }, 
        {
            "location": "/guides/shortnames/#dataset-name-examples", 
            "text": "Here are some examples that demonstrate how to use shorter names when\nreferencing a dataset. For these examples, we'll assume your username is  fooster ,\nthe dataset you've initialized in the current directory is called  my_dataset ,\nand the most recent version of the dataset is  2 . Each example will show\ntwo code snippets: the first showing a command using a shortened name, and a\nsecond showing the same command with the full-length equivalent of the\nshortened name.  Here, we check the status of an older dataset version (version 1):  $ floyd data status  1   $ floyd data status fooster/datasets/my_dataset/1  Here, we open the browser to view the most recent version of our dataset.\nSpecifying nothing uses all our defaults--the current user's username, the\ndataset initialized in the current directory, and the most recent version of\nthe dataset:  $ floyd data output  $ floyd data output fooster/datasets/my_dataset/1  In this example, let's assume we're in a directory with a project initialized\nin it (otherwise the  floyd run  command would not work). We'll mount a dataset\ncalled  hello  that belongs to the current user. We want the most recent\nversion of the dataset (which we'll say is  2 ), so we don't specify the\nversion number.  $ floyd run --data my_dataset:/model  python train.py   $ floyd run --data fooster/datasets/hello/2:/model  python train.py", 
            "title": "Dataset Name Examples"
        }, 
        {
            "location": "/guides/shortnames/#output-name-examples", 
            "text": "When referring to the output of a job, you'll need to be sure to tack  /output \nonto the end of the shortened name so the CLI knows you're referring to the\njob's output, and not the job itself. In the examples below, We'll assume that\nour username is  fooster  and that we have a project named  my_project \ninitialized in the current directory. Here are a couple examples:  Below, we want to mount the output of  fooster/projects/my_project/3/output  at  /model :  $ floyd run --data  3 /output:/model  python eval.py   $ floyd run --data fooster/projects/my_project/3/output:/model  python eval.py   Here we clone the output of job number 1 of our current project:  $ floyd data clone  1 /output  $ floyd data clone fooster/projects/my_project/1/output", 
            "title": "Output Name Examples"
        }, 
        {
            "location": "/commands/", 
            "text": "Floyd Commands\n\n\nBelow are the commands that are part of Floyd CLI.\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nfloyd login\n\n\nLogin to Floyd.\n\n\n\n\n\n\nfloyd init\n\n\nInitialize a Floyd project\n\n\n\n\n\n\nfloyd run\n\n\nRun your project on Floyd\n\n\n\n\n\n\nfloyd data\n\n\nManage data on Floyd\n\n\n\n\n\n\nfloyd logs\n\n\nStream logs of your job\n\n\n\n\n\n\nfloyd status\n\n\nCheck status of your jobs\n\n\n\n\n\n\nfloyd clone\n\n\nClone an existing floyd project\n\n\n\n\n\n\nfloyd output\n\n\nView the output of a job\n\n\n\n\n\n\nfloyd info\n\n\nSee details of a job\n\n\n\n\n\n\nfloyd stop\n\n\nTerminate a job\n\n\n\n\n\n\nfloyd logout\n\n\nLogout from Floyd\n\n\n\n\n\n\nfloyd version\n\n\nSee version of floyd client\n\n\n\n\n\n\nfloyd upgrade\n\n\nUpgrade floyd client", 
            "title": "floyd"
        }, 
        {
            "location": "/commands/#floyd-commands", 
            "text": "Below are the commands that are part of Floyd CLI.     Command  Description      floyd login  Login to Floyd.    floyd init  Initialize a Floyd project    floyd run  Run your project on Floyd    floyd data  Manage data on Floyd    floyd logs  Stream logs of your job    floyd status  Check status of your jobs    floyd clone  Clone an existing floyd project    floyd output  View the output of a job    floyd info  See details of a job    floyd stop  Terminate a job    floyd logout  Logout from Floyd    floyd version  See version of floyd client    floyd upgrade  Upgrade floyd client", 
            "title": "Floyd Commands"
        }, 
        {
            "location": "/commands/login/", 
            "text": "Login to Floyd.\n\n\nUsage\n\n\nfloyd login\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--token\n\n\nFalse\n\n\nIf specified, browser will not open. You can paste your token in the command line. \nNote\n: This is only supported with \nversion 0.7.2+\n of \nfloyd-cli\n. If you get an error, please upgrade using \npip install -U floyd-cli\n and try again\n\n\n\n\n\n\n--username\n\n\nFalse\n\n\nFloydHub username. If specified, must include \n--password\n\n\n\n\n\n\n--password\n\n\nFalse\n\n\nFloydHub password\n\n\n\n\n\n\n\n\nDescription\n\n\nYou need to login to Floyd before running any other command. The login flow will require an access token from the Floydhub \nwebsite. You will be prompted to enter you credentials to get your access token. Copy and paste the token on the command line \nto complete login.\n\n\nAlternatively, you can use the username and password parameters to directly login from the command line. In case you\nrun in to any issues logging in with username / password, try logging in with the token as explain above.\n\n\nExample\n\n\nTo automatically open your browser\n\n$ floyd login\nAuthentication token page will now open in your browser. Continue? \n[\nY/n\n]\n:\nPlease paste the token here:\nLogin Successful as alice\n\n\n\nIn case you use remote machines and do not have access to the browser, you can copy the token from the \n\ndashboard\n and use the \n--token\n parameter when you login.\n\n$ floyd login --token\nPlease copy and paste the token here:\nLogin Successful as alice\n\n\n\nIf you are using the credentials directly:\n\n$ floyd login --username alice --password \nredacted\n\nLogin Successful as alice\n\n\n\nAuthentication Tokens\n\n\nFloyd uses \nJson Web Tokens\n for authentication. Your \ntoken will be stored in the \n~/.floydconfig\n file. They are valid for 7 days after \nwhich you need to login again. This file will be removed when you \nlogout\n.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd login"
        }, 
        {
            "location": "/commands/login/#usage", 
            "text": "floyd login", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/login/#options", 
            "text": "Name, shorthand  Default  Description      --token  False  If specified, browser will not open. You can paste your token in the command line.  Note : This is only supported with  version 0.7.2+  of  floyd-cli . If you get an error, please upgrade using  pip install -U floyd-cli  and try again    --username  False  FloydHub username. If specified, must include  --password    --password  False  FloydHub password", 
            "title": "Options"
        }, 
        {
            "location": "/commands/login/#description", 
            "text": "You need to login to Floyd before running any other command. The login flow will require an access token from the Floydhub \nwebsite. You will be prompted to enter you credentials to get your access token. Copy and paste the token on the command line \nto complete login.  Alternatively, you can use the username and password parameters to directly login from the command line. In case you\nrun in to any issues logging in with username / password, try logging in with the token as explain above.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/login/#example", 
            "text": "To automatically open your browser $ floyd login\nAuthentication token page will now open in your browser. Continue?  [ Y/n ] :\nPlease paste the token here:\nLogin Successful as alice  In case you use remote machines and do not have access to the browser, you can copy the token from the  dashboard  and use the  --token  parameter when you login. $ floyd login --token\nPlease copy and paste the token here:\nLogin Successful as alice  If you are using the credentials directly: $ floyd login --username alice --password  redacted \nLogin Successful as alice", 
            "title": "Example"
        }, 
        {
            "location": "/commands/login/#authentication-tokens", 
            "text": "Floyd uses  Json Web Tokens  for authentication. Your \ntoken will be stored in the  ~/.floydconfig  file. They are valid for 7 days after \nwhich you need to login again. This file will be removed when you  logout .", 
            "title": "Authentication Tokens"
        }, 
        {
            "location": "/commands/login/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/init/", 
            "text": "Initialize a Floyd project.\n\n\nUsage\n\n\nfloyd init PROJECT_NAME\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nPROJECT_NAME\n\n\n\n\nName of your project (Pick a name from the projects page in web dashboard)\n\n\n\n\n\n\n\n\nDescription\n\n\nThis command initializes the current directory for a given project name and tracks all the files and subdirectories. \nMake sure that the project name you enter here already exists in Floyd. In case the project name does not exist, \nthe CLI will open the create project page in your browser.\n\n\nTo initialize a new dataset you should use \nfloyd data init\n command.\n\n\nThe init command also creates a \n.floydignore\n file. Any files and directories you do not want Floyd to track can be added \nto this file. When you run your project on Floyd, these files will not be uploaded. More details on how floydignore \nfile works is available \nhere\n.\n\n\nExample\n\n\nInitialize a floyd project in your project directory.\n\n$ \ncd\n /code/project\n$ floyd init style-transfer\nProject \nstyle-transfer\n initialized in current directory\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd init"
        }, 
        {
            "location": "/commands/init/#usage", 
            "text": "floyd init PROJECT_NAME", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/init/#options", 
            "text": "Name, shorthand  Default  Description      PROJECT_NAME   Name of your project (Pick a name from the projects page in web dashboard)", 
            "title": "Options"
        }, 
        {
            "location": "/commands/init/#description", 
            "text": "This command initializes the current directory for a given project name and tracks all the files and subdirectories. \nMake sure that the project name you enter here already exists in Floyd. In case the project name does not exist, \nthe CLI will open the create project page in your browser.  To initialize a new dataset you should use  floyd data init  command.  The init command also creates a  .floydignore  file. Any files and directories you do not want Floyd to track can be added \nto this file. When you run your project on Floyd, these files will not be uploaded. More details on how floydignore \nfile works is available  here .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/init/#example", 
            "text": "Initialize a floyd project in your project directory. $  cd  /code/project\n$ floyd init style-transfer\nProject  style-transfer  initialized in current directory", 
            "title": "Example"
        }, 
        {
            "location": "/commands/init/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/run/", 
            "text": "Run your project on Floyd.\n\n\nUsage\n\n\nfloyd run \n[\nOPTIONS\n]\n \n[\nCOMMAND\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--gpu/--cpu\n\n\ncpu\n\n\nIf specified, runs the job on a GPU (G1) instance or CPU (C1) instance. See instance specifications on the \npricing\n page.\n\n\n\n\n\n\n--data \nID:mount\n\n\n\n\nID\n of the data source to link to. \nmount\n specifies the path to mount it at. You can use this parameter multiple times. See \ndata\n section for more details.\n\n\n\n\n\n\n--mode [jupyter|serve]\n\n\ncommand\n\n\nSpecify the mode you want to run the project. The default behavior executes the command you specify. See \njupyter\n and \nserve\n sections for more info on them.\n\n\n\n\n\n\n--no-open\n\n\n\n\nYou can disable the CLI from opening the jupyter notebook url. It will print the URL instead.\n\n\n\n\n\n\n--env [tensorflow:py3|tensorflow:py2|...]\n\n\nkeras:py3\n\n\nSpecify the environment you want to use for your project. See \nenvironments\n for the full list.\n\n\n\n\n\n\n--message \nmessage_str\n\n\n\n\nAttach a message to the specific run of the project.\n\n\n\n\n\n\n--tensorboard\n\n\n\n\nStarts tensorboard in the environment. Tensorboard URL can be found in the dashboard.\n\n\n\n\n\n\ncommand\n\n\n\n\nCommand to execute when running your project on Floyd.\n\n\n\n\n\n\n\n\nDescription\n\n\nThis command syncs the code tracked by the CLI to the Floyd servers and executes your command. You can see the progress\nwith \nstatus\n command. To view the logs from your code use \nlogs\n command.\n\n\nExample\n\n\n$ floyd run --env tensorflow --gpu \npython mnist_cnn.py\n\nSyncing code ...\nNAME\n-----------------------------\nfloydhub/projects/lung-cancer/2\n\n...\n$ floyd logs floydhub/projects/lung-cancer/2\n\n\n\n\nfloyd_requirements.txt\n\n\nFloyd runs standard Docker images for various deep learning frameworks.(See \nenvironments\n for details). If your\ncode requires additional Python dependencies you can specify them in a \nfloyd_requirements.txt\n file and place it at the root\ndirectory of your project. These dependencies will be installed before running your code.\n\n\nExample\n\n\n$ cat floyd_requirements.txt\nPillow\nscipy\n$ floyd run \npython train_tf.py -lr 0.01 -output /output/model.bin\n\n\n\n\n\nJupyter notebook\n\n\nFloyd supports running Jupyter/iPython notebooks on the server. Make sure that the notebook (.ipynb) files are present in the\ncurrent directory. Use \n--mode jupyter\n and you will be presented with a URL to view your Jupyter environment. You do not need\nto specify a command in this mode. See \njupyter\n page for more details.\n\n\nExample\n\n\n$ floyd run --mode jupyter\n...\nPath to jupyter notebook: https://www.floydhub.com/notebooks/g8uGRZFQz85meArJGToEcs\n\n\n\n\nAttaching multiple datasets\n\n\nYou can attach upto 5 datasets when you run a project using the run command. You can specify both\ndatasets you uploaded and output datasets of your previous runs. You can specify the mount point\nalso when you specify the data id to mount.\n\n\nExample:\n\n\n$ floyd run --data udacity/datasets/celeba/1:training --data udacity/datasets/mnist/1:testing \npython script.py\n\n\n\nThe above datasets will be mounted at \n/training\n and \n/testing\n respectively.\n\n\nServe\n\n\nFloyd can be used to host the model you generated as a REST api. This api can be used to evaluate your model over HTTP.\nUse \n--mode serve\n and you will be presented with a URL to access your API. Floyd currently supports only Flask apps.\nIt runs app.py file and expects the service to run on port 5000. You do not need to specify a command in this mode.\nSee \nserve\n page for more details.\n\n\nExample\n\n\n$ floyd run --mode serve\n...\nPath to service endpoint: https://www.floydhub.com/expose/vbKSKgVYGgZqmM9i3LjLBb\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd run"
        }, 
        {
            "location": "/commands/run/#usage", 
            "text": "floyd run  [ OPTIONS ]   [ COMMAND ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/run/#options", 
            "text": "Name, shorthand  Default  Description      --gpu/--cpu  cpu  If specified, runs the job on a GPU (G1) instance or CPU (C1) instance. See instance specifications on the  pricing  page.    --data  ID:mount   ID  of the data source to link to.  mount  specifies the path to mount it at. You can use this parameter multiple times. See  data  section for more details.    --mode [jupyter|serve]  command  Specify the mode you want to run the project. The default behavior executes the command you specify. See  jupyter  and  serve  sections for more info on them.    --no-open   You can disable the CLI from opening the jupyter notebook url. It will print the URL instead.    --env [tensorflow:py3|tensorflow:py2|...]  keras:py3  Specify the environment you want to use for your project. See  environments  for the full list.    --message  message_str   Attach a message to the specific run of the project.    --tensorboard   Starts tensorboard in the environment. Tensorboard URL can be found in the dashboard.    command   Command to execute when running your project on Floyd.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/run/#description", 
            "text": "This command syncs the code tracked by the CLI to the Floyd servers and executes your command. You can see the progress\nwith  status  command. To view the logs from your code use  logs  command.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/run/#example", 
            "text": "$ floyd run --env tensorflow --gpu  python mnist_cnn.py \nSyncing code ...\nNAME\n-----------------------------\nfloydhub/projects/lung-cancer/2\n\n...\n$ floyd logs floydhub/projects/lung-cancer/2", 
            "title": "Example"
        }, 
        {
            "location": "/commands/run/#floyd_requirementstxt", 
            "text": "Floyd runs standard Docker images for various deep learning frameworks.(See  environments  for details). If your\ncode requires additional Python dependencies you can specify them in a  floyd_requirements.txt  file and place it at the root\ndirectory of your project. These dependencies will be installed before running your code.", 
            "title": "floyd_requirements.txt"
        }, 
        {
            "location": "/commands/run/#example_1", 
            "text": "$ cat floyd_requirements.txt\nPillow\nscipy\n$ floyd run  python train_tf.py -lr 0.01 -output /output/model.bin", 
            "title": "Example"
        }, 
        {
            "location": "/commands/run/#jupyter-notebook", 
            "text": "Floyd supports running Jupyter/iPython notebooks on the server. Make sure that the notebook (.ipynb) files are present in the\ncurrent directory. Use  --mode jupyter  and you will be presented with a URL to view your Jupyter environment. You do not need\nto specify a command in this mode. See  jupyter  page for more details.", 
            "title": "Jupyter notebook"
        }, 
        {
            "location": "/commands/run/#example_2", 
            "text": "$ floyd run --mode jupyter\n...\nPath to jupyter notebook: https://www.floydhub.com/notebooks/g8uGRZFQz85meArJGToEcs", 
            "title": "Example"
        }, 
        {
            "location": "/commands/run/#attaching-multiple-datasets", 
            "text": "You can attach upto 5 datasets when you run a project using the run command. You can specify both\ndatasets you uploaded and output datasets of your previous runs. You can specify the mount point\nalso when you specify the data id to mount.", 
            "title": "Attaching multiple datasets"
        }, 
        {
            "location": "/commands/run/#example_3", 
            "text": "$ floyd run --data udacity/datasets/celeba/1:training --data udacity/datasets/mnist/1:testing  python script.py  \nThe above datasets will be mounted at  /training  and  /testing  respectively.", 
            "title": "Example:"
        }, 
        {
            "location": "/commands/run/#serve", 
            "text": "Floyd can be used to host the model you generated as a REST api. This api can be used to evaluate your model over HTTP.\nUse  --mode serve  and you will be presented with a URL to access your API. Floyd currently supports only Flask apps.\nIt runs app.py file and expects the service to run on port 5000. You do not need to specify a command in this mode.\nSee  serve  page for more details.", 
            "title": "Serve"
        }, 
        {
            "location": "/commands/run/#example_4", 
            "text": "$ floyd run --mode serve\n...\nPath to service endpoint: https://www.floydhub.com/expose/vbKSKgVYGgZqmM9i3LjLBb", 
            "title": "Example"
        }, 
        {
            "location": "/commands/run/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/data/", 
            "text": "Manage your data sets on Floyd. The subcommands are:\n\n\n\n\n\n\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nfloyd data init\n\n\nInitialize a dataset\n\n\n\n\n\n\nfloyd data upload\n\n\nCreate a new dataset version\n\n\n\n\n\n\nfloyd data status\n\n\nList all your datasets\n\n\n\n\n\n\nfloyd data clone\n\n\nClone an existing dataset\n\n\n\n\n\n\nfloyd data delete\n\n\nDelete your datasets\n\n\n\n\n\n\nfloyd data add\n\n\nAdd job output to dataset\n\n\n\n\n\n\nfloyd data output\n\n\nView contents of a dataset\n\n\n\n\n\n\n\n\nfloyd data init\n\n\nInitialize a Floyd dataset.\n\n\nUsage\n\n\nfloyd data init DATASET_NAME\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nDATASET_NAME\n\n\n\n\nName of the dataset (Pick a name from the dataset page in web dashboard)\n\n\n\n\n\n\n\n\nDescription\n\n\nFloyd can manage your experiment datasets and make them available when running your projects.This command initializes the\ncurrent directory and tracks all files and subdirectories. Make sure the dataset name you enter here already\nexists in Floyd. In case the dataset name does not exist, the CLI will open the create dataset page in your browser.\n\n\nExample\n\n\nInitialize a floyd dataset in your data directory.\n\n$ \ncd\n /data/mnist\n$ floyd data init mnist\nData \nsource\n \nmnist\n initialized in current directory\n\n\n\n\n\nfloyd data upload\n\n\nUpload a new version of dataset\n\n\nUsage\n\n\nfloyd data upload\n\n\n\n\nDescription\n\n\nUpload contents of the current directory as a new version of the dataset. This data can now be referred to in the \nrun\n command.\nAt run time the data will be available at the \n/input\n path.\n\n\nFloyd also versions your data so you can choose any specific version to use in your runs.\n\n\nCurrently this command does NOT respect a \n.floydignore\n file. This functionality may be added in the future.\n\n\nExample\n\n\n$ floyd data upload\nCreating data source. Uploading files ...\nNAME\n--------------------\nalice/mnist-data:1\n\n\nFloyd will generate a data id for the uploaded dataset. This uploaded dataset can be used in your future experiments, if needed,\nusing this data id. See \nhere\n for more details.\n\n\n\n\nfloyd data status\n\n\nView your datasets on Floyd\n\n\nUsage\n\n\nfloyd data status \n[\nNAME or ID\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nName or ID of your data.\n\n\n\n\n\n\n\n\nDescription\n\n\nLists all your datasets on Floyd with more details.\n\n\nThis command can take a \nshortened job name\n.\n\n\nExample\n\n\n$ floyd data status\nDATA NAME                         CREATED         STATUS    DISK USAGE\n--------------------------------  --------------  --------  ------------\nmckay/datasets/zeroes/1            \n57\n seconds ago  valid     \n180\n.0 KB\nmckay/datasets/mnist/1             \n2\n minutes ago   valid     \n10\n.0 KB\n\n\n\n\n\n\nfloyd data delete\n\n\nDelete datasets on FloydHub\n\n\nUsage\n\n\nfloyd data delete \n[\nOPTIONS\n]\n \n[\nNAMES or IDS\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNAMES or IDS\n\n\n\n\nOne or more Names or IDs of your data.\n\n\n\n\n\n\n--yes\n, \n-y\n\n\nFalse\n\n\nSkip delete confirmation step\n\n\n\n\n\n\n\n\nDescription\n\n\nDeletes your datasets from FloydHub. This data will no longer\nbe accessible.\n\n\nNote: You do \nnot\n have to be in the project directory to run this command.\nThis command can take a \nshortened job name\n.\n\n\nExample\n\n\n$ floyd data delete mckay/datasets/mnist/1\nDelete Data: mckay/datasets/mnist/1? \n[\ny/N\n]\n: y\nData deleted\n\n\n\n\n\n\nfloyd data add\n\n\nCopy job output data to a dataset\n\n\nUsage\n\n\nfloyd data add ID\n\n\n\n\nDescription\n\n\nAdd the contents of a job output to a dataset. This will appear as a new version of data in the dataset. This command is useful if you\nwant to save the output of specific jobs under a dataset. Note: You will be charged for the disk usage separately.\nThis new data can now be referred to in the \nrun\n command.\n\n\nThis command can take a \nshortened job name\n.\n\n\nExample\n\n\n$ floyd data add mckay/projects/mnist/1/output\nDATA NAME                         CREATED    STATUS    DISK USAGE\n--------------------------------  ---------  --------  ------------\nmckay/datasets/mnist/1            just now   valid     \n10\n.0 KB\n\n\nFloyd will generate a new version for the added data. This can be used in your future experiments, if needed,\nusing this data id. See \nhere\n for more details.\n\n\n\n\nfloyd data output\n\n\nView datasets\n\n\nUsage\n\n\nfloyd data output \n[\nOPTIONS\n]\n ID\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--url\n, \n-u\n\n\n\n\nOnly print the URL. The data directory can be viewed in the browser.\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nNAME or ID of your data.\n\n\n\n\n\n\n\n\nDescription\n\n\nThe output command gives the url to access a dataset. This command by default opens the data url\nin your default browser.\n\n\nExample\n\n\n$ floyd data output mckay/datasets/mnist/1\nOpening output directory in your browser ...\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd data"
        }, 
        {
            "location": "/commands/data/#floyd-data-init", 
            "text": "Initialize a Floyd dataset.", 
            "title": "floyd data init"
        }, 
        {
            "location": "/commands/data/#usage", 
            "text": "floyd data init DATASET_NAME", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#options", 
            "text": "Name, shorthand  Default  Description      DATASET_NAME   Name of the dataset (Pick a name from the dataset page in web dashboard)", 
            "title": "Options"
        }, 
        {
            "location": "/commands/data/#description", 
            "text": "Floyd can manage your experiment datasets and make them available when running your projects.This command initializes the\ncurrent directory and tracks all files and subdirectories. Make sure the dataset name you enter here already\nexists in Floyd. In case the dataset name does not exist, the CLI will open the create dataset page in your browser.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example", 
            "text": "Initialize a floyd dataset in your data directory. $  cd  /data/mnist\n$ floyd data init mnist\nData  source   mnist  initialized in current directory", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#floyd-data-upload", 
            "text": "Upload a new version of dataset", 
            "title": "floyd data upload"
        }, 
        {
            "location": "/commands/data/#usage_1", 
            "text": "floyd data upload", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#description_1", 
            "text": "Upload contents of the current directory as a new version of the dataset. This data can now be referred to in the  run  command.\nAt run time the data will be available at the  /input  path.  Floyd also versions your data so you can choose any specific version to use in your runs.  Currently this command does NOT respect a  .floydignore  file. This functionality may be added in the future.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example_1", 
            "text": "$ floyd data upload\nCreating data source. Uploading files ...\nNAME\n--------------------\nalice/mnist-data:1 \nFloyd will generate a data id for the uploaded dataset. This uploaded dataset can be used in your future experiments, if needed,\nusing this data id. See  here  for more details.", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#floyd-data-status", 
            "text": "View your datasets on Floyd", 
            "title": "floyd data status"
        }, 
        {
            "location": "/commands/data/#usage_2", 
            "text": "floyd data status  [ NAME or ID ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#options_1", 
            "text": "Name, shorthand  Default  Description      NAME or ID   Name or ID of your data.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/data/#description_2", 
            "text": "Lists all your datasets on Floyd with more details.  This command can take a  shortened job name .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example_2", 
            "text": "$ floyd data status\nDATA NAME                         CREATED         STATUS    DISK USAGE\n--------------------------------  --------------  --------  ------------\nmckay/datasets/zeroes/1             57  seconds ago  valid      180 .0 KB\nmckay/datasets/mnist/1              2  minutes ago   valid      10 .0 KB", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#floyd-data-delete", 
            "text": "Delete datasets on FloydHub", 
            "title": "floyd data delete"
        }, 
        {
            "location": "/commands/data/#usage_3", 
            "text": "floyd data delete  [ OPTIONS ]   [ NAMES or IDS ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#options_2", 
            "text": "Name, shorthand  Default  Description      NAMES or IDS   One or more Names or IDs of your data.    --yes ,  -y  False  Skip delete confirmation step", 
            "title": "Options"
        }, 
        {
            "location": "/commands/data/#description_3", 
            "text": "Deletes your datasets from FloydHub. This data will no longer\nbe accessible.  Note: You do  not  have to be in the project directory to run this command.\nThis command can take a  shortened job name .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example_3", 
            "text": "$ floyd data delete mckay/datasets/mnist/1\nDelete Data: mckay/datasets/mnist/1?  [ y/N ] : y\nData deleted", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#floyd-data-add", 
            "text": "Copy job output data to a dataset", 
            "title": "floyd data add"
        }, 
        {
            "location": "/commands/data/#usage_4", 
            "text": "floyd data add ID", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#description_4", 
            "text": "Add the contents of a job output to a dataset. This will appear as a new version of data in the dataset. This command is useful if you\nwant to save the output of specific jobs under a dataset. Note: You will be charged for the disk usage separately.\nThis new data can now be referred to in the  run  command.  This command can take a  shortened job name .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example_4", 
            "text": "$ floyd data add mckay/projects/mnist/1/output\nDATA NAME                         CREATED    STATUS    DISK USAGE\n--------------------------------  ---------  --------  ------------\nmckay/datasets/mnist/1            just now   valid      10 .0 KB \nFloyd will generate a new version for the added data. This can be used in your future experiments, if needed,\nusing this data id. See  here  for more details.", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#floyd-data-output", 
            "text": "View datasets", 
            "title": "floyd data output"
        }, 
        {
            "location": "/commands/data/#usage_5", 
            "text": "floyd data output  [ OPTIONS ]  ID", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/data/#options_3", 
            "text": "Name, shorthand  Default  Description      --url ,  -u   Only print the URL. The data directory can be viewed in the browser.    NAME or ID   NAME or ID of your data.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/data/#description_5", 
            "text": "The output command gives the url to access a dataset. This command by default opens the data url\nin your default browser.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/data/#example_5", 
            "text": "$ floyd data output mckay/datasets/mnist/1\nOpening output directory in your browser ...", 
            "title": "Example"
        }, 
        {
            "location": "/commands/data/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/logs/", 
            "text": "View the logs of your run\n\n\nUsage\n\n\nfloyd logs \n[\nOPTIONS\n]\n ID\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--url\n, \n-u\n\n\n\n\nOnly print the URL. The logs can be viewed in the browser.\n\n\n\n\n\n\n--tail\n, \n-t\n\n\n\n\nStream the output of your code in real-time.\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nName or ID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nAny data sent to STDOUT and STDERR by your code will become available here. Make sure your\nlogs are flushed out if you prefer to view logs in real-time. There will be some information from\nFloyd servers before and after your project logs. They are usually useful for debugging purposes.\n\n\nThis command can take a \nshortened job name\n.\n\n\nExample\n\n\n$ floyd logs floydhub/projects/deep-photo-styletransfer/4\nPreparing to run\nStarting container...\n\n\n#################################################\n\n\nRun Output:\n...\n\n\n#################################################\n\n\nWaiting \nfor\n container to complete...\n\n[\nsuccess\n]\n Finishing execution\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd logs"
        }, 
        {
            "location": "/commands/logs/#usage", 
            "text": "floyd logs  [ OPTIONS ]  ID", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/logs/#options", 
            "text": "Name, shorthand  Default  Description      --url ,  -u   Only print the URL. The logs can be viewed in the browser.    --tail ,  -t   Stream the output of your code in real-time.    NAME or ID   Name or ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/logs/#description", 
            "text": "Any data sent to STDOUT and STDERR by your code will become available here. Make sure your\nlogs are flushed out if you prefer to view logs in real-time. There will be some information from\nFloyd servers before and after your project logs. They are usually useful for debugging purposes.  This command can take a  shortened job name .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/logs/#example", 
            "text": "$ floyd logs floydhub/projects/deep-photo-styletransfer/4\nPreparing to run\nStarting container... ################################################# \n\nRun Output:\n... ################################################# \n\nWaiting  for  container to complete... [ success ]  Finishing execution", 
            "title": "Example"
        }, 
        {
            "location": "/commands/logs/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/status/", 
            "text": "View status of your jobs.\n\n\nUsage\n\n\nfloyd status \n[\nNAME or ID\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nName or ID\n\n\n\n\nID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nShows the status of a run, if the name is specified. It can also list the status of all\nthe runs in the current project. You need to be in the project directory for this command to work.\n\n\nThis command can take a \nshortened job name\n.\n\n\nExamples\n\n\n$ floyd status\nJOB NAME                                 CREATED       STATUS      DURATION\n(\ns\n)\n  INSTANCE    DESCRIPTION\n---------------------------------        ------------  --------  -------------  ----------  -------------\nfloydhub/projects/mnist-tensorboard/3    \n19\n hours ago  success              \n16\n  g1\nfloydhub/projects/mnist-tensorboard/2    \n19\n hours ago  success               \n3\n  c1\nfloydhub/projects/mnist-tensorboard/1    \n5\n days ago    success              \n26\n  c1\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd status"
        }, 
        {
            "location": "/commands/status/#usage", 
            "text": "floyd status  [ NAME or ID ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/status/#options", 
            "text": "Name, shorthand  Default  Description      Name or ID   ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/status/#description", 
            "text": "Shows the status of a run, if the name is specified. It can also list the status of all\nthe runs in the current project. You need to be in the project directory for this command to work.  This command can take a  shortened job name .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/status/#examples", 
            "text": "$ floyd status\nJOB NAME                                 CREATED       STATUS      DURATION ( s )   INSTANCE    DESCRIPTION\n---------------------------------        ------------  --------  -------------  ----------  -------------\nfloydhub/projects/mnist-tensorboard/3     19  hours ago  success               16   g1\nfloydhub/projects/mnist-tensorboard/2     19  hours ago  success                3   c1\nfloydhub/projects/mnist-tensorboard/1     5  days ago    success               26   c1", 
            "title": "Examples"
        }, 
        {
            "location": "/commands/status/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/output/", 
            "text": "View the output of a job.\n\n\nUsage\n\n\nfloyd output \n[\nOPTIONS\n]\n ID\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n--url\n, \n-u\n\n\n\n\nOnly print the URL. The output directory can be viewed in the browser.\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nName or ID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nMost jobs generate output. Any output that needs to be retained after the job is finished should be send to \n/output\n path.\nThis is the only path Floyd will preserve. The output command gives the url to access this output. This command by default opens the\noutput url in your default browser.\n\n\nThis command can take a \nshortened job name\n.\n\n\nExample\n\n\n$ floyd output floydhub/projects/deep-photo-styletransfer/4\nOpening output directory in your browser ...\n\n\n\n\nDownloading output\n\n\nTo download the output you can use the data \nclone\n command.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd output"
        }, 
        {
            "location": "/commands/output/#usage", 
            "text": "floyd output  [ OPTIONS ]  ID", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/output/#options", 
            "text": "Name, shorthand  Default  Description      --url ,  -u   Only print the URL. The output directory can be viewed in the browser.    NAME or ID   Name or ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/output/#description", 
            "text": "Most jobs generate output. Any output that needs to be retained after the job is finished should be send to  /output  path.\nThis is the only path Floyd will preserve. The output command gives the url to access this output. This command by default opens the\noutput url in your default browser.  This command can take a  shortened job name .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/output/#example", 
            "text": "$ floyd output floydhub/projects/deep-photo-styletransfer/4\nOpening output directory in your browser ...", 
            "title": "Example"
        }, 
        {
            "location": "/commands/output/#downloading-output", 
            "text": "To download the output you can use the data  clone  command.", 
            "title": "Downloading output"
        }, 
        {
            "location": "/commands/output/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/clone/", 
            "text": "Clone the code for a specific job.\n\n\nUsage\n\n\nfloyd clone JOB_NAME\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nJOB_NAME or ID\n\n\n\n\nName or ID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nUse this command to clone an existing project on floyd. The code used for the job is downloaded to the\ncurrent directory. This will override any existing file or directory in the process.\n\n\nThis command is a great way to get started on floyd by starting from an existing project.\n\n\nThis command can take a \nshortened job name\n.\n\n\nExample\n\n\n$ floyd clone floydhub/projects/deep-photo-styletransfer/4\nDownloading the tar file to the current directory ...\nUntarring the contents of the file ...\nCleaning up the tar file ...\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd clone"
        }, 
        {
            "location": "/commands/clone/#usage", 
            "text": "floyd clone JOB_NAME", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/clone/#options", 
            "text": "Name, shorthand  Default  Description      JOB_NAME or ID   Name or ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/clone/#description", 
            "text": "Use this command to clone an existing project on floyd. The code used for the job is downloaded to the\ncurrent directory. This will override any existing file or directory in the process.  This command is a great way to get started on floyd by starting from an existing project.  This command can take a  shortened job name .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/clone/#example", 
            "text": "$ floyd clone floydhub/projects/deep-photo-styletransfer/4\nDownloading the tar file to the current directory ...\nUntarring the contents of the file ...\nCleaning up the tar file ...", 
            "title": "Example"
        }, 
        {
            "location": "/commands/clone/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/info/", 
            "text": "View the details of a job.\n\n\nUsage\n\n\nfloyd info \n[\nOPTIONS\n]\n NAME\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nName or ID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nThis command gives detailed information about your job.\n\n\nThis command can take a \nshortened job name\n.\n\n\nSome useful information about the information this command provides:\n\n\nOutput Name\n\n\nOutput name is the reference to the output generated by your run.\n\n\nUrl\n\n\nIf your job is running in \njupyter\n or \nserve\n mode, you can get their URL here.\n\n\nExample\n\n\n$ floyd info floydhub/projects/mnist-tensorboard/1\n-----------  ----------------------------------------\nJob name     floydhub/projects/mnist-tensorboard/1\nOutput name  floydhub/projects/mnist-tensorboard/1\nCreated      \n19\n hours ago\nStatus       success\nDuration\n(\ns\n)\n  \n16\n\nInstance     g1\nDescription\n-----------  ----------------------------------------\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd info"
        }, 
        {
            "location": "/commands/info/#usage", 
            "text": "floyd info  [ OPTIONS ]  NAME", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/info/#options", 
            "text": "Name, shorthand  Default  Description      NAME or ID   Name or ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/info/#description", 
            "text": "This command gives detailed information about your job.  This command can take a  shortened job name .  Some useful information about the information this command provides:", 
            "title": "Description"
        }, 
        {
            "location": "/commands/info/#output-name", 
            "text": "Output name is the reference to the output generated by your run.", 
            "title": "Output Name"
        }, 
        {
            "location": "/commands/info/#url", 
            "text": "If your job is running in  jupyter  or  serve  mode, you can get their URL here.", 
            "title": "Url"
        }, 
        {
            "location": "/commands/info/#example", 
            "text": "$ floyd info floydhub/projects/mnist-tensorboard/1\n-----------  ----------------------------------------\nJob name     floydhub/projects/mnist-tensorboard/1\nOutput name  floydhub/projects/mnist-tensorboard/1\nCreated       19  hours ago\nStatus       success\nDuration ( s )    16 \nInstance     g1\nDescription\n-----------  ----------------------------------------", 
            "title": "Example"
        }, 
        {
            "location": "/commands/info/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/stop/", 
            "text": "Terminate a queued or running job.\n\n\nUsage\n\n\nfloyd stop NAME\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nNAME or ID\n\n\n\n\nName or ID of your job.\n\n\n\n\n\n\n\n\nDescription\n\n\nSometimes you want to terminate a job before it can finish. The stop command sends a request\nto the server to stop the job. You can view the \nstatus\n of the job to confirm. When you stop\na job, you will be charged only for the duration your job was running.\n\n\nThis command can take a \nshortened job name\n.\n\n\nExample\n\n\n$ floyd stop floydhub/projects/mnist-tensorboard/4\nExperiment shutdown request submitted. Check status to confirm shutdown\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd stop"
        }, 
        {
            "location": "/commands/stop/#usage", 
            "text": "floyd stop NAME", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/stop/#options", 
            "text": "Name, shorthand  Default  Description      NAME or ID   Name or ID of your job.", 
            "title": "Options"
        }, 
        {
            "location": "/commands/stop/#description", 
            "text": "Sometimes you want to terminate a job before it can finish. The stop command sends a request\nto the server to stop the job. You can view the  status  of the job to confirm. When you stop\na job, you will be charged only for the duration your job was running.  This command can take a  shortened job name .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/stop/#example", 
            "text": "$ floyd stop floydhub/projects/mnist-tensorboard/4\nExperiment shutdown request submitted. Check status to confirm shutdown", 
            "title": "Example"
        }, 
        {
            "location": "/commands/stop/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/delete/", 
            "text": "Delete one or more floyd jobs.\n\n\nUsage\n\n\nfloyd delete \n[\nIDS\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDefault\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nIDS\n\n\n\n\nOne or more IDs of your job.\n\n\n\n\n\n\n--yes\n, \n-y\n\n\nFalse\n\n\nSkip delete confirmation step\n\n\n\n\n\n\n\n\nDescription\n\n\nDeletes a job from FloydHub. The experiment information is no longer\navailable. You will not be able to access the code uploaded to run the\nexperiment. You need to make sure that the project is currently not\nrunning. If so, you can use the \nstop\n command for that.\n\n\nNote: You do \nnot\n have to be in the project directory to run this command.\n\n\nThis command can take a \nshortened job name\n.\n\n\nExample\n\n\n$ floyd delete floydhub/projects/fastText/1 floydhub/projects/cnr/1\nDelete Run: floydhub/projects/fastText/1? \n[\ny/N\n]\n: y\nExperiment deleted\nDelete Run: floydhub/projects/cnr/1? \n[\ny/N\n]\n: y\nExperiment deleted\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd delete"
        }, 
        {
            "location": "/commands/delete/#usage", 
            "text": "floyd delete  [ IDS ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/delete/#options", 
            "text": "Name, shorthand  Default  Description      IDS   One or more IDs of your job.    --yes ,  -y  False  Skip delete confirmation step", 
            "title": "Options"
        }, 
        {
            "location": "/commands/delete/#description", 
            "text": "Deletes a job from FloydHub. The experiment information is no longer\navailable. You will not be able to access the code uploaded to run the\nexperiment. You need to make sure that the project is currently not\nrunning. If so, you can use the  stop  command for that.  Note: You do  not  have to be in the project directory to run this command.  This command can take a  shortened job name .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/delete/#example", 
            "text": "$ floyd delete floydhub/projects/fastText/1 floydhub/projects/cnr/1\nDelete Run: floydhub/projects/fastText/1?  [ y/N ] : y\nExperiment deleted\nDelete Run: floydhub/projects/cnr/1?  [ y/N ] : y\nExperiment deleted", 
            "title": "Example"
        }, 
        {
            "location": "/commands/delete/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/logout/", 
            "text": "Logout from Floyd.\n\n\nDescription\n\n\nLogout the CLI from Floyd\n\n\nUsage\n\n\nfloyd \nlogout\n\n\n\n\n\nDescription\n\n\nLogs you out and expires your current token. You will need to login again to run further commands.\n\n\nExample\n\n\n$ floyd \nlogout\n\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd logout"
        }, 
        {
            "location": "/commands/logout/#logout-from-floyd", 
            "text": "", 
            "title": "Logout from Floyd."
        }, 
        {
            "location": "/commands/logout/#description", 
            "text": "Logout the CLI from Floyd", 
            "title": "Description"
        }, 
        {
            "location": "/commands/logout/#usage", 
            "text": "floyd  logout", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/logout/#description_1", 
            "text": "Logs you out and expires your current token. You will need to login again to run further commands.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/logout/#example", 
            "text": "$ floyd  logout", 
            "title": "Example"
        }, 
        {
            "location": "/commands/logout/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/upgrade/", 
            "text": "Upgrade the floyd client.\n\n\nUsage\n\n\nfloyd upgrade\n\n\n\n\nDescription\n\n\nThis will upgrade the floyd cli to the latest version using pip.\n\n\nExample\n\n\n$ floyd upgrade\nCollecting floyd-cli from ...\n...\nSuccessfully installed floyd-cli-0.9.1\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd upgrade"
        }, 
        {
            "location": "/commands/upgrade/#usage", 
            "text": "floyd upgrade", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/upgrade/#description", 
            "text": "This will upgrade the floyd cli to the latest version using pip.", 
            "title": "Description"
        }, 
        {
            "location": "/commands/upgrade/#example", 
            "text": "$ floyd upgrade\nCollecting floyd-cli from ...\n...\nSuccessfully installed floyd-cli-0.9.1", 
            "title": "Example"
        }, 
        {
            "location": "/commands/upgrade/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/version/", 
            "text": "Get the current version of floyd-cli\n\n\nUsage\n\n\nfloyd version\n\n\n\n\nDescription\n\n\nPrints the current version of floyd-cli\n\n\nExample\n\n\n$ floyd version\n\n0\n.9.1\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd version"
        }, 
        {
            "location": "/commands/version/#usage", 
            "text": "floyd version", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/version/#description", 
            "text": "Prints the current version of floyd-cli", 
            "title": "Description"
        }, 
        {
            "location": "/commands/version/#example", 
            "text": "$ floyd version 0 .9.1", 
            "title": "Example"
        }, 
        {
            "location": "/commands/version/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/commands/restart/", 
            "text": "Restat/re-run a previous job\n\n\nUsage\n\n\nfloyd restart JOB_NAME \n[\nOPTIONS\n]\n \n[\nCOMMAND\n]\n\n\n\n\n\nOptions\n\n\n\n\n\n\n\n\nName, shorthand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nJOB_NAME\n\n\nName or ID of your job.\n\n\n\n\n\n\nAll the \n[OPTIONS]\n and \n[COMMAND]\n of \nfloyd run\n as documented \nhere\n\n\n\n\n\n\n\n\n\n\nDescription\n\n\nUse this command to restart/re-run a previous job.  Any \n[OPTIONS]\n or\n\n[COMMAND]\n passed to \nfloyd restart\n will override the \n[OPTIONS]\n or\n\n[COMMAND]\n of the original job. This can be useful if you want to\nrestart/re-run a previous job with different \n[OPTIONS]\n (like a different\ninstance type), or override its \n[COMMAND]\n.\n\n\nWhen using \nfloyd restart\n, a copy of your code is not uploaded (since the code\nof the previous job is used). This can save on startup time.\n\n\nThis command can take a \nshortened job name\n.\n\n\nExamples\n\n\n# Override the command\n$ floyd restart mckay/projects/mnist/1 \npython train.py\n\n\n\n\n\n# Run the job on a GPU server\n$ floyd restart mckay/projects/mnist/1 --gpu\n\n\n\n\n# Run the job with a new version of a dataset\n$ floyd restart mckay/projects/mnist/1 --data mckay/datasets/mnist/1:mnist\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "floyd restart"
        }, 
        {
            "location": "/commands/restart/#usage", 
            "text": "floyd restart JOB_NAME  [ OPTIONS ]   [ COMMAND ]", 
            "title": "Usage"
        }, 
        {
            "location": "/commands/restart/#options", 
            "text": "Name, shorthand  Description      JOB_NAME  Name or ID of your job.    All the  [OPTIONS]  and  [COMMAND]  of  floyd run  as documented  here", 
            "title": "Options"
        }, 
        {
            "location": "/commands/restart/#description", 
            "text": "Use this command to restart/re-run a previous job.  Any  [OPTIONS]  or [COMMAND]  passed to  floyd restart  will override the  [OPTIONS]  or [COMMAND]  of the original job. This can be useful if you want to\nrestart/re-run a previous job with different  [OPTIONS]  (like a different\ninstance type), or override its  [COMMAND] .  When using  floyd restart , a copy of your code is not uploaded (since the code\nof the previous job is used). This can save on startup time.  This command can take a  shortened job name .", 
            "title": "Description"
        }, 
        {
            "location": "/commands/restart/#examples", 
            "text": "# Override the command\n$ floyd restart mckay/projects/mnist/1  python train.py   # Run the job on a GPU server\n$ floyd restart mckay/projects/mnist/1 --gpu  # Run the job with a new version of a dataset\n$ floyd restart mckay/projects/mnist/1 --data mckay/datasets/mnist/1:mnist", 
            "title": "Examples"
        }, 
        {
            "location": "/commands/restart/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/examples/style_transfer/", 
            "text": "Neural Style Transfer is an algorithm for combining the content of one image with the style of another image\nusing convolutional neural networks. Here's an example that maps the artistic style of The Starry Night\nonto a night-time photograph of the Stanford campus:\n\n\n\n\n\n\n\n\nWe will use this example to demonstrate how Floyd can be used to deploy your trained model as a REST API endpoint that can be accessed over the web.\nThis feature is very useful if you want to quickly compare models or have others play with your models. This guide will\nwalk you through how to do this.\n\n\nSetup project\n\n\nFor this guide we will be using \nFast Style Transfer\n\nproject.\n\n\n$ git clone https://github.com/floydhub/fast-style-transfer\n$ \ncd\n fast-style-transfer\n$ floyd init fast-style-transfer\nProject \nfast-style-transfer\n initialized in the current directory\n\n\n\n\nTrain a model\n\n\nYou can train your model by running the \nstyle.py\n script in this repo on Floyd. You can specify any style image to use in the command line. Just\ndownload it and keep it in current path. In this example we will be starting from a\n\npre-trained model\n.\n\n\nTraining data\n\n\nThis project also requires access to the imagenet-vgg-verydeep-19 model and image training data. Floyd already has this data source available.\nYou can mount this at runtime using the \n--data\n parameter.\n\n\nTraining\n\n\n$ floyd run --gpu --env tensorflow-0.12:py2 --data narenst/datasets/coco-train-2014/1:images --data narenst/datasets/neural-style-transfer-pre-trained-models/1:models --data floydhub/datasets/imagenet-vgg-verydeep-19/3:vgg \npython style.py --vgg-path /vgg/imagenet-vgg-verydeep-19.mat --train-path /images/train2014 --style examples/style/la_muse.jpg --base-model-path /models/la_muse.ckpt --epoch 1 --total-iterations 10 --checkpoint-dir /output\n\n\n\n\n\nThis will kick off a new job on Floyd. This will take a few minutes to run and will generate the model. You can follow along the progress\nby using the \nlogs\n command.\n\n\n$ floyd logs \nJOB_NAME\n -t\n\n\nNow you need to \nget the name\n of the \nOutput\n generated by your job. Floyd \ninfo\n can give you that information.\n\n\n$ floyd info \nJOB_NAME\n\n\n\n\n\nEvaluate your model\n\n\nYou can evaluate the generated model by running \nevaluate.py\n on sample images. Use the output name from the training step\nas the datasource in this step. Add any image you want to style transfer to the \nimages\n directory. Then run \nevaluate.py\n.\n\n\nfloyd run --env tensorflow-0.12:py2 --data \nREPLACE_WITH_OUTPUT_NAME\n:input \npython evaluate.py --allow-different-dimensions  --checkpoint /input/fns.ckpt --in-path ./images/ --out-path /output/\n\n\n\nYou can track the status of the run with the status or logs command.\n\n\n$ floyd status \nJOB_NAME\n\n$ floyd logs \nJOB_NAME\n -t\n\n\n\n\nAfter the job finishes successfully, view the output directory to see the style transferred images. Run the floyd \noutput\n\nfor this.\n\n\n$ floyd output \nJOB_NAME\n\n\n\n\n\nImproving the model\n\n\nYou may notice that the output does not look great. That is because we ran the training for a small number of iterations. To train\na fully working model try the train step again, this time without setting \n--total-iterations\n and increasing the \n--epoch\n to 2.\nIt takes about 8 hours to train a model that works well. You can instead try one of our pre-trained models in the next section.\n\n\nEvaluate pre-trained models\n\n\nIf you want to try out some awesome pre-trained models for various styles, you can use the datasource with models available publicly.\nYou can play with any of these model and style transfer any image you prefer. Just add them to \nimages\n directory. And point to the\nright model in the \n--checkpoint\n parameter.\n\n\nfloyd run --env tensorflow-0.12:py2 --data narenst/datasets/neural-style-transfer-pre-trained-models/1:models \npython evaluate.py --allow-different-dimensions  --checkpoint /models/la_muse.ckpt --in-path ./images/ --out-path /output/\n\n\n\n\n\nYou can track the status of the run with the status command.\n\n\n$ floyd status \nJOB_NAME\n\n\n\n\n\nWhen the experiment is finished, you can see the style transferred images by running:\n\n\n$ floyd output \nJOB_NAME\n\n\n\n\n\n\n\nModel API\n\n\nYou can now host this model as a REST API. This means you can send any image to this API as a HTTP request and it will be style transferred.\n\n\nServe mode\n\n\nFloyd \nrun\n command has a \nserve\n mode. This will upload the files in the current directory and run a special command -\n\npython app.py\n. Floyd expects this file to contain the code to run a web server and listen on port \n5000\n. You can see the\n\napp.py\n file in the sample repository. This file handles the\nincoming request, executes the code in \nevaluate.py\n and returns the output. Before serving your model through REST API,\nyou need to create a \nfloyd_requirements.txt\n and declare the flask requirement in it.\n\n\nNote that this feature is in preview mode and is not production ready yet\n\n\n$ floyd run --env tensorflow-0.12:py2 --data narenst/datasets/neural-style-transfer-pre-trained-models/1:input --mode serve\nSyncing code ...\nRUN ID                  NAME                              VERSION\n----------------------  ------------------------------  ---------\nDJSdJAVa3u7AsFEMZMBBL5  floydhub/fast-style-transfer:5          \n5\n\n\nPath to service endpoint: https://www.floydhub.com/expose/t4AdkU6awahkT3ooNazw8c\n\nTo view logs enter:\n    floyd logs DJSdJAVa3u7AsFEMZMBBL5\n\n\n\n\nSending requests to the REST API\n\n\nNow you can send any image file as request to this api and it will return the style transferred image.\n\n\ncurl -o taipei_output.jpg -F \nfile=@./images/taipei101.jpg\n https://www.floydhub.com/expose/t4AdkU6awahkT3ooNazw8c\n\n\n\n\n\n\nYou will see the default style (\nla_muse\n) is applied to the input image.\n\n\nTrying out different models\n\n\nYou can also pass in the name of the checkpoint to use and the image will be style transferred accordingly:\n\n\ncurl -o taipei_udnie.jpg -F \nfile=@./images/taipei101.jpg\n -F \ncheckpoint=udnie.ckpt\n  https://www.floydhub.com/expose/MUDFXViCLArG2drppvU3nm\n\n\n\n\n\n\nThis uses a different style checkpoint to render the image. All the logic for this is present in the \napp.py\n file. You can update it to\nbe as complex as you prefer.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Style Transfer"
        }, 
        {
            "location": "/examples/style_transfer/#setup-project", 
            "text": "For this guide we will be using  Fast Style Transfer \nproject.  $ git clone https://github.com/floydhub/fast-style-transfer\n$  cd  fast-style-transfer\n$ floyd init fast-style-transfer\nProject  fast-style-transfer  initialized in the current directory", 
            "title": "Setup project"
        }, 
        {
            "location": "/examples/style_transfer/#train-a-model", 
            "text": "You can train your model by running the  style.py  script in this repo on Floyd. You can specify any style image to use in the command line. Just\ndownload it and keep it in current path. In this example we will be starting from a pre-trained model .", 
            "title": "Train a model"
        }, 
        {
            "location": "/examples/style_transfer/#training-data", 
            "text": "This project also requires access to the imagenet-vgg-verydeep-19 model and image training data. Floyd already has this data source available.\nYou can mount this at runtime using the  --data  parameter.", 
            "title": "Training data"
        }, 
        {
            "location": "/examples/style_transfer/#training", 
            "text": "$ floyd run --gpu --env tensorflow-0.12:py2 --data narenst/datasets/coco-train-2014/1:images --data narenst/datasets/neural-style-transfer-pre-trained-models/1:models --data floydhub/datasets/imagenet-vgg-verydeep-19/3:vgg  python style.py --vgg-path /vgg/imagenet-vgg-verydeep-19.mat --train-path /images/train2014 --style examples/style/la_muse.jpg --base-model-path /models/la_muse.ckpt --epoch 1 --total-iterations 10 --checkpoint-dir /output   This will kick off a new job on Floyd. This will take a few minutes to run and will generate the model. You can follow along the progress\nby using the  logs  command.  $ floyd logs  JOB_NAME  -t \nNow you need to  get the name  of the  Output  generated by your job. Floyd  info  can give you that information.  $ floyd info  JOB_NAME", 
            "title": "Training"
        }, 
        {
            "location": "/examples/style_transfer/#evaluate-your-model", 
            "text": "You can evaluate the generated model by running  evaluate.py  on sample images. Use the output name from the training step\nas the datasource in this step. Add any image you want to style transfer to the  images  directory. Then run  evaluate.py .  floyd run --env tensorflow-0.12:py2 --data  REPLACE_WITH_OUTPUT_NAME :input  python evaluate.py --allow-different-dimensions  --checkpoint /input/fns.ckpt --in-path ./images/ --out-path /output/  \nYou can track the status of the run with the status or logs command.  $ floyd status  JOB_NAME \n$ floyd logs  JOB_NAME  -t  After the job finishes successfully, view the output directory to see the style transferred images. Run the floyd  output \nfor this.  $ floyd output  JOB_NAME", 
            "title": "Evaluate your model"
        }, 
        {
            "location": "/examples/style_transfer/#improving-the-model", 
            "text": "You may notice that the output does not look great. That is because we ran the training for a small number of iterations. To train\na fully working model try the train step again, this time without setting  --total-iterations  and increasing the  --epoch  to 2.\nIt takes about 8 hours to train a model that works well. You can instead try one of our pre-trained models in the next section.", 
            "title": "Improving the model"
        }, 
        {
            "location": "/examples/style_transfer/#evaluate-pre-trained-models", 
            "text": "If you want to try out some awesome pre-trained models for various styles, you can use the datasource with models available publicly.\nYou can play with any of these model and style transfer any image you prefer. Just add them to  images  directory. And point to the\nright model in the  --checkpoint  parameter.  floyd run --env tensorflow-0.12:py2 --data narenst/datasets/neural-style-transfer-pre-trained-models/1:models  python evaluate.py --allow-different-dimensions  --checkpoint /models/la_muse.ckpt --in-path ./images/ --out-path /output/   You can track the status of the run with the status command.  $ floyd status  JOB_NAME   When the experiment is finished, you can see the style transferred images by running:  $ floyd output  JOB_NAME", 
            "title": "Evaluate pre-trained models"
        }, 
        {
            "location": "/examples/style_transfer/#model-api", 
            "text": "You can now host this model as a REST API. This means you can send any image to this API as a HTTP request and it will be style transferred.", 
            "title": "Model API"
        }, 
        {
            "location": "/examples/style_transfer/#serve-mode", 
            "text": "Floyd  run  command has a  serve  mode. This will upload the files in the current directory and run a special command - python app.py . Floyd expects this file to contain the code to run a web server and listen on port  5000 . You can see the app.py  file in the sample repository. This file handles the\nincoming request, executes the code in  evaluate.py  and returns the output. Before serving your model through REST API,\nyou need to create a  floyd_requirements.txt  and declare the flask requirement in it.  Note that this feature is in preview mode and is not production ready yet  $ floyd run --env tensorflow-0.12:py2 --data narenst/datasets/neural-style-transfer-pre-trained-models/1:input --mode serve\nSyncing code ...\nRUN ID                  NAME                              VERSION\n----------------------  ------------------------------  ---------\nDJSdJAVa3u7AsFEMZMBBL5  floydhub/fast-style-transfer:5           5 \n\nPath to service endpoint: https://www.floydhub.com/expose/t4AdkU6awahkT3ooNazw8c\n\nTo view logs enter:\n    floyd logs DJSdJAVa3u7AsFEMZMBBL5", 
            "title": "Serve mode"
        }, 
        {
            "location": "/examples/style_transfer/#sending-requests-to-the-rest-api", 
            "text": "Now you can send any image file as request to this api and it will return the style transferred image.  curl -o taipei_output.jpg -F  file=@./images/taipei101.jpg  https://www.floydhub.com/expose/t4AdkU6awahkT3ooNazw8c   You will see the default style ( la_muse ) is applied to the input image.", 
            "title": "Sending requests to the REST API"
        }, 
        {
            "location": "/examples/style_transfer/#trying-out-different-models", 
            "text": "You can also pass in the name of the checkpoint to use and the image will be style transferred accordingly:  curl -o taipei_udnie.jpg -F  file=@./images/taipei101.jpg  -F  checkpoint=udnie.ckpt   https://www.floydhub.com/expose/MUDFXViCLArG2drppvU3nm   This uses a different style checkpoint to render the image. All the logic for this is present in the  app.py  file. You can update it to\nbe as complex as you prefer.", 
            "title": "Trying out different models"
        }, 
        {
            "location": "/examples/style_transfer/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/examples/deep_corrector/", 
            "text": "Deep Text Corrector is an Tensorflow project made by \nAlex Paino\n for correcting\ngrammatical errors in short sentences. For example, the message \"I'm going to\nstore\" would be unaffected by typical autocorrection systems, when the user\nmost likely intendend to write \"I'm going to \nthe\n store\".\n\n\nIn this guide we will train a Tensorflow model for correcting sentences and use\nit to evaluate input sentences.  Finally we will deploy the trained model as a\nREST endpoint that can be used to evaluate input sequences in real time.\n\n\nProject setup\n\n\nThe code for this project is available on Floyd's \nGithub page\n. Clone the project and\n\ninitialize\n a floyd project.\n\n\n$ git clone https://github.com/floydhub/deep-text-corrector\n$ \ncd\n deep-text-corrector\n$ floyd init deep-text-corrector\n\n\n\n\nTraining\n\n\nDataset\n\n\nFor this project we will use the Cornel Movie-Dialogs Corpus for training and testing.\nThe dataset should be preprocessed and split into 3 sets: 80% for training, and 10%\neach for validation and testing. This preprocessed dataset is available publicly on\n\nFloydHub\n.\n\n\nTraining\n\n\nYou can train the deep corrector model by running \ncorrect_text.py\n script with required\nparameters. Below is the \ncommand\n to start a training job on Floyd:\n\n\n$ floyd run --gpu --env tensorflow-0.12:py2 --data floydhub/datasets/deep-text-corrector/1:input \npython correct_text.py --num_steps 1000 --train_path /input/data/movie_dialog_train.txt --val_path /input/data/movie_dialog_val.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --output_path /output\n\n\n\n\n\nNotes:\n\n\n\n\nThe input dataset is passed using the \n--data\n parameter. This mounts the pre-processed\nCornell Movie Dialog dataset at \n/input\n path. You will notice that other parameters use files\nmounted in this path.\n\n\nThe data name \nfloydhub/datasets/deep-text-corrector/1\n\npoints to the pre-processed dataset on FloydHub.\n\n\nThe job is running on a gpu instance (Because of the \n--gpu\n flag).\n\n\nThis project uses Tensorflow-0.12 installed on Python 2. (See the \n--env\n flag)\n\n\n\n\nThis job takes about 10 minutes to run and generate a model. You can follow along the progress\nby using the \nlogs\n command.\n\n\n$ floyd logs \nJOB_NAME\n -t\n\n\n\n\nFloyd saves any content stored in the \n/output\n directory after the job is\nfinished. This output can be used as a datasource in the next project.  To get\nthe name of the output generated by your job use the\n\ninfo\n command.\n\n\n$ floyd info \nJOB_NAME\n\n\n\n\n\nEvaluating\n\n\nTo evaluate your model you can run the \ncorrect_text.py\n script with the \ndecode\n flag.\nYou need a file containing short messages for evaluation. The \ntest.txt\n file already has some\ninputs. You can update or add more strings to this file - one per line. You also need to\nuse the output from the training step above as the datasource in this step.\n\n\nfloyd run --env tensorflow-0.12:py2 --data \nREPLACE_WITH_JOB_OUTPUT_NAME\n:input \npython correct_text.py --train_path /input/data/movie_dialog_train.txt --test_path test.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --input_path /input --decode\n\n\n\n\n\nYou can track the status of the run with the status or logs command. The logs should print the\nconcerted messages from the test.txt file.\n\n\n$ floyd status \nJOB_NAME\n\n$ floyd logs \nJOB_NAME\n -t\n\n\n\n\nImproving your model\n\n\nYou may notice that the output does not look great. In fact, the algorithm would've added more\nmistakes into the sentences than correct it. That is because we ran the training for a small number\nof iterations. To train a fully working model try the training step again, this time by setting\nthe flag \nnum_steps\n to a large value. In general, about 20000 steps are necessary to give a\nworking corrector model. (Note: This takes a few hours to run on the GPU instance)\n\n\nEvaluate pre-trained models\n\n\nIf you want to try out a pre-trained model, FloydHub has a public job output for\nthis. You can mount it with job output name:\n\nfloydhub/deep-text-corrector/23/output\n\n.\n\n\nfloyd run --env tensorflow-0.12:py2 --data floydhub/deep-text-corrector/23/output:input \npython correct_text.py --train_path /input/data/movie_dialog_train.txt --test_path test.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --input_path /input --decode\n\n\n\n\n\nThis model should perform better on the given inputs compared to the previous one.\n\n\nServe model through REST API\n\n\nFloydHub supports seving mode for demo and testing purpose. If you run a job\nwith \n--mode serve\n flag, FloydHub will run the \napp.py\n file in your project\nand attach it to a dynamic service endpoint:\n\n\nfloyd run --mode serve --env tensorflow-0.12:py2 --data floydhub/deep-text-corrector/23/output:input\n\n\n\n\nThe above command will print out a service endpoint for this job in your terminal console.\n\n\nThe service endpoint will take couple minutes to become ready. Once it's up, you can interact with the model by sending text you want to correct:\n\n\ncurl -X POST -d \nI see it tomorrow\n \nREPLACE_WITH_YOUR_SERVICE_ENDPOINT\n\n\n\n\n\nAny job running in serving mode will stay up until it reaches maximum runtime. So\nonce you are done testing, remember to shutdown the job.\n\n\nNote that this feature is in preview mode and is not production ready yet\n\n\nWhat Next?\n\n\nThe model was trained using movie dialogues which are not the greatest sources of gramatically correct\nsentences. An improvement to this approach would be to use other datasources like \nProject Gutenberg\n.\nThis project was also discussed on \nHackerNews\n and you can\nfind lots of interesting alternatives there.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Deep Text Corrector"
        }, 
        {
            "location": "/examples/deep_corrector/#project-setup", 
            "text": "The code for this project is available on Floyd's  Github page . Clone the project and initialize  a floyd project.  $ git clone https://github.com/floydhub/deep-text-corrector\n$  cd  deep-text-corrector\n$ floyd init deep-text-corrector", 
            "title": "Project setup"
        }, 
        {
            "location": "/examples/deep_corrector/#training", 
            "text": "", 
            "title": "Training"
        }, 
        {
            "location": "/examples/deep_corrector/#dataset", 
            "text": "For this project we will use the Cornel Movie-Dialogs Corpus for training and testing.\nThe dataset should be preprocessed and split into 3 sets: 80% for training, and 10%\neach for validation and testing. This preprocessed dataset is available publicly on FloydHub .", 
            "title": "Dataset"
        }, 
        {
            "location": "/examples/deep_corrector/#training_1", 
            "text": "You can train the deep corrector model by running  correct_text.py  script with required\nparameters. Below is the  command  to start a training job on Floyd:  $ floyd run --gpu --env tensorflow-0.12:py2 --data floydhub/datasets/deep-text-corrector/1:input  python correct_text.py --num_steps 1000 --train_path /input/data/movie_dialog_train.txt --val_path /input/data/movie_dialog_val.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --output_path /output   Notes:   The input dataset is passed using the  --data  parameter. This mounts the pre-processed\nCornell Movie Dialog dataset at  /input  path. You will notice that other parameters use files\nmounted in this path.  The data name  floydhub/datasets/deep-text-corrector/1 \npoints to the pre-processed dataset on FloydHub.  The job is running on a gpu instance (Because of the  --gpu  flag).  This project uses Tensorflow-0.12 installed on Python 2. (See the  --env  flag)   This job takes about 10 minutes to run and generate a model. You can follow along the progress\nby using the  logs  command.  $ floyd logs  JOB_NAME  -t  Floyd saves any content stored in the  /output  directory after the job is\nfinished. This output can be used as a datasource in the next project.  To get\nthe name of the output generated by your job use the info  command.  $ floyd info  JOB_NAME", 
            "title": "Training"
        }, 
        {
            "location": "/examples/deep_corrector/#evaluating", 
            "text": "To evaluate your model you can run the  correct_text.py  script with the  decode  flag.\nYou need a file containing short messages for evaluation. The  test.txt  file already has some\ninputs. You can update or add more strings to this file - one per line. You also need to\nuse the output from the training step above as the datasource in this step.  floyd run --env tensorflow-0.12:py2 --data  REPLACE_WITH_JOB_OUTPUT_NAME :input  python correct_text.py --train_path /input/data/movie_dialog_train.txt --test_path test.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --input_path /input --decode   You can track the status of the run with the status or logs command. The logs should print the\nconcerted messages from the test.txt file.  $ floyd status  JOB_NAME \n$ floyd logs  JOB_NAME  -t", 
            "title": "Evaluating"
        }, 
        {
            "location": "/examples/deep_corrector/#improving-your-model", 
            "text": "You may notice that the output does not look great. In fact, the algorithm would've added more\nmistakes into the sentences than correct it. That is because we ran the training for a small number\nof iterations. To train a fully working model try the training step again, this time by setting\nthe flag  num_steps  to a large value. In general, about 20000 steps are necessary to give a\nworking corrector model. (Note: This takes a few hours to run on the GPU instance)", 
            "title": "Improving your model"
        }, 
        {
            "location": "/examples/deep_corrector/#evaluate-pre-trained-models", 
            "text": "If you want to try out a pre-trained model, FloydHub has a public job output for\nthis. You can mount it with job output name: floydhub/deep-text-corrector/23/output \n.  floyd run --env tensorflow-0.12:py2 --data floydhub/deep-text-corrector/23/output:input  python correct_text.py --train_path /input/data/movie_dialog_train.txt --test_path test.txt --config DefaultMovieDialogConfig --data_reader_type MovieDialogReader --input_path /input --decode   This model should perform better on the given inputs compared to the previous one.", 
            "title": "Evaluate pre-trained models"
        }, 
        {
            "location": "/examples/deep_corrector/#serve-model-through-rest-api", 
            "text": "FloydHub supports seving mode for demo and testing purpose. If you run a job\nwith  --mode serve  flag, FloydHub will run the  app.py  file in your project\nand attach it to a dynamic service endpoint:  floyd run --mode serve --env tensorflow-0.12:py2 --data floydhub/deep-text-corrector/23/output:input  The above command will print out a service endpoint for this job in your terminal console.  The service endpoint will take couple minutes to become ready. Once it's up, you can interact with the model by sending text you want to correct:  curl -X POST -d  I see it tomorrow   REPLACE_WITH_YOUR_SERVICE_ENDPOINT   Any job running in serving mode will stay up until it reaches maximum runtime. So\nonce you are done testing, remember to shutdown the job.  Note that this feature is in preview mode and is not production ready yet", 
            "title": "Serve model through REST API"
        }, 
        {
            "location": "/examples/deep_corrector/#what-next", 
            "text": "The model was trained using movie dialogues which are not the greatest sources of gramatically correct\nsentences. An improvement to this approach would be to use other datasources like  Project Gutenberg .\nThis project was also discussed on  HackerNews  and you can\nfind lots of interesting alternatives there.", 
            "title": "What Next?"
        }, 
        {
            "location": "/examples/deep_corrector/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/examples/dcgan/", 
            "text": "Deep Convolutional Generative Adversarial Networks are a class of CNN and one\nof the first approaches that made GANs stable and usable for learning\nfeatures from images in unsupervised learning. This project is a port of the\n\npytorch/examples/dcgan\n.\nAt the end of this example you will be able to use DCGANs for generating images\nfrom your dataset.\n\n\n\n\nIn this guide we will train a \nPytorch\n model in an\nunsupervised task and use it to generate images from an input vector Z(100\ndimensional uniform distribution). Finally, we will deploy the trained model as\na REST endpoint that can be used to generate images in real time.\n\n\nProject setup\n\n\nThe code for this project is available on this \nGitHub\nrepository\n. Once \nlogged\nin\n, clone the project and \ninitialize\n a\nfloyd project.\n\n\n$ git clone https://github.com/floydhub/dcgan.git\n$ \ncd\n dcgan\n$ floyd init dcgan\n\n\n\n\nTraining\n\n\nDataset\n\n\nFor this project we will use the \nLabeled Faces in the Wild Home\n, aka LFW, for the training.\nSince we are learning in an unsupervised regime, there is not a train/dev/test split, but we use the BCE Loss Error of the Discriminator and Generator as good metrics to learn (minmax game strategy from Game Theory). Moreover, we can visualize the generated images every epoch.\nThis preprocessed dataset is available publicly on\n\nFloydHub\n.\n\n\nTraining\n\n\nYou can train the DCGAN model by running \nmain.py\n script with required\nparameters. Below is the \ncommand\n to start the training job on Floyd:\n\n\n$ floyd run \n\\\n\n--gpu \n\\\n\n--env pytorch-0.2 \n\\\n\n--data redeipirati/datasets/lfw/1:lfw \n\\\n\n\npython main.py --dataset lfw --dataroot /lfw --outf /output --cuda --ngpu 1 --niter 20\n\n\n\n\n\nNotes:\n\n\n\n\n--gpu\n ensures the job will run on a GPU server.\n\n\n--env pytorch-0.2\n ensures the job will be run in FloydHub's \npytorch-0.2\n\n  environment.\n\n\nThe input dataset is passed using \n--data redeipirati/datasets/lfw/1:lfw\n.\n  This mounts the pre-processed LFW dataset (available\n  \nhere\n under FloydHub\n  user \n@redeipirati\n's account) at\n  \n/lfw\n. Also note that \nmain.py\n's \n--dataroot\n flag specifies \n/lfw\n as the\n  path where the data will be available. This ensures that the\n  \ncode knows where to find the dataset\n.\n\n\n\n\nThis job takes about 20 minutes to run and generate a model. You can follow along the progress\nby using the \nlogs\n command. If you run the model with default value it will take about 1-5 minutes per epoch.\n\n\n$ floyd logs \nJOB_NAME\n -t\n\n\n\n\nFloyd saves any content stored in the \n/output\n directory at the end of the\njob. This output can be used as a datasource in the next project. To get the\nname of the output generated by your job use the \ninfo\n\ncommand.\n\n\n$ floyd info \nJOB_NAME\n\n\n\n\n\nEvaluating\n\n\nTo evaluate your model you can run the\n\ngenerate.py\n script included in the GitHub repository\n.\nThe script needs the path of a checkpoint Generator model, which you pass to\nthe script using its \n--netG\n flag. Just make sure that you mount the\ncheckpoint Generator model at the path you pass to the \n--netG\n flag. Below, we\nmount the model at \n/model\n. You can also provide a serialized Zvector\n(\n--Zvector\n) to experiment with latent Z vector arithmetic landscape and for\nanalyzing the semantic information encoded during training.\n\n\nfloyd run \n\\\n\n--gpu \n\\\n\n--env pytorch-0.2 \n\\\n\n--data \nREPLACE_WITH_JOB_OUTPUT_NAME\n:model \n\\\n\n\npython generate.py --netG /model/\nREPLACE_WITH_MODEL_CHECKPOINT_PATH\n --ngpu 1 --cuda\n\n\n\n\n\n# Provide a serialized Zvector\n\nfloyd run \n\\\n\n--gpu \n\\\n\n--env pytorch-0.2 \n\\\n\n--data \nREPLACE_WITH_JOB_OUTPUT_NAME\n:model \n\\\n\n\npython generate.py --netG /model/\nREPLACE_WITH_MODEL_CHECKPOINT_PATH\n --Zvector /model/\nREPLACE_WITH_SERIALIZED_Z_VECTOR_PATH\n --ngpu 1 --cuda\n\n\n\n\n\nYou can track the status of the run with the status or logs command.\n\n\n$ floyd status \nJOB_NAME\n\n$ floyd logs \nJOB_NAME\n -t\n\n\n\n\nImproving your model\n\n\nYou may notice that the output does not look great. In fact, the algorithm has\nnot yet learned how to correctly represent a face.  That is because we ran the\ntraining for a small number of iterations. To train a fully working model, try\nthe training step again, this time setting the flag \n--niter\n to a large value,\nsuch as 300. In general, about 300/500 epochs (or even more, much more!) are\nnecessary to have an accetable model. (Note: This takes a few hours to run on\nthe GPU instance!)\nKeep in mind that \nall the classes of generative networks are neither stable\nnor production ready\n, this is an exciting field of research and everyone can\ncontribute with new ideas.\n\n\nEvaluate pre-trained models\n\n\nIf you want to try out a pre-trained model, FloydHub has a public dataset model\nwith the checkpoints (300 epochs training) for this located \nhere\n. You can mount it with\n\n--data redeipirati/datasets/dcgan-300-epochs-models/1:/model\n.\n\n\nfloyd run \n\\\n\n--gpu \n\\\n\n--env pytorch-0.2 \n\\\n\n--data redeipirati/datasets/dcgan-300-epochs-models/1:/model \n\\\n\n\npython generate.py --netG /model/netG_epoch_299.pth --ngpu 1 --cuda\n\n\n\n\n\nThis model should perform better compared to the previous one. You can also provide the \n--Zvector\n parameter to explore the latent Z vector landscape. We have also provided to you the zvector used for evaluating our model in the attached dataset:\n\n\nfloyd run --gpu \n\\\n\n--env pytorch-0.2  \n\\\n\n--data redeipirati/datasets/dcgan-300-epochs-models/1:/model \n\\\n\n\npython generate.py --netG /model/netG_epoch_299.pth --Zvector /model/zvector.pth --ngpu 1 --cuda\n\n\n\n\n\nServe the Model with a REST API\n\n\nFloydHub supports seving mode for demo and testing purposes. Before serving\nyour model through a REST API, you need to create a \nfloyd_requirements.txt\n\nand declare the \nflask\n requirement in it. If you run a job with \n--mode serve\n\nflag, FloydHub will run the \napp.py\n file in your project and attach it to a\ndynamic service endpoint:\n\n\nfloyd run \n\\\n\n--gpu \n\\\n\n--mode serve \n\\\n\n--env pytorch-0.2 \n\\\n\n--data \nREPLACE_WITH_JOB_OUTPUT_NAME\n:input\n\n\n\n\nThe above command will print out a service endpoint for this job in your terminal console.\n\n\nThe service endpoint will take couple minutes to become ready. Once it's up, you can interact with the model by sending a serialized Zvector file with a POST request or simply generate images from random noise with a GET request(you can also use the \nckp\n parameter to chose a specific checkpoint):\n\n\n# e.g. of a GET req\n\ncurl -X GET -o \nNAME_\n_PATH_DOWNLOADED_IMG\n -F \nckp=\nMODEL_CHECKPOINT\n \nSERVICE_ENDPOINT\n\ncurl -X GET -o prova.png -F \nckp=netG_epoch_99.pth\n https://www.floydhub.com/expose/hellllllllo!!!!\n\n\n# e.g. of a POST req\n\ncurl -X POST -o \nNAME_\n_PATH_DOWNLOADED_IMG\n -F \nfile=@\nZVECTOR_SERIALIZED_PATH\n \nSERVICE_ENDPOINT\n\ncurl -X POST -o prova.png -F \nfile=@./parameter/zvector.pth\n https://www.floydhub.com/expose/hellllllllo!!!!\n\n\n\n\nAny job running in serving mode will stay up until it reaches maximum runtime. So\nonce you are done testing, \nremember to shutdown the job.\n\n\nNote that this feature is in preview mode and is not production ready yet\n\n\nWhat Next?\n\n\nIn the original paper the model was trained on the\n\nLSUN\n dataset, and then the learned features were\nused to perform an image classification task on the CIFAR-10 dataset. DCGAN was\none of the first stable models based on GAN and the first which tried to\nlearn features from images in an unsupervised regime. GAN is an extremely\nactive research area because they can provide an unlimited amount of high\nquality data which is necessary to train Deep Learning models.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Deep Convolutional Generative Adversarial Networks"
        }, 
        {
            "location": "/examples/dcgan/#project-setup", 
            "text": "The code for this project is available on this  GitHub\nrepository . Once  logged\nin , clone the project and  initialize  a\nfloyd project.  $ git clone https://github.com/floydhub/dcgan.git\n$  cd  dcgan\n$ floyd init dcgan", 
            "title": "Project setup"
        }, 
        {
            "location": "/examples/dcgan/#training", 
            "text": "", 
            "title": "Training"
        }, 
        {
            "location": "/examples/dcgan/#dataset", 
            "text": "For this project we will use the  Labeled Faces in the Wild Home , aka LFW, for the training.\nSince we are learning in an unsupervised regime, there is not a train/dev/test split, but we use the BCE Loss Error of the Discriminator and Generator as good metrics to learn (minmax game strategy from Game Theory). Moreover, we can visualize the generated images every epoch.\nThis preprocessed dataset is available publicly on FloydHub .", 
            "title": "Dataset"
        }, 
        {
            "location": "/examples/dcgan/#training_1", 
            "text": "You can train the DCGAN model by running  main.py  script with required\nparameters. Below is the  command  to start the training job on Floyd:  $ floyd run  \\ \n--gpu  \\ \n--env pytorch-0.2  \\ \n--data redeipirati/datasets/lfw/1:lfw  \\  python main.py --dataset lfw --dataroot /lfw --outf /output --cuda --ngpu 1 --niter 20   Notes:   --gpu  ensures the job will run on a GPU server.  --env pytorch-0.2  ensures the job will be run in FloydHub's  pytorch-0.2 \n  environment.  The input dataset is passed using  --data redeipirati/datasets/lfw/1:lfw .\n  This mounts the pre-processed LFW dataset (available\n   here  under FloydHub\n  user  @redeipirati 's account) at\n   /lfw . Also note that  main.py 's  --dataroot  flag specifies  /lfw  as the\n  path where the data will be available. This ensures that the\n   code knows where to find the dataset .   This job takes about 20 minutes to run and generate a model. You can follow along the progress\nby using the  logs  command. If you run the model with default value it will take about 1-5 minutes per epoch.  $ floyd logs  JOB_NAME  -t  Floyd saves any content stored in the  /output  directory at the end of the\njob. This output can be used as a datasource in the next project. To get the\nname of the output generated by your job use the  info \ncommand.  $ floyd info  JOB_NAME", 
            "title": "Training"
        }, 
        {
            "location": "/examples/dcgan/#evaluating", 
            "text": "To evaluate your model you can run the generate.py  script included in the GitHub repository .\nThe script needs the path of a checkpoint Generator model, which you pass to\nthe script using its  --netG  flag. Just make sure that you mount the\ncheckpoint Generator model at the path you pass to the  --netG  flag. Below, we\nmount the model at  /model . You can also provide a serialized Zvector\n( --Zvector ) to experiment with latent Z vector arithmetic landscape and for\nanalyzing the semantic information encoded during training.  floyd run  \\ \n--gpu  \\ \n--env pytorch-0.2  \\ \n--data  REPLACE_WITH_JOB_OUTPUT_NAME :model  \\  python generate.py --netG /model/ REPLACE_WITH_MODEL_CHECKPOINT_PATH  --ngpu 1 --cuda   # Provide a serialized Zvector \nfloyd run  \\ \n--gpu  \\ \n--env pytorch-0.2  \\ \n--data  REPLACE_WITH_JOB_OUTPUT_NAME :model  \\  python generate.py --netG /model/ REPLACE_WITH_MODEL_CHECKPOINT_PATH  --Zvector /model/ REPLACE_WITH_SERIALIZED_Z_VECTOR_PATH  --ngpu 1 --cuda   You can track the status of the run with the status or logs command.  $ floyd status  JOB_NAME \n$ floyd logs  JOB_NAME  -t", 
            "title": "Evaluating"
        }, 
        {
            "location": "/examples/dcgan/#improving-your-model", 
            "text": "You may notice that the output does not look great. In fact, the algorithm has\nnot yet learned how to correctly represent a face.  That is because we ran the\ntraining for a small number of iterations. To train a fully working model, try\nthe training step again, this time setting the flag  --niter  to a large value,\nsuch as 300. In general, about 300/500 epochs (or even more, much more!) are\nnecessary to have an accetable model. (Note: This takes a few hours to run on\nthe GPU instance!)\nKeep in mind that  all the classes of generative networks are neither stable\nnor production ready , this is an exciting field of research and everyone can\ncontribute with new ideas.", 
            "title": "Improving your model"
        }, 
        {
            "location": "/examples/dcgan/#evaluate-pre-trained-models", 
            "text": "If you want to try out a pre-trained model, FloydHub has a public dataset model\nwith the checkpoints (300 epochs training) for this located  here . You can mount it with --data redeipirati/datasets/dcgan-300-epochs-models/1:/model .  floyd run  \\ \n--gpu  \\ \n--env pytorch-0.2  \\ \n--data redeipirati/datasets/dcgan-300-epochs-models/1:/model  \\  python generate.py --netG /model/netG_epoch_299.pth --ngpu 1 --cuda   This model should perform better compared to the previous one. You can also provide the  --Zvector  parameter to explore the latent Z vector landscape. We have also provided to you the zvector used for evaluating our model in the attached dataset:  floyd run --gpu  \\ \n--env pytorch-0.2   \\ \n--data redeipirati/datasets/dcgan-300-epochs-models/1:/model  \\  python generate.py --netG /model/netG_epoch_299.pth --Zvector /model/zvector.pth --ngpu 1 --cuda", 
            "title": "Evaluate pre-trained models"
        }, 
        {
            "location": "/examples/dcgan/#serve-the-model-with-a-rest-api", 
            "text": "FloydHub supports seving mode for demo and testing purposes. Before serving\nyour model through a REST API, you need to create a  floyd_requirements.txt \nand declare the  flask  requirement in it. If you run a job with  --mode serve \nflag, FloydHub will run the  app.py  file in your project and attach it to a\ndynamic service endpoint:  floyd run  \\ \n--gpu  \\ \n--mode serve  \\ \n--env pytorch-0.2  \\ \n--data  REPLACE_WITH_JOB_OUTPUT_NAME :input  The above command will print out a service endpoint for this job in your terminal console.  The service endpoint will take couple minutes to become ready. Once it's up, you can interact with the model by sending a serialized Zvector file with a POST request or simply generate images from random noise with a GET request(you can also use the  ckp  parameter to chose a specific checkpoint):  # e.g. of a GET req \ncurl -X GET -o  NAME_ _PATH_DOWNLOADED_IMG  -F  ckp= MODEL_CHECKPOINT   SERVICE_ENDPOINT \ncurl -X GET -o prova.png -F  ckp=netG_epoch_99.pth  https://www.floydhub.com/expose/hellllllllo!!!! # e.g. of a POST req \ncurl -X POST -o  NAME_ _PATH_DOWNLOADED_IMG  -F  file=@ ZVECTOR_SERIALIZED_PATH   SERVICE_ENDPOINT \ncurl -X POST -o prova.png -F  file=@./parameter/zvector.pth  https://www.floydhub.com/expose/hellllllllo!!!!  Any job running in serving mode will stay up until it reaches maximum runtime. So\nonce you are done testing,  remember to shutdown the job.  Note that this feature is in preview mode and is not production ready yet", 
            "title": "Serve the Model with a REST API"
        }, 
        {
            "location": "/examples/dcgan/#what-next", 
            "text": "In the original paper the model was trained on the LSUN  dataset, and then the learned features were\nused to perform an image classification task on the CIFAR-10 dataset. DCGAN was\none of the first stable models based on GAN and the first which tried to\nlearn features from images in an unsupervised regime. GAN is an extremely\nactive research area because they can provide an unlimited amount of high\nquality data which is necessary to train Deep Learning models.", 
            "title": "What Next?"
        }, 
        {
            "location": "/examples/dcgan/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/faqs/installation/", 
            "text": "Using conda to install floyd-cli\n\n\nIf you are using Anaconda Python, you can also use \nconda\n to install \nfloyd-cli\n, instead of \nvirtualenv\n.\n\n\nconda create -n \ninsert-your-env-name-here\n\n\nsource\n activate \ninsert-your-env-name-here\n\npip install -U floyd-cli\n\n\n\n\nPlease see \nthis guide\n on creating virtual environments for Python with conda.\n\n\nUsing sudo to install floyd-cli\n\n\nTry this if you see a permission error, such as \nPermission denied\n or \nAccess is denied\n. If you are not using virtualenv and you are installing \nfloyd-cli\n globally you may need to use \nsudo\n:\n\n\nsudo pip install -U floyd-cli\n\n\n\n\nDealing with missing dependencies when installing floyd-cli\n\n\nNot all python environments are installed the same way. So sometimes you may run\ninto install issues. If \npip\n cannot install dependencies itself, you may see errors like:\n\n\n...\nFailed building wheel \nfor\n scandir\n...\n\n\n\n\nor\n\n\n...\nNo distributions matching the version \nfor\n backports.tempfile \n(\nfrom floyd-cli\n)\n\n...\n\n\n\n\nIn such cases, you can install the dependencies directly:\n\n\npip install -U scandir\npip install -U backports.tempfile\n\n\n\n\nand then try installing \nfloyd-cli\n.\n\n\nPython.h: No such file or directory\n\n\nIf you get this error in a linux environment:\n\n\n...\nPython.h: No such file or directory\n\n\n\n\nyou need to install \npython-dev\n package\n\n\nsudo apt-get install python-dev\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Installation"
        }, 
        {
            "location": "/faqs/installation/#using-conda-to-install-floyd-cli", 
            "text": "If you are using Anaconda Python, you can also use  conda  to install  floyd-cli , instead of  virtualenv .  conda create -n  insert-your-env-name-here  source  activate  insert-your-env-name-here \npip install -U floyd-cli  Please see  this guide  on creating virtual environments for Python with conda.", 
            "title": "Using conda to install floyd-cli"
        }, 
        {
            "location": "/faqs/installation/#using-sudo-to-install-floyd-cli", 
            "text": "Try this if you see a permission error, such as  Permission denied  or  Access is denied . If you are not using virtualenv and you are installing  floyd-cli  globally you may need to use  sudo :  sudo pip install -U floyd-cli", 
            "title": "Using sudo to install floyd-cli"
        }, 
        {
            "location": "/faqs/installation/#dealing-with-missing-dependencies-when-installing-floyd-cli", 
            "text": "Not all python environments are installed the same way. So sometimes you may run\ninto install issues. If  pip  cannot install dependencies itself, you may see errors like:  ...\nFailed building wheel  for  scandir\n...  or  ...\nNo distributions matching the version  for  backports.tempfile  ( from floyd-cli ) \n...  In such cases, you can install the dependencies directly:  pip install -U scandir\npip install -U backports.tempfile  and then try installing  floyd-cli .", 
            "title": "Dealing with missing dependencies when installing floyd-cli"
        }, 
        {
            "location": "/faqs/installation/#pythonh-no-such-file-or-directory", 
            "text": "If you get this error in a linux environment:  ...\nPython.h: No such file or directory  you need to install  python-dev  package  sudo apt-get install python-dev", 
            "title": "Python.h: No such file or directory"
        }, 
        {
            "location": "/faqs/installation/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/faqs/authentication/", 
            "text": "Login\n\n\nWindows\n\n\nI get \"Invalid Token\" error on my Windows 10 machine when I run floyd login.\n\n\nIf you are using Windows command shell, there is an issue with pasting the token using the \nstandard \nCtrl + V\n shortcut. You need to use the Shell's Edit menu to paste the token. After copying the token from the browser, right click on the top bar of the command shell and select Edit -\n Paste. See image below:\n\n\n\n\nI still get the \"Invalid Token\" error after trying the above suggestion.\n\n\nIn some windows shells (like Git Bash) there is an extra space added to the token field\nbefore you paste the token. So you need to hit Backspace and clear out the field before pasting \nthe token. So the steps are:\n\n\n\n\nType \nfloyd login\n in the console.\n\n\nFrom the FloydHub web page, select the token and click on the \"Copy to clipboard\" button.\n\n\n\nIn the console, hit \"backspace\" a few times to remove the extra characters from the token login prompt request.\n\n\nRight click on the menu bar, and select \"Edit\", and then \"Paste\"\n\n\nThen press \"Enter\"\n\n\n\n\nYou should be able to login successfully now. If it's still not working, please give it a try on powershell.\n\n\nSignup\n\n\nHow does the free CPU / GPU hours work?\n\n\nEvery one who signups to Floydhub will receive 2 hours of free CPU / GPU time\nfor running your projects. We hope this will give you enough time to evaluate\nFloydhub for your needs. We are working on a new free plan right now to better\nhelp new users explore the platform.\n\n\nEmail Verification\n\n\nAfter you signup on FloydHub, you have to verify your email address. You will receive an automated email from Floyd with a link that you can click to verify.\n\n\nI did not receive my verification email\n\n\nAs soon as you sign up on FloydHub, you should receive an automated email in your inbox with instructions to verify your email address. \n\n\nIf you do not receive an email within a few minutes:\n\n\n\n\nPlease check your spam folder. If the email is there, please \"Mark as not Spam\" to avoid this happening in the future\n\n\nIf you still don't receive an email, please try resending the verification email by clicking on \"Resend Verification Email\" at \nfloydhub.com/settings/security\n\n\n\n\n\n\n\n\nIf this still doesn't work, it is likely that your mail server (e.g. your work email server) is filtering out our emails. Please check with your email adminstrator to allow emails from the \nfloydhub.com\n domain.\n\n\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Signup and Login"
        }, 
        {
            "location": "/faqs/authentication/#login", 
            "text": "", 
            "title": "Login"
        }, 
        {
            "location": "/faqs/authentication/#windows", 
            "text": "", 
            "title": "Windows"
        }, 
        {
            "location": "/faqs/authentication/#i-get-invalid-token-error-on-my-windows-10-machine-when-i-run-floyd-login", 
            "text": "If you are using Windows command shell, there is an issue with pasting the token using the \nstandard  Ctrl + V  shortcut. You need to use the Shell's Edit menu to paste the token. After copying the token from the browser, right click on the top bar of the command shell and select Edit -  Paste. See image below:", 
            "title": "I get \"Invalid Token\" error on my Windows 10 machine when I run floyd login."
        }, 
        {
            "location": "/faqs/authentication/#i-still-get-the-invalid-token-error-after-trying-the-above-suggestion", 
            "text": "In some windows shells (like Git Bash) there is an extra space added to the token field\nbefore you paste the token. So you need to hit Backspace and clear out the field before pasting \nthe token. So the steps are:   Type  floyd login  in the console.  From the FloydHub web page, select the token and click on the \"Copy to clipboard\" button.  In the console, hit \"backspace\" a few times to remove the extra characters from the token login prompt request.  Right click on the menu bar, and select \"Edit\", and then \"Paste\"  Then press \"Enter\"   You should be able to login successfully now. If it's still not working, please give it a try on powershell.", 
            "title": "I still get the \"Invalid Token\" error after trying the above suggestion."
        }, 
        {
            "location": "/faqs/authentication/#signup", 
            "text": "", 
            "title": "Signup"
        }, 
        {
            "location": "/faqs/authentication/#how-does-the-free-cpu-gpu-hours-work", 
            "text": "Every one who signups to Floydhub will receive 2 hours of free CPU / GPU time\nfor running your projects. We hope this will give you enough time to evaluate\nFloydhub for your needs. We are working on a new free plan right now to better\nhelp new users explore the platform.", 
            "title": "How does the free CPU / GPU hours work?"
        }, 
        {
            "location": "/faqs/authentication/#email-verification", 
            "text": "After you signup on FloydHub, you have to verify your email address. You will receive an automated email from Floyd with a link that you can click to verify.", 
            "title": "Email Verification"
        }, 
        {
            "location": "/faqs/authentication/#i-did-not-receive-my-verification-email", 
            "text": "As soon as you sign up on FloydHub, you should receive an automated email in your inbox with instructions to verify your email address.   If you do not receive an email within a few minutes:   Please check your spam folder. If the email is there, please \"Mark as not Spam\" to avoid this happening in the future  If you still don't receive an email, please try resending the verification email by clicking on \"Resend Verification Email\" at  floydhub.com/settings/security     If this still doesn't work, it is likely that your mail server (e.g. your work email server) is filtering out our emails. Please check with your email adminstrator to allow emails from the  floydhub.com  domain.", 
            "title": "I did not receive my verification email"
        }, 
        {
            "location": "/faqs/authentication/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/faqs/job/", 
            "text": "Why does \nfloyd status\n return an empty list even though I have several\n\n\nruns in my account?\n\n\nFloyd CLI uses the project directory to store the run information (similar to git). So you need\nto be in the directory where you initialized the project and you should be able to see all your\nruns. You can also use the \nweb dashboard\n to view all your\nprojects in one place.\n\n\nWhat do I do when I get \"What do you do when you get \u201cYou are over the allowed limits for this operation. Consider upgrading your account\u201d?\n\n\nFloydhub currently allows only 1 active job per user for trial plan and 3 active jobs for individual plan. If you require more concurrency, contact\nus from the \npricing\n page.\n\n\nI get \"Too many open files\" error when I run my project.\n\n\nFloyd CLI throws this error when you have too many files in your current directory that needs to be uploaded.\nThe actual limit depends on your OS / machine specs.\n\n\nYou can either:\n\n\n\n\nRemove unnecessary files from the directory (like build directory, docs etc.)\n\n\nAdd them to \n.floydignore\n file. Floyd CLI will just ignore these directories.\nSee the \nfloydignore\n documentation to understand how this can be configured.\n\n\nTar them into a single file and untar them at runtime.\n\n\n\n\nAlternatively, instead of uploading files from your local machine, you can also\n\ndownload files\n from a remote URL\ndirectly into Floyd servers.\n\n\nWhy do I get an \"Experiments limit reached\" error when I run a job?\n\n\nFloydHub currently allows only 1 active job in the free Trial plan and 3 active jobs in the Individual plan. \nIf you see an \nError\n:\n \nExperiments\n \nlimit\n \nreached\n message when you run a job, it means you have maxed out your \nconcurrency limits. Please stop you running job(s) or wait for them to finish, and try again.\n\n\nWe have to enforce this concurrency constraint because we have a finite number of GPU machines and have to ensure that no single user is starving the group. In the near future, we will support queueing of jobs so that you can queue multiple jobs to be run as slots become available.\n\n\nI ran my project in Jupyter mode but the url does not seem to work.\n\n\nJupyter notebook server takes a couple of minutes to start. Until then you will get a \"Bad Gateway\"\nor similar error when you access the URL. You can check the status of the Jupyter notebook\nby running the \nlogs\n command.\n\n\nAm I using the GPU instance by default?\n\n\nJobs are run on CPU instances by default. You can specify \n--gpu\n to run them on GPU instances.\n\n\nMy job is taking a while to \"sync changes\". How do I make it go faster?\n\n\nFloyd CLI uploads \nall\n the files in your current directory before starting your experiment.\nThere are a few ways to make this go faster:\n\n\n\n\nRemove unnecessary files from the directory (like build directory, docs etc.)\n\n\nAdd sub-directories to \n.floydignore\n file. Floyd CLI will ignore and not upload these sub-directories.\nSee the \ninit\n command and \nignore files guide\n to understand how this can be configured.\n\n\nIf you have large data files consider uploading them separately as a \ndata source\n.\nYou can then \nrefer\n to them in your project.\n\n\n\n\nMy job finished but how I do I see my output?\n\n\nYou can use the floyd \noutput\n command to view the output of your\nproject. If you want to use this output in your next run view \nthis guide\n.\n\n\nDo I have to pay for the entire time my Jupyter Notebook is running?\n\n\nUnfortunately, yes. As much as we would like to, we are unable to charge you only for the \ncomputation time\n.\n\n\nThis is an engineering challenge. When you start a Jupyter Notebook instance and start executing commands,\nyour state is maintained in memory (both CPU and GPU memory). So the instance has to be alive for the\nentire duration of your notebook, not just when you are executing commands.\n\n\nFor example, when you execute \nimport\n \ntensorflow\n\ncommand, Tensorflow will allocate the entire GPU memory to the current session and waits for the next command. This makes the instance unusable by anyone else, so we have to charge you for the duration your Notebook is alive.\n\n\nCan I view my Jupyter Notebook after my job has stopped?\n\n\nYes. When you use a Jupyter Notebook on FloydHub, your Notebook is saved periodically in the \n/output\n dir. So, your work is not lost after your job has ended, shutdown or timed out.\n\n\nYou can view your saved Notebook using the \nfloyd output\n command. Example:\n\n\n$ floyd output redeipirati/projects/pytorch-fast-neural-style/3/output\n\n\n\n\nOr in the \nOutput\n tab of your job on the web dashboard, example: \nwww.floydhub.com/redeipirati/projects/pytorch-fast-neural-style/3/output\n\n\nCan I restart a stopped or timed out job?\n\n\nUnfortunately, not directly. We will be implementing a single command to do this soon!\n\n\nIn the meanwhile, you can follow these steps to do this manually:\n\n\n\n\nJupyter Notebook\n: Your Notebook is \nsaved periodically\n. To restart your Notebook after your job has stopped, please download the output of your stopped job to your machine and start another job. Example:\n\n\n\n\n# Download the saved Notebook from previous job\n\n\n# NOTE: This will overwrite the contents of your current dir\n\n$ floyd data clone redeipirati/projects/pytorch-fast-neural-style/3/output\n\n\n# Start a new job\n\n$ floyd run --mode jupyter\n\n\n\n\n\n\nScript\n: If you are running a script/command, you will have to start a new job using the \nfloyd run \ncommand\n command.\n\n\n\n\nWhy is my job in the \"Queued\" state for several minutes?\n\n\nThis means that a machine is being prepared to run your job. \n\n\nMost times, we have several CPU and GPU machines that are ready and your job can start execution in a few seconds. During high traffic periods, we may not have a machine ready for you and have to spin up a new instance for your job on-demand (details below). This might take up to 10 minutes in some cases. We are actively working on reducing this wait time.\n\n\nDetails\n: When you execute a \nfloyd run\n command, Floyd does several things in the background:\n\n\n\n\nProvision a CPU or GPU instance on the cloud\n\n\nSet up a deep learning environment with GPU drivers and the correct environment (as specified by \n--env\n) installed using Docker\n\n\nMount any data you specify using the \n--data\n flag\n\n\nSpin up a Jupyter server, if \n--mode jupyter\n flag\n\n\n\n\nEach of these steps can take up to a couple of minutes. Usually Steps 1 and 2 are already done, but during peak usage hours, we might have to do this on-demand.\n\n\nWhy do I see \"Setting up your instance...\" for several minutes when running a Jupyter Notebook?\n\n\nThe \nSetting up your instance...\n message is displayed when a machine is being prepared to run your Jupyter Notebook.\n\n\n\n\nWhen you execute a \nfloyd run --mode jupyter\n command, the CLI waits for a CPU or GPU machine to be ready before it opens up an interactive Jupyter Notebook for your work on. Usually, this takes a few seconds, but during high traffic periods it can take up to 10 minutes in some cases.\n\n\nFor more details on why it takes time, please see \nWhy is my job in the \"Queued\" state for several minutes?\n\n\nWhy are my logs not displayed in real-time?\n\n\nYou can stream your logs from the CLI using the \nfloyd logs -t \nJOB_NAME\n command. However, sometimes you may notice that the logs are not displayed in real-time. This is because of output buffering. Please make sure that your logs are flushed out if you prefer to view real-time logs.\n\n\nFor example, in Python:\n\n\nimport\n \nsys\n\n\n...\n\n\nprint\n(\nHello world\n)\n\n\nsys\n.\nstdout\n.\nflush\n()\n\n\n\n\n\nWhy did my job timeout after 1 hour?\n\n\nYou are likely in the Free Trial Plan. Jobs run in the trial plan have a maximum runtime of 1 hour. It will automatically timeout after that. \n\n\n\n\nYou can upgrade to the \nPaid Plan\n to overcome these limits.\n\n\nWhy was my CPU job Killed without warning?\n\n\nOccasionally, you may notice that your CPU job died without warning. The output logs just display \nKilled\n. For example,\n\n\n################################################################################\n\n\n\n2017\n-07-24 \n03\n:33:42,530 INFO - Run Output:\n...\n\n2017\n-07-24 \n03\n:33:52,920 INFO - Using TensorFlow backend.\n\n2017\n-07-24 \n03\n:34:04,381 INFO - \n loading UNet of size 1152x256...\n\n2017\n-07-24 \n03\n:34:10,942 INFO - Epoch \n1\n/100\n\n2017\n-07-24 \n03\n:35:17,221 INFO - Killed\n\n2017\n-07-24 \n03\n:35:18,680 INFO - \n\n################################################################################\n\n\n\n\n\nThis happens when your machine runs out of memory (OOM). i.e. your job consumes more memory than is available on our CPU machines.\n\n\nAll jobs run on FloydHub are executed inside a Docker container. Our current CPU machines have \n7GB memory\n. When memory used by your job exceeds 7GB, Docker automatically kills the job to protect the system and avoid abuse.\n\n\nThe resolution is to optimize your code to consume less memory. For example, read less data into your in-memory datastructures or reduce your batch size. We will be introducing more powerful CPUs, which higher memory in the near future.\n\n\nWhy did I get a \"Long running FloydHub Jupyter job detected\" email?\n\n\nFloydHub monitors Jupyter notebook instances that are no longer actively used and notifies the owner. This is a reminder in case the user forgot to turn off the instance after use and should help save resources.\n\n\nNote: If you create \nTerminals within Jupyter notebooks\n, this feature will not work. This is because FloydHub cannot reliably detect if the instance is being used or not.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Job"
        }, 
        {
            "location": "/faqs/job/#why-does-floyd-status-return-an-empty-list-even-though-i-have-several", 
            "text": "runs in my account?  Floyd CLI uses the project directory to store the run information (similar to git). So you need\nto be in the directory where you initialized the project and you should be able to see all your\nruns. You can also use the  web dashboard  to view all your\nprojects in one place.", 
            "title": "Why does floyd status return an empty list even though I have several"
        }, 
        {
            "location": "/faqs/job/#what-do-i-do-when-i-get-what-do-you-do-when-you-get-you-are-over-the-allowed-limits-for-this-operation-consider-upgrading-your-account", 
            "text": "Floydhub currently allows only 1 active job per user for trial plan and 3 active jobs for individual plan. If you require more concurrency, contact\nus from the  pricing  page.", 
            "title": "What do I do when I get \"What do you do when you get \u201cYou are over the allowed limits for this operation. Consider upgrading your account\u201d?"
        }, 
        {
            "location": "/faqs/job/#i-get-too-many-open-files-error-when-i-run-my-project", 
            "text": "Floyd CLI throws this error when you have too many files in your current directory that needs to be uploaded.\nThe actual limit depends on your OS / machine specs.  You can either:   Remove unnecessary files from the directory (like build directory, docs etc.)  Add them to  .floydignore  file. Floyd CLI will just ignore these directories.\nSee the  floydignore  documentation to understand how this can be configured.  Tar them into a single file and untar them at runtime.   Alternatively, instead of uploading files from your local machine, you can also download files  from a remote URL\ndirectly into Floyd servers.", 
            "title": "I get \"Too many open files\" error when I run my project."
        }, 
        {
            "location": "/faqs/job/#why-do-i-get-an-experiments-limit-reached-error-when-i-run-a-job", 
            "text": "FloydHub currently allows only 1 active job in the free Trial plan and 3 active jobs in the Individual plan. \nIf you see an  Error :   Experiments   limit   reached  message when you run a job, it means you have maxed out your \nconcurrency limits. Please stop you running job(s) or wait for them to finish, and try again.  We have to enforce this concurrency constraint because we have a finite number of GPU machines and have to ensure that no single user is starving the group. In the near future, we will support queueing of jobs so that you can queue multiple jobs to be run as slots become available.", 
            "title": "Why do I get an \"Experiments limit reached\" error when I run a job?"
        }, 
        {
            "location": "/faqs/job/#i-ran-my-project-in-jupyter-mode-but-the-url-does-not-seem-to-work", 
            "text": "Jupyter notebook server takes a couple of minutes to start. Until then you will get a \"Bad Gateway\"\nor similar error when you access the URL. You can check the status of the Jupyter notebook\nby running the  logs  command.", 
            "title": "I ran my project in Jupyter mode but the url does not seem to work."
        }, 
        {
            "location": "/faqs/job/#am-i-using-the-gpu-instance-by-default", 
            "text": "Jobs are run on CPU instances by default. You can specify  --gpu  to run them on GPU instances.", 
            "title": "Am I using the GPU instance by default?"
        }, 
        {
            "location": "/faqs/job/#my-job-is-taking-a-while-to-sync-changes-how-do-i-make-it-go-faster", 
            "text": "Floyd CLI uploads  all  the files in your current directory before starting your experiment.\nThere are a few ways to make this go faster:   Remove unnecessary files from the directory (like build directory, docs etc.)  Add sub-directories to  .floydignore  file. Floyd CLI will ignore and not upload these sub-directories.\nSee the  init  command and  ignore files guide  to understand how this can be configured.  If you have large data files consider uploading them separately as a  data source .\nYou can then  refer  to them in your project.", 
            "title": "My job is taking a while to \"sync changes\". How do I make it go faster?"
        }, 
        {
            "location": "/faqs/job/#my-job-finished-but-how-i-do-i-see-my-output", 
            "text": "You can use the floyd  output  command to view the output of your\nproject. If you want to use this output in your next run view  this guide .", 
            "title": "My job finished but how I do I see my output?"
        }, 
        {
            "location": "/faqs/job/#do-i-have-to-pay-for-the-entire-time-my-jupyter-notebook-is-running", 
            "text": "Unfortunately, yes. As much as we would like to, we are unable to charge you only for the  computation time .  This is an engineering challenge. When you start a Jupyter Notebook instance and start executing commands,\nyour state is maintained in memory (both CPU and GPU memory). So the instance has to be alive for the\nentire duration of your notebook, not just when you are executing commands.  For example, when you execute  import   tensorflow \ncommand, Tensorflow will allocate the entire GPU memory to the current session and waits for the next command. This makes the instance unusable by anyone else, so we have to charge you for the duration your Notebook is alive.", 
            "title": "Do I have to pay for the entire time my Jupyter Notebook is running?"
        }, 
        {
            "location": "/faqs/job/#can-i-view-my-jupyter-notebook-after-my-job-has-stopped", 
            "text": "Yes. When you use a Jupyter Notebook on FloydHub, your Notebook is saved periodically in the  /output  dir. So, your work is not lost after your job has ended, shutdown or timed out.  You can view your saved Notebook using the  floyd output  command. Example:  $ floyd output redeipirati/projects/pytorch-fast-neural-style/3/output  Or in the  Output  tab of your job on the web dashboard, example:  www.floydhub.com/redeipirati/projects/pytorch-fast-neural-style/3/output", 
            "title": "Can I view my Jupyter Notebook after my job has stopped?"
        }, 
        {
            "location": "/faqs/job/#can-i-restart-a-stopped-or-timed-out-job", 
            "text": "Unfortunately, not directly. We will be implementing a single command to do this soon!  In the meanwhile, you can follow these steps to do this manually:   Jupyter Notebook : Your Notebook is  saved periodically . To restart your Notebook after your job has stopped, please download the output of your stopped job to your machine and start another job. Example:   # Download the saved Notebook from previous job  # NOTE: This will overwrite the contents of your current dir \n$ floyd data clone redeipirati/projects/pytorch-fast-neural-style/3/output # Start a new job \n$ floyd run --mode jupyter   Script : If you are running a script/command, you will have to start a new job using the  floyd run  command  command.", 
            "title": "Can I restart a stopped or timed out job?"
        }, 
        {
            "location": "/faqs/job/#why-is-my-job-in-the-queued-state-for-several-minutes", 
            "text": "This means that a machine is being prepared to run your job.   Most times, we have several CPU and GPU machines that are ready and your job can start execution in a few seconds. During high traffic periods, we may not have a machine ready for you and have to spin up a new instance for your job on-demand (details below). This might take up to 10 minutes in some cases. We are actively working on reducing this wait time.  Details : When you execute a  floyd run  command, Floyd does several things in the background:   Provision a CPU or GPU instance on the cloud  Set up a deep learning environment with GPU drivers and the correct environment (as specified by  --env ) installed using Docker  Mount any data you specify using the  --data  flag  Spin up a Jupyter server, if  --mode jupyter  flag   Each of these steps can take up to a couple of minutes. Usually Steps 1 and 2 are already done, but during peak usage hours, we might have to do this on-demand.", 
            "title": "Why is my job in the \"Queued\" state for several minutes?"
        }, 
        {
            "location": "/faqs/job/#why-do-i-see-setting-up-your-instance-for-several-minutes-when-running-a-jupyter-notebook", 
            "text": "The  Setting up your instance...  message is displayed when a machine is being prepared to run your Jupyter Notebook.   When you execute a  floyd run --mode jupyter  command, the CLI waits for a CPU or GPU machine to be ready before it opens up an interactive Jupyter Notebook for your work on. Usually, this takes a few seconds, but during high traffic periods it can take up to 10 minutes in some cases.  For more details on why it takes time, please see  Why is my job in the \"Queued\" state for several minutes?", 
            "title": "Why do I see \"Setting up your instance...\" for several minutes when running a Jupyter Notebook?"
        }, 
        {
            "location": "/faqs/job/#why-are-my-logs-not-displayed-in-real-time", 
            "text": "You can stream your logs from the CLI using the  floyd logs -t  JOB_NAME  command. However, sometimes you may notice that the logs are not displayed in real-time. This is because of output buffering. Please make sure that your logs are flushed out if you prefer to view real-time logs.  For example, in Python:  import   sys  ...  print ( Hello world )  sys . stdout . flush ()", 
            "title": "Why are my logs not displayed in real-time?"
        }, 
        {
            "location": "/faqs/job/#why-did-my-job-timeout-after-1-hour", 
            "text": "You are likely in the Free Trial Plan. Jobs run in the trial plan have a maximum runtime of 1 hour. It will automatically timeout after that.    You can upgrade to the  Paid Plan  to overcome these limits.", 
            "title": "Why did my job timeout after 1 hour?"
        }, 
        {
            "location": "/faqs/job/#why-was-my-cpu-job-killed-without-warning", 
            "text": "Occasionally, you may notice that your CPU job died without warning. The output logs just display  Killed . For example,  ################################################################################  2017 -07-24  03 :33:42,530 INFO - Run Output:\n... 2017 -07-24  03 :33:52,920 INFO - Using TensorFlow backend. 2017 -07-24  03 :34:04,381 INFO -   loading UNet of size 1152x256... 2017 -07-24  03 :34:10,942 INFO - Epoch  1 /100 2017 -07-24  03 :35:17,221 INFO - Killed 2017 -07-24  03 :35:18,680 INFO -  ################################################################################   This happens when your machine runs out of memory (OOM). i.e. your job consumes more memory than is available on our CPU machines.  All jobs run on FloydHub are executed inside a Docker container. Our current CPU machines have  7GB memory . When memory used by your job exceeds 7GB, Docker automatically kills the job to protect the system and avoid abuse.  The resolution is to optimize your code to consume less memory. For example, read less data into your in-memory datastructures or reduce your batch size. We will be introducing more powerful CPUs, which higher memory in the near future.", 
            "title": "Why was my CPU job Killed without warning?"
        }, 
        {
            "location": "/faqs/job/#why-did-i-get-a-long-running-floydhub-jupyter-job-detected-email", 
            "text": "FloydHub monitors Jupyter notebook instances that are no longer actively used and notifies the owner. This is a reminder in case the user forgot to turn off the instance after use and should help save resources.  Note: If you create  Terminals within Jupyter notebooks , this feature will not work. This is because FloydHub cannot reliably detect if the instance is being used or not.", 
            "title": "Why did I get a \"Long running FloydHub Jupyter job detected\" email?"
        }, 
        {
            "location": "/faqs/job/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/faqs/plans/", 
            "text": "Please see here for \nBilling FAQs\n\n\nPlans\n\n\nWhich Plan is right for me?\n\n\nFree Plan\n\n\nIf you're just exploring, the Free plan is for you! You are automatically\nenrolled in the Free plan when you sign up on FloydHub. It includes 20 hours of\nfree CPU every month. You cannot, however, use a GPU or run multiple jobs in\nparallel.\n\n\nData Scientist Plan\n\n\nOur Data Scientist plans offer varying levels of \njob concurrency\n, GPU computing hours\nand storage. You can also purchase \nPowerups\n to add more compute hours to supplement your plan.\n\n\nFor more details on our different plans, visit our \npricing\npage\n. Please see the \nfeature comparison\ntable\n for a full list of each\nplan's features.\n\n\nWhat is in the Trial plan?\n\n\nAll users that sign up on FloydHub are automatically enrolled in the Free plan.\nRefer to our \npricing page\n for\ndetails on the Free plan.\n\n\nOnce you've signed up, take FloydHub for a spin with our \nQuick Start\nGuide\n or \nJupyter Notebook\nGuide\n!\n\n\nDo the plans come with preemptible or dedicated instances?\n\n\nThe GPU and CPU compute hours included in your plan (Free or Data Scientist) are\n\npreemptible instances\n. This means that there is a small chance that your job will be terminated without notice. In practice, this happens infrequently and this is perfect for most users. If you need \ndedicated instances\n for your jobs, you\ncan buy the GPU+ or CPU+ Powerups.\n\n\nDo my remaining compute credits roll over each month if I don't use them all?\n\n\nNo, your monthly Plan compute credits are not rolled over. However, your Powerup credits will remain valid for one year from purchase date.\n\n\nWhat happened to the old Pay-as-you-go Individual Plan?\n\n\nWe are transitioning from the Individual Plan, which offered a pay-as-you-go payment method, to our current pricing plan. The Individual Plan is no longer available for new users.\n\n\nI am in the Pay-as-you-go Individual Plan. What will happen to me?\n\n\nIf you signed up for the Individual Plan before August 20\nth\n 2017, you will be grandfathered till October 1\nst\n 2017. After this, you will be automatically enrolled in the Free plan. Please note that any remaining promotional credits will also expire on this date.\n\n\nPlease \nupgrade\n to one of the Data Scientist plans to continue using FloydHub without interruption. We will also be reaching out to you with more information about this transition.\n\n\nWhy did I not get 100 free GPU hours when I signed up?\n\n\nWe offered 100 hours of free GPU for all users during our promotional period. This has ended.\n\n\nAre there any academic discounts for students?\n\n\nWe don't have discounts. However, a lot of students create content for us. If you are willing to contribute high quality content to FLoydHub, we will give you \nfree GPU credits\n in exchange!\n\n\nContent we are looking for:\n\n\n\n\nTechnical blogs on deep learning and AI\n\n\nFloydHub tutorials, text or video\n\n\nPort popular deep learning projects to FloydHub\n\n\nCreate interesting datasets\n\n\nInsert your own idea here\n\n\n\n\nIf this is interesting to you, please let us know about it \nhere\n.\n\n\nCompute\n\n\nWhat is job concurrency?\n\n\nJob concurrency is the number of jobs you can run in parallel. Each plan has a limit\non the number of concurrent jobs you can run. For example, in the Free plan,\nyou can only run 1 job at a time. In the Data Scientist Pro plan, you can run up\nto 8 jobs in parallel.\n\n\nHaving a higher concurrency is useful when you want to parallelize your training, for\nexample while hyperparameter sweeping.\n\n\nWhat will happen to my running job when I run out of computing credits?\n\n\nYou job will be shutdown immediately when you run out of computing credits.\n\n\nIf you run long-running jobs and expect them to exceed the computing hours offered by your plan, you can purchase \nPowerups\n.\n\n\nYou can also enable auto-refresh on your Powerups to ensure your long-running jobs are never killed because you ran out of computing hours. We'll automatically refresh your selected Powerup so that your job can continue running.\n\n\nPreemptible vs. Dedicated Instances\n\n\nPreemptible Instances\n\n\nPreemptible instances have medium job uptime SLA of 98%. This means that there is a small chance that your job can be terminated (preempted) at any point during its runtime by FloydHub if it requires access to those resources for other, higher priority tasks.\n\n\nPreemptible instances (CPU / GPU) offer top notch compute at affordable prices, in exchange for fault tolerance.\n\n\nNote that SLA refers to what we can guarantee. In practice, this happens infrequently. Historically, less than 0.1% of jobs run on FloydHub have encountered interruption. However, you need to be aware that there is the possibility.\n\n\nWhy Do You Use Preemptible Instances?\n\n\nTo be able to offer you compute at a much lower cost.\n\n\nWe have a fixed pool of resources that we have to allocate amongst all our users. Some of our users require dedicated instances and are willing to pay the premium for uninterrupted access. But, the majority of our users can tolerate a 98% job uptime SLA for the significant price savings that preemptible instances offer.\n\n\nWill I Get a Refund if My Job is Preempted?\n\n\nNo.\n\n\nOur preemptible instances have a 98% job uptime SLA. By using them, you are\naccepting a small chance of your job being terminated without notice, in\nexchange for paying a much lower price than dedicated instances.\n\n\nHow Will I Know When My Job Is Preempted?\n\n\nYour job's state will turn from \nRunning\n to \nShutdown\n. We will send you a\nnotification informing you about this. Unfortunately, we are currently unable\nto warn your ahead of time of an impending preemption.\n\n\nWhat is the SLA of Preemptible Instances?\n\n\nPreemptible instances have 98% job up time SLA.\n\n\nDedicated Instances\n\n\nDedicated instances have high job uptime SLA of 99.95%. Use dedicated instances for your jobs if they are critical or not fault tolerant. You can purchase '+' \nPowerups\n (CPU+ / GPU+) to utilize dedicated instances.\n\n\nWhy Do You Use Dedicated Instances?\n\n\nIf your job is not fault tolerant and cannot withstand a small (\n2%) chance\nof your job being shutdown without notice, you should use our \n+\n Dedicated instances.\n\n\nPrice sensitivity also plays a factor - dedicated instances are more expensive\nthan premptible instances.\n\n\nGiven that deep learning models typically train over long periods of time, it\nis good practice to build your application to be fault tolerant by regularly checkpointing your training.\n\n\nWhat is the SLA of Dedicated instances?\n\n\nDedicated instances have 99.95% job up time SLA.\n\n\nWhat is the difference between GPU vs. GPU+ and CPU vs. CPU+?\n\n\nGPU and CPU are \npreemptible instances\n. GPU+ and CPU+ are \ndedicated instances\n.\n\n\nPowerups\n\n\nWhat are Powerups?\n\n\nYour subscription plan comes with a monthly quota of CPU and GPU computing hours.\nIf you need more computing hours, you can buy Powerups to supplement your plan.\n\n\nWhat Powerups should I buy?\n\n\nThis depends on your computing needs. We offer multiple tiers of Powerups:\n\n\n\n\nPreemptible vs. Dedicated\n: CPU / GPU are affordable \npreemptible instances\n, CPU+ / GPU+ are high-reliability \ndedicated instances\n.\n\n\n10 vs. 50 vs. 100 hours\n: Purchase a pack that suits your computing needs. Note that the larger packs offer compute at a much cheaper rate/hour than smaller packs.\n\n\nAuto-refresh\n: You can enable auto-refresh on any pack.\n\n\n\n\nIf you are \njust starting out\n and need more computing hours than your plan offers, you can start with the \nGPU10 Powerup\n.\n\n\nIf you run \nlong-running jobs\n, you should purchase the \nGPU100 with auto-refresh enabled\n, to ensure that you never run out of computing credits.\n\n\nIf you run \ncritical jobs\n that are not fault-tolerant, you should purchase the \nGPU+ Powerup\n.\n\n\nHow can I buy Powerups?\n\n\nYou can purchase them from your \nPowerups Dashboard\n\n\nWhy would I enable auto-refresh?\n\n\nAuto-refresh ensures your long-running jobs are never killed because you \nran out of computing hours\n. We'll automatically refresh your selected Powerup so that your Job can continue running.\n\n\nCan I buy a Powerup if I am in the Free Plan?\n\n\nNo. You have to be enrolled in one of the Data Scientist plans to be eligible for\npurchasing Powerups.\n\n\nDo Powerups expire?\n\n\nYes. Powerups are valid for 1 year from the date of purchase.\n\n\nHow will my Powerups be used?\n\n\nYour compute hours will be consumed in the following order:\n\n\n\n\nHours from your subscription plan\n\n\nHours from free credits\n\n\nHours from Powerups\n\n\n\n\nStorage\n\n\nHow much storage do I get?\n\n\nEach plan comes with its own storage limit. Please see the \nfeature comparison\ntable\n for details.\n\n\nWhat counts against my storage?\n\n\nStorage is consumed by the datasets that you upload, your code and the data that your jobs output.\n\n\nNote that you are only responsible for the data that you own. For example, if you use a public dataset in your job, you won't be charged for it.\n\n\nCan I buy more storage than my plan offers?\n\n\nIf our Data Scientist Pro plan's storage doesn't meet your needs, please\ncontact us at \n.\n\n\nWe will soon have a storage Powerup that can you buy to add more storage to your base plan.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Plans"
        }, 
        {
            "location": "/faqs/plans/#plans", 
            "text": "", 
            "title": "Plans"
        }, 
        {
            "location": "/faqs/plans/#which-plan-is-right-for-me", 
            "text": "", 
            "title": "Which Plan is right for me?"
        }, 
        {
            "location": "/faqs/plans/#free-plan", 
            "text": "If you're just exploring, the Free plan is for you! You are automatically\nenrolled in the Free plan when you sign up on FloydHub. It includes 20 hours of\nfree CPU every month. You cannot, however, use a GPU or run multiple jobs in\nparallel.", 
            "title": "Free Plan"
        }, 
        {
            "location": "/faqs/plans/#data-scientist-plan", 
            "text": "Our Data Scientist plans offer varying levels of  job concurrency , GPU computing hours\nand storage. You can also purchase  Powerups  to add more compute hours to supplement your plan.  For more details on our different plans, visit our  pricing\npage . Please see the  feature comparison\ntable  for a full list of each\nplan's features.", 
            "title": "Data Scientist Plan"
        }, 
        {
            "location": "/faqs/plans/#what-is-in-the-trial-plan", 
            "text": "All users that sign up on FloydHub are automatically enrolled in the Free plan.\nRefer to our  pricing page  for\ndetails on the Free plan.  Once you've signed up, take FloydHub for a spin with our  Quick Start\nGuide  or  Jupyter Notebook\nGuide !", 
            "title": "What is in the Trial plan?"
        }, 
        {
            "location": "/faqs/plans/#do-the-plans-come-with-preemptible-or-dedicated-instances", 
            "text": "The GPU and CPU compute hours included in your plan (Free or Data Scientist) are preemptible instances . This means that there is a small chance that your job will be terminated without notice. In practice, this happens infrequently and this is perfect for most users. If you need  dedicated instances  for your jobs, you\ncan buy the GPU+ or CPU+ Powerups.", 
            "title": "Do the plans come with preemptible or dedicated instances?"
        }, 
        {
            "location": "/faqs/plans/#do-my-remaining-compute-credits-roll-over-each-month-if-i-dont-use-them-all", 
            "text": "No, your monthly Plan compute credits are not rolled over. However, your Powerup credits will remain valid for one year from purchase date.", 
            "title": "Do my remaining compute credits roll over each month if I don't use them all?"
        }, 
        {
            "location": "/faqs/plans/#what-happened-to-the-old-pay-as-you-go-individual-plan", 
            "text": "We are transitioning from the Individual Plan, which offered a pay-as-you-go payment method, to our current pricing plan. The Individual Plan is no longer available for new users.", 
            "title": "What happened to the old Pay-as-you-go Individual Plan?"
        }, 
        {
            "location": "/faqs/plans/#i-am-in-the-pay-as-you-go-individual-plan-what-will-happen-to-me", 
            "text": "If you signed up for the Individual Plan before August 20 th  2017, you will be grandfathered till October 1 st  2017. After this, you will be automatically enrolled in the Free plan. Please note that any remaining promotional credits will also expire on this date.  Please  upgrade  to one of the Data Scientist plans to continue using FloydHub without interruption. We will also be reaching out to you with more information about this transition.", 
            "title": "I am in the Pay-as-you-go Individual Plan. What will happen to me?"
        }, 
        {
            "location": "/faqs/plans/#why-did-i-not-get-100-free-gpu-hours-when-i-signed-up", 
            "text": "We offered 100 hours of free GPU for all users during our promotional period. This has ended.", 
            "title": "Why did I not get 100 free GPU hours when I signed up?"
        }, 
        {
            "location": "/faqs/plans/#are-there-any-academic-discounts-for-students", 
            "text": "We don't have discounts. However, a lot of students create content for us. If you are willing to contribute high quality content to FLoydHub, we will give you  free GPU credits  in exchange!  Content we are looking for:   Technical blogs on deep learning and AI  FloydHub tutorials, text or video  Port popular deep learning projects to FloydHub  Create interesting datasets  Insert your own idea here   If this is interesting to you, please let us know about it  here .", 
            "title": "Are there any academic discounts for students?"
        }, 
        {
            "location": "/faqs/plans/#compute", 
            "text": "", 
            "title": "Compute"
        }, 
        {
            "location": "/faqs/plans/#what-is-job-concurrency", 
            "text": "Job concurrency is the number of jobs you can run in parallel. Each plan has a limit\non the number of concurrent jobs you can run. For example, in the Free plan,\nyou can only run 1 job at a time. In the Data Scientist Pro plan, you can run up\nto 8 jobs in parallel.  Having a higher concurrency is useful when you want to parallelize your training, for\nexample while hyperparameter sweeping.", 
            "title": "What is job concurrency?"
        }, 
        {
            "location": "/faqs/plans/#what-will-happen-to-my-running-job-when-i-run-out-of-computing-credits", 
            "text": "You job will be shutdown immediately when you run out of computing credits.  If you run long-running jobs and expect them to exceed the computing hours offered by your plan, you can purchase  Powerups .  You can also enable auto-refresh on your Powerups to ensure your long-running jobs are never killed because you ran out of computing hours. We'll automatically refresh your selected Powerup so that your job can continue running.", 
            "title": "What will happen to my running job when I run out of computing credits?"
        }, 
        {
            "location": "/faqs/plans/#preemptible-vs-dedicated-instances", 
            "text": "", 
            "title": "Preemptible vs. Dedicated Instances"
        }, 
        {
            "location": "/faqs/plans/#preemptible-instances", 
            "text": "Preemptible instances have medium job uptime SLA of 98%. This means that there is a small chance that your job can be terminated (preempted) at any point during its runtime by FloydHub if it requires access to those resources for other, higher priority tasks.  Preemptible instances (CPU / GPU) offer top notch compute at affordable prices, in exchange for fault tolerance.  Note that SLA refers to what we can guarantee. In practice, this happens infrequently. Historically, less than 0.1% of jobs run on FloydHub have encountered interruption. However, you need to be aware that there is the possibility.", 
            "title": "Preemptible Instances"
        }, 
        {
            "location": "/faqs/plans/#why-do-you-use-preemptible-instances", 
            "text": "To be able to offer you compute at a much lower cost.  We have a fixed pool of resources that we have to allocate amongst all our users. Some of our users require dedicated instances and are willing to pay the premium for uninterrupted access. But, the majority of our users can tolerate a 98% job uptime SLA for the significant price savings that preemptible instances offer.", 
            "title": "Why Do You Use Preemptible Instances?"
        }, 
        {
            "location": "/faqs/plans/#will-i-get-a-refund-if-my-job-is-preempted", 
            "text": "No.  Our preemptible instances have a 98% job uptime SLA. By using them, you are\naccepting a small chance of your job being terminated without notice, in\nexchange for paying a much lower price than dedicated instances.", 
            "title": "Will I Get a Refund if My Job is Preempted?"
        }, 
        {
            "location": "/faqs/plans/#how-will-i-know-when-my-job-is-preempted", 
            "text": "Your job's state will turn from  Running  to  Shutdown . We will send you a\nnotification informing you about this. Unfortunately, we are currently unable\nto warn your ahead of time of an impending preemption.", 
            "title": "How Will I Know When My Job Is Preempted?"
        }, 
        {
            "location": "/faqs/plans/#what-is-the-sla-of-preemptible-instances", 
            "text": "Preemptible instances have 98% job up time SLA.", 
            "title": "What is the SLA of Preemptible Instances?"
        }, 
        {
            "location": "/faqs/plans/#dedicated-instances", 
            "text": "Dedicated instances have high job uptime SLA of 99.95%. Use dedicated instances for your jobs if they are critical or not fault tolerant. You can purchase '+'  Powerups  (CPU+ / GPU+) to utilize dedicated instances.", 
            "title": "Dedicated Instances"
        }, 
        {
            "location": "/faqs/plans/#why-do-you-use-dedicated-instances", 
            "text": "If your job is not fault tolerant and cannot withstand a small ( 2%) chance\nof your job being shutdown without notice, you should use our  +  Dedicated instances.  Price sensitivity also plays a factor - dedicated instances are more expensive\nthan premptible instances.  Given that deep learning models typically train over long periods of time, it\nis good practice to build your application to be fault tolerant by regularly checkpointing your training.", 
            "title": "Why Do You Use Dedicated Instances?"
        }, 
        {
            "location": "/faqs/plans/#what-is-the-sla-of-dedicated-instances", 
            "text": "Dedicated instances have 99.95% job up time SLA.", 
            "title": "What is the SLA of Dedicated instances?"
        }, 
        {
            "location": "/faqs/plans/#what-is-the-difference-between-gpu-vs-gpu-and-cpu-vs-cpu", 
            "text": "GPU and CPU are  preemptible instances . GPU+ and CPU+ are  dedicated instances .", 
            "title": "What is the difference between GPU vs. GPU+ and CPU vs. CPU+?"
        }, 
        {
            "location": "/faqs/plans/#powerups", 
            "text": "", 
            "title": "Powerups"
        }, 
        {
            "location": "/faqs/plans/#what-are-powerups", 
            "text": "Your subscription plan comes with a monthly quota of CPU and GPU computing hours.\nIf you need more computing hours, you can buy Powerups to supplement your plan.", 
            "title": "What are Powerups?"
        }, 
        {
            "location": "/faqs/plans/#what-powerups-should-i-buy", 
            "text": "This depends on your computing needs. We offer multiple tiers of Powerups:   Preemptible vs. Dedicated : CPU / GPU are affordable  preemptible instances , CPU+ / GPU+ are high-reliability  dedicated instances .  10 vs. 50 vs. 100 hours : Purchase a pack that suits your computing needs. Note that the larger packs offer compute at a much cheaper rate/hour than smaller packs.  Auto-refresh : You can enable auto-refresh on any pack.   If you are  just starting out  and need more computing hours than your plan offers, you can start with the  GPU10 Powerup .  If you run  long-running jobs , you should purchase the  GPU100 with auto-refresh enabled , to ensure that you never run out of computing credits.  If you run  critical jobs  that are not fault-tolerant, you should purchase the  GPU+ Powerup .", 
            "title": "What Powerups should I buy?"
        }, 
        {
            "location": "/faqs/plans/#how-can-i-buy-powerups", 
            "text": "You can purchase them from your  Powerups Dashboard", 
            "title": "How can I buy Powerups?"
        }, 
        {
            "location": "/faqs/plans/#why-would-i-enable-auto-refresh", 
            "text": "Auto-refresh ensures your long-running jobs are never killed because you  ran out of computing hours . We'll automatically refresh your selected Powerup so that your Job can continue running.", 
            "title": "Why would I enable auto-refresh?"
        }, 
        {
            "location": "/faqs/plans/#can-i-buy-a-powerup-if-i-am-in-the-free-plan", 
            "text": "No. You have to be enrolled in one of the Data Scientist plans to be eligible for\npurchasing Powerups.", 
            "title": "Can I buy a Powerup if I am in the Free Plan?"
        }, 
        {
            "location": "/faqs/plans/#do-powerups-expire", 
            "text": "Yes. Powerups are valid for 1 year from the date of purchase.", 
            "title": "Do Powerups expire?"
        }, 
        {
            "location": "/faqs/plans/#how-will-my-powerups-be-used", 
            "text": "Your compute hours will be consumed in the following order:   Hours from your subscription plan  Hours from free credits  Hours from Powerups", 
            "title": "How will my Powerups be used?"
        }, 
        {
            "location": "/faqs/plans/#storage", 
            "text": "", 
            "title": "Storage"
        }, 
        {
            "location": "/faqs/plans/#how-much-storage-do-i-get", 
            "text": "Each plan comes with its own storage limit. Please see the  feature comparison\ntable  for details.", 
            "title": "How much storage do I get?"
        }, 
        {
            "location": "/faqs/plans/#what-counts-against-my-storage", 
            "text": "Storage is consumed by the datasets that you upload, your code and the data that your jobs output.  Note that you are only responsible for the data that you own. For example, if you use a public dataset in your job, you won't be charged for it.", 
            "title": "What counts against my storage?"
        }, 
        {
            "location": "/faqs/plans/#can-i-buy-more-storage-than-my-plan-offers", 
            "text": "If our Data Scientist Pro plan's storage doesn't meet your needs, please\ncontact us at  .  We will soon have a storage Powerup that can you buy to add more storage to your base plan.", 
            "title": "Can I buy more storage than my plan offers?"
        }, 
        {
            "location": "/faqs/plans/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/faqs/billing/", 
            "text": "Please see here for \nPlan FAQs\n\n\nPayment Questions\n\n\nWhat forms of payments do you accept?\n\n\nWe accept Visa, MasterCard, American Express and Discover credit and debit cards. We do not accept prepaid cards at the moment. \n\n\nWe are a US based company. If you are outside the US, please ensure your card has \ninternational transactions enabled.\n\n\nDo you keep my credit card information?\n\n\nNo, we do not retain any credit card information. We use\n\nStripe\n to process payments.\n\n\nPayment processing failed. Whay am I unable to add my payment method?\n\n\nWhen you submit your payment details, you may received a \"Error: Payment processing failed\" error notification.\n\n\n\n\nSome common causes are:\n\n\n\n\nCredit and Debit cards only\n: We currently accept only Visa, Mastercard and AmEx credit and debit cards. We do not accept prepaid cards. Please ensure you are using a valid credit or debit card.\n\n\nBalance\n: We issue a small $0-1 transaction on your card as a pending authorization request between our billing system and the bank that issued your credit or debit card. If this authorization fails, you won't be able to add your card. Please ensure you have enough balance and are using a valid card. \nNote\n: This is an authorization request only, not an actual charge.\n\n\nFraud Detection\n: We use Stripe for managing all our payments. They have their own fraud detection algorithm which can decline some cards. Please try a different payment method.\n\n\nInternational Cards\n: We are a US based company. If you are outside the US please ensure your card has international transactions enabled.\n\n\n\n\nIf you are still unable to add your payment, please contact us directly at \n.\n\n\nBilling Questions\n\n\nWhat am I billed for?\n\n\nYour usage includes compute (CPU / GPU) usage and storage consumption. \n\n\n\n\n\n\nCompute\n: You will be billed exactly for the duration that your job runs, rounded off to the nearest second.\n\n\nNote that you are only charged for compute when your job is in the \nRunning\n state. You will \nnot\n be charged when your job is in any other state, including \nQueued\n and \nShutdown\n.\n\n\n\n\n\n\nStorage\n: You will also be charged for the storage you consume, rounded off to the nearest kB. \n\n\nStorage is consumed by the datasets that you upload, your code and the data that your jobs output.\n\n\n\n\n\n\nWhen will my card be charged?\n\n\nFor subscription plans, your card will be charged every month on the day you upgraded to the plan. For example, if you upgraded from the Free to the Data Scientist Pro plan on the 22\nnd\n of August, you will be charged immediately. You will be then billed on the 22\nnd\n of every month.\n\n\nWhen you purchase Powerups, either directly from the \nPowerups Dashboard\n or via auto-refresh, you will be charged at the time of purchase.\n\n\nCan I upgrade or downgrade my plan?\n\n\nYou can upgrade or downgrade your subscriptions at anytime from your \nPlans page\n under Settings on your dashboard.\n\n\nWhen you upgrade, you will be immediately elevated to the new plan. When you downgrade, your new plan will start at the end of your billing cycle (since you will have already paid for the month in advance).\n\n\nUpgrades and downgrades inside the Data Scientist plans do not affect any Powerups you \nmay have purchased.\n\n\nHow do upgrades work?\n\n\nWhen you upgrade from the Free plan to a paid plan, you will be charged immediately. You will then be billed on the 1 month anniversary of your subscription date every month. For example, if you upgrade on August 20\nth\n, 2017, you will be charged on that day and on the 20\nth\n of every subsequent month.\n\n\nWhen you upgrade from one paid plan to another, you will be charged for your new plan on a pro-rated basis. Lets say your billing cycle is on the 20\nth\n of every month. If you upgrade from the Data Scientist Base to the Pro plan on the 5\nth\n, you will be charged for the new plan on a pro-rated basis (5\nth\n to 20\nth\n).\n\n\nHow do downgrades work?\n\n\nYou can downgrade at any time to a lower or Free plan. \n\n\nIf you are over the usage limits for the plan you are downgrading to, you will have to handle that first. For example, if you are downgrading from the Data Scientist Pro to Base plan, but have 300GB data, you will have to delete some of your data before downgrading since the Base plan only offers 100 GB.\n\n\nIf you downgrade to the Free plan, you will no longer have access to any Powerups that you may have purchased.\n\n\nHow do I remove my credit card?\n\n\nPlease downgrade to the Free plan. We will remove your credit card at the end of your billing cycle.\n\n\nDo my remaining compute credits roll over each month if I don't use them all?\n\n\nNo, your monthly Plan compute credits are not rolled over. However, your Powerup credits will remain valid for one year from purchase date.\n\n\nDo you offer refunds?\n\n\nNo, we do not offer refunds. If there are extenuating circumstances, please open a ticket by contacting our support team.\n\n\n\n\nHelp make this document better\n\n\nThis guide, as well as the rest of our docs, are open-source and available on \n\nGitHub\n. We welcome your contributions.\n\n\n\n\nSuggest an edit to this page (by clicking the edit icon at the top next to the title).\n\n\nOpen an issue about this page\n to report a problem.", 
            "title": "Billing"
        }, 
        {
            "location": "/faqs/billing/#payment-questions", 
            "text": "", 
            "title": "Payment Questions"
        }, 
        {
            "location": "/faqs/billing/#what-forms-of-payments-do-you-accept", 
            "text": "We accept Visa, MasterCard, American Express and Discover credit and debit cards. We do not accept prepaid cards at the moment.   We are a US based company. If you are outside the US, please ensure your card has \ninternational transactions enabled.", 
            "title": "What forms of payments do you accept?"
        }, 
        {
            "location": "/faqs/billing/#do-you-keep-my-credit-card-information", 
            "text": "No, we do not retain any credit card information. We use Stripe  to process payments.", 
            "title": "Do you keep my credit card information?"
        }, 
        {
            "location": "/faqs/billing/#payment-processing-failed-whay-am-i-unable-to-add-my-payment-method", 
            "text": "When you submit your payment details, you may received a \"Error: Payment processing failed\" error notification.   Some common causes are:   Credit and Debit cards only : We currently accept only Visa, Mastercard and AmEx credit and debit cards. We do not accept prepaid cards. Please ensure you are using a valid credit or debit card.  Balance : We issue a small $0-1 transaction on your card as a pending authorization request between our billing system and the bank that issued your credit or debit card. If this authorization fails, you won't be able to add your card. Please ensure you have enough balance and are using a valid card.  Note : This is an authorization request only, not an actual charge.  Fraud Detection : We use Stripe for managing all our payments. They have their own fraud detection algorithm which can decline some cards. Please try a different payment method.  International Cards : We are a US based company. If you are outside the US please ensure your card has international transactions enabled.   If you are still unable to add your payment, please contact us directly at  .", 
            "title": "Payment processing failed. Whay am I unable to add my payment method?"
        }, 
        {
            "location": "/faqs/billing/#billing-questions", 
            "text": "", 
            "title": "Billing Questions"
        }, 
        {
            "location": "/faqs/billing/#what-am-i-billed-for", 
            "text": "Your usage includes compute (CPU / GPU) usage and storage consumption.     Compute : You will be billed exactly for the duration that your job runs, rounded off to the nearest second.  Note that you are only charged for compute when your job is in the  Running  state. You will  not  be charged when your job is in any other state, including  Queued  and  Shutdown .    Storage : You will also be charged for the storage you consume, rounded off to the nearest kB.   Storage is consumed by the datasets that you upload, your code and the data that your jobs output.", 
            "title": "What am I billed for?"
        }, 
        {
            "location": "/faqs/billing/#when-will-my-card-be-charged", 
            "text": "For subscription plans, your card will be charged every month on the day you upgraded to the plan. For example, if you upgraded from the Free to the Data Scientist Pro plan on the 22 nd  of August, you will be charged immediately. You will be then billed on the 22 nd  of every month.  When you purchase Powerups, either directly from the  Powerups Dashboard  or via auto-refresh, you will be charged at the time of purchase.", 
            "title": "When will my card be charged?"
        }, 
        {
            "location": "/faqs/billing/#can-i-upgrade-or-downgrade-my-plan", 
            "text": "You can upgrade or downgrade your subscriptions at anytime from your  Plans page  under Settings on your dashboard.  When you upgrade, you will be immediately elevated to the new plan. When you downgrade, your new plan will start at the end of your billing cycle (since you will have already paid for the month in advance).  Upgrades and downgrades inside the Data Scientist plans do not affect any Powerups you \nmay have purchased.", 
            "title": "Can I upgrade or downgrade my plan?"
        }, 
        {
            "location": "/faqs/billing/#how-do-upgrades-work", 
            "text": "When you upgrade from the Free plan to a paid plan, you will be charged immediately. You will then be billed on the 1 month anniversary of your subscription date every month. For example, if you upgrade on August 20 th , 2017, you will be charged on that day and on the 20 th  of every subsequent month.  When you upgrade from one paid plan to another, you will be charged for your new plan on a pro-rated basis. Lets say your billing cycle is on the 20 th  of every month. If you upgrade from the Data Scientist Base to the Pro plan on the 5 th , you will be charged for the new plan on a pro-rated basis (5 th  to 20 th ).", 
            "title": "How do upgrades work?"
        }, 
        {
            "location": "/faqs/billing/#how-do-downgrades-work", 
            "text": "You can downgrade at any time to a lower or Free plan.   If you are over the usage limits for the plan you are downgrading to, you will have to handle that first. For example, if you are downgrading from the Data Scientist Pro to Base plan, but have 300GB data, you will have to delete some of your data before downgrading since the Base plan only offers 100 GB.  If you downgrade to the Free plan, you will no longer have access to any Powerups that you may have purchased.", 
            "title": "How do downgrades work?"
        }, 
        {
            "location": "/faqs/billing/#how-do-i-remove-my-credit-card", 
            "text": "Please downgrade to the Free plan. We will remove your credit card at the end of your billing cycle.", 
            "title": "How do I remove my credit card?"
        }, 
        {
            "location": "/faqs/billing/#do-my-remaining-compute-credits-roll-over-each-month-if-i-dont-use-them-all", 
            "text": "No, your monthly Plan compute credits are not rolled over. However, your Powerup credits will remain valid for one year from purchase date.", 
            "title": "Do my remaining compute credits roll over each month if I don't use them all?"
        }, 
        {
            "location": "/faqs/billing/#do-you-offer-refunds", 
            "text": "No, we do not offer refunds. If there are extenuating circumstances, please open a ticket by contacting our support team.", 
            "title": "Do you offer refunds?"
        }, 
        {
            "location": "/faqs/billing/#help-make-this-document-better", 
            "text": "This guide, as well as the rest of our docs, are open-source and available on  GitHub . We welcome your contributions.   Suggest an edit to this page (by clicking the edit icon at the top next to the title).  Open an issue about this page  to report a problem.", 
            "title": "Help make this document better"
        }, 
        {
            "location": "/home/using_datasets/", 
            "text": "Redirecting...", 
            "title": ""
        }, 
        {
            "location": "/home/using_datasets/#redirecting", 
            "text": "", 
            "title": "Redirecting..."
        }, 
        {
            "location": "/home/install/", 
            "text": "Redirecting...", 
            "title": ""
        }, 
        {
            "location": "/home/install/#redirecting", 
            "text": "", 
            "title": "Redirecting..."
        }, 
        {
            "location": "/home/getting_started/", 
            "text": "Redirecting...", 
            "title": ""
        }, 
        {
            "location": "/home/getting_started/#redirecting", 
            "text": "", 
            "title": "Redirecting..."
        }
    ]
}